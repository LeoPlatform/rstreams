[{"url":"https://rstreams.org/rstreams-guides/core-concepts/fundamentals/","title":"Fundamentals","description":"Fundamental concepts.","content":"  Events   Event ID TODO\n  Pipeline (Pipe) TODO\n  Pipeline Step TODO\n  Stream Pipeline Step or Bots/Queues. TODO\n  Checkpoint Writing last read event ID back to queue. TODO\n  Event source timestamp Ancestors and derivatives and the value it provides TODO\n  Started timestamp (bot) TODO\n  Ended timestamp (bot) TODO\n  Correlation ID TODO\n  Units (bot) TODO\n  Checkpoint TODO\n"},{"url":"https://rstreams.org/why-rstreams/","title":"Why RStreams?","description":"What is RStreams and why should I use it?","content":"  Why RStreams? Why not just AWS services?   Less Friction AWS services rock. Each service started life to serve a specific purpose and then grew in size and scope, becoming useful to more and more use cases. Yet, each service was born and iterated from a starting foundation, giving each a sweet spot for where it excels and step outside this sweet spot and friction emerges.\nTODO: talk about why we\u0026rsquo;d use queues to solve problems and the reactive mindset\n"},{"url":"https://rstreams.org/rstreams-guides/checkpointing/","title":"Checkpointing","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-guides/fanout/","title":"Fanout","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-botmon/getting-started/","title":"Getting Started","description":"","content":"Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet, tempus metus quis, pharetra turpis. Phasellus at massa sit amet ante semper fermentum sed eget lectus. Quisque id dictum magna turpis.\n Etiam vestibulum risus vel arcu elementum eleifend. Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet\n Etiam vestibulum risus vel arcu elementum eleifend. Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet, tempus metus quis, pharetra turpis. Phasellus at massa sit amet ante semper fermentum sed eget lectus. Quisque id dictum magna, et dapibus turpis.Etiam vestibulum risus vel arcu elementum eleifend. Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet, tempus metus quis, pharetra turpis. Phasellus at massa sit amet ante semper fermentum sed eget lectus. Quisque id dictum magna, et dapibus turpis.\n"},{"url":"https://rstreams.org/rstreams-bus/getting-started/","title":"Getting Started","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-flow/getting-started/","title":"Getting Started","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-node-sdk/getting-started/","title":"Getting Started","description":"","content":"\r\r\r ToC\r   Are you setup to run the examples? Principle Operations Write to the bus  Write a single object to the bus Write multiple objects to the bus Stream multiple objects to the bus fast      \r\r  Are you setup to run the examples? \r\r\r\rExpand this section if you\u0026rsquo;re not sure  \rAll examples in the SDK documentation assume that when these apps run, the RStreams SDK can discover the configuration it needs. The config it needs is the AWS resource IDs of the RStreams Bus instance deployed in your AWS account. Things like the ID of the kinesis stream used by the bus and so on.\nOf course, in a production environment the SDK will get the config in an intelligent and safe manner, say from AWS Secrets Manager. See the RStreams Flow Configuring RStreams doc.\nHere\u0026rsquo;s the typescript type of the config.\n  Get the config You will first need to get this config. By default, the RStreams Bus puts a secret in secrets manager that is the JSON config blob. The secret will be named rstreams-\u0026lt;bus name\u0026gt;. Go get the JSON config from this secret.\n  Save the config   As a file Create a file named rstreams.config.json and put it in the same directory you are running your app in or in any parent director and the SDK will just find it and use it.\n  As an environment variable Create an environment variable named RSTREAMS_CONFIG whose value is the config JSON blob.\n  As an argument to the SDK itself Create a variable in the code that is the config and then pass it into the SDK\u0026rsquo;s constructor.\n1 2const RSTREAMS_BUS_CONFIG: ConfigurationResources = { 3 \u0026#34;Region\u0026#34;: \u0026#34;some-value\u0026#34;, 4 \u0026#34;LeoStream\u0026#34;: \u0026#34;some-value\u0026#34;, 5 \u0026#34;LeoCron\u0026#34;: \u0026#34;some-value\u0026#34;, 6 \u0026#34;LeoSettings\u0026#34;: \u0026#34;some-value\u0026#34;, 7 \u0026#34;LeoEvent\u0026#34;: \u0026#34;some-value\u0026#34;, 8 \u0026#34;LeoKinesisStream\u0026#34; : \u0026#34;some-value\u0026#34;, 9 \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;some-value\u0026#34;, 10 \u0026#34;LeoS3\u0026#34;: \u0026#34;some-value\u0026#34; 11}; 12 13const rsdk: RStreamsSdk = new RStreamsSdk(RSTREAMS_BUS_CONFIG); \r   Principle Operations Write\nYou\u0026rsquo;re going to want to write to the bus, meaning send a data event to a specific queue of the bus. Queues maintain their order, with the newest at the front of the queue and the oldest data at the back of the queue.\nRead\nYou\u0026rsquo;re going to want to read from the bus, meaning read events from a queue of the bus. You typically read from the last place you read from last in a queue. Or, if this is your bot\u0026rsquo;s first time reading from a queue then the oldest event in the queue is the default. Or, you can read events in a specific range back in time in the queue.\nTransform\nYou\u0026rsquo;re going to want to read from the bus, change the data somehow or cause a side effect like writing to some database, and then write the changed data to a different queue.\n  Write to the bus You want to write data to an RStreams qeuue.\nTODO: include link to git project so can checkout and run\n  Write a single object to the bus Let\u0026rsquo;s say we want to populate an RStreams queue with people we retrieve from an API that generates random people. The steps to do that are\n Line 6 : Create an instance of the SDK Line 7 : Go get a single random person from a public API using the Axios library Line 8 : Call the putEvent SDK API to send an event up to the RStreams Bus  The first argument is the ID of the bot this code is running as The second argument is the ID of the RStreams queue to send the event to The third argument is the JSON object to send    1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRaw, PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const person = await getRandomPerson(); 8 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 9} 10 11async function getRandomPerson(): Promise\u0026lt;PersonRaw\u0026gt; { 12 const NUM_EVENTS = 1; 13 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 14 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 15 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 16 17 if (status !== 200) { 18 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 19 } 20 21 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 22 23 return data.results[0]; 24} 25 26(async () =\u0026gt; { 27 await main(); 28})() \r\r\r PersonRaw \u0026amp; PersonRawResults interfaces\r 1export interface PersonRaw { 2 gender: string; 3 name: { 4 title: string; 5 first: string; 6 last: string; 7 } 8 location: { 9 street: { 10 number: number; 11 name: string; 12 } 13 city: string; 14 state: string; 15 country: string; 16 postcode: number; 17 coordinates: { 18 longitude: string; 19 latitude: string; 20 } 21 timezone: { 22 offset: string; 23 description: string; 24 } 25 } 26 email: string; 27 dob: { 28 date: string; 29 age: number; 30 } 31 nat: string; 32} 33 34export interface PersonRawResults { 35 results: PersonRaw[]; 36} \r\rView results in Botmon\nIf you go to Botmon, you will see that the rstreams-example.people queue now has an event in it. \r\r\rExpand for Botmon screenshots  \r  Go to Botmon and search for rstreams-example.people in the search field     Botmon now shows a visual representation of the bot and the queue, click on the gear icon after hovering over the queue and then click on Events     Botmon now shows the events loaded into the queue     \r\n  Write multiple objects to the bus So, instead of reading one person from the public API we used in the example above, let\u0026rsquo;s say we get 100 people at a time from the public API and we want to write them to the bus. Here\u0026rsquo;s what that looks like.\n1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const people = await getRandomPeople(); 8 9 //HINT: this will have very bad performance. This is just to illustrate a point. 10 // Don\u0026#39;t use putEvent in a loop this way in practice! 11 for (const person of people.results) { 12 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 13 } 14} 15 16async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 17 const NUM_EVENTS = 100; 18 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 19 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 20 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 21 22 if (status !== 200) { 23 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 24 } 25 26 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 27 28 return data; 29} 30 31(async () =\u0026gt; { 32 await main(); 33})() The only difference in this example is that we pass in 100 to the public API, getting back 100 objects as an array. We then loop through them, making a connection to the RStreams Bus for each and every event. It\u0026rsquo;s simple and it works but this is bad. The putEvent API is really only meant for one or maybe a handful of events. To understand why, consider what the RStreams SDK is doing when you call putEvent.\n It\u0026rsquo;s opening a connection to AWS Kinesis It sending the single event on that connection each time to Kinesis The event flows through Kinesis until an RStreams Kinesis processor reads the single event and writes it to the RStreams Dynamo DB queue table, putting the event in the correct queue  RStreams is designed to handle the continuos generation of data events that flow into a given queue, is read from that queue and mutated and then sent to other queues. It is today doing this with very large amounts of concurrently received events. The RStreams SDK has a better way to work with sending larger amounts of data to the bus, meaning to an RStreams queue.\n  Stream multiple objects to the bus fast It\u0026rsquo;s time to tackle the idea of streams. If you aren\u0026rsquo;t well versed on streams, jump over and read the Streams Primer. It\u0026rsquo;s short and sweet and may well convert you to streams if you aren\u0026rsquo;t already.\n1import { RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const es = rsdk.streams.eventstream; 8 const people = await getRandomPeople(); 9 10 await rsdk.streams.pipeAsync( 11 es.readArray(people.results), 12 rsdk.load(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, 13 {records: 25, time: 5000, useS3: true}) 14 ); 15} 16 17async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 18 const NUM_EVENTS = 100; 19 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 20 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 21 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 22 23 if (status !== 200) { 24 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 25 } 26 27 return data; 28} 29 30(async () =\u0026gt; { 31 await main(); 32})() "},{"url":"https://rstreams.org/why-rstreams/getting-started/","title":"Getting Started","description":"","content":"RStreams is a server-side bus installed in an AWS account and an SDK for creating reactive applications that use the bus and a monitoring web app called Botmon. RStreams Flow is an opinionated framework to make getting a client-side project up and running super easy.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/sink-streams/load/","title":"Load Stream","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1 Example 2        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API docs: async version | sync version\nTODO: desc\n  When would I use this?  TODO TODO    Runnable Examples   Example 1 TODO\n\r\r Example 1 code\r 1TODO \r\r  Example 2 TODO\n\r\r Example 2 code\r 1TODO \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/put/","title":"Put","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1: Write a Single Object to the Bus Example 2: Write multiple objects to the bus (slow performance)        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API docs: async version | sync version\nA stand-alone function, meaning one that doesn\u0026rsquo;t use pipes and streams, that reads events from the specified source RStreams queue and then calls your transform function allowing you to do anything you want to with the data.\n  When would I use this?  You\u0026rsquo;re in an app and want to send a single event to an RStreams queue on a very infrequent basis You\u0026rsquo;ve got a pipe that does something and you want to enhance it, as the side effect of a given stream step function, to send events to another RStreams queue    Runnable Examples   Example 1: Write a Single Object to the Bus The first example is a naive example that sends data to an RStreams queue one at a time. The code makes a call out to a free API that returns random people, gets a single person back and then on line 6 uses putEvent to send that person to the rstreams-example.people queue, doing so as a bot with ID of rstreams-example.load-people.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\n\r\r Example 1 code\r 1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRaw, PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const person = await getRandomPerson(); 8 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 9} 10 11async function getRandomPerson(): Promise\u0026lt;PersonRaw\u0026gt; { 12 const NUM_EVENTS = 1; 13 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 14 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 15 16 if (status !== 200) { 17 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 18 } 19 20 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 21 22 return data.results[0]; 23} 24 25(async () =\u0026gt; { 26 await main(); 27})() \r\rNote: Person types referenced in the examples\n  View results in Botmon If you go to Botmon, you will see that the rstreams-example.people queue now has an event in it. \r\r\rExpand for Botmon screenshots  \r  Go to Botmon and search for rstreams-example.people in the search field     Botmon now shows a visual representation of the bot and the queue, click on the gear icon after hovering over the queue and then click on Events     Botmon now shows the events loaded into the queue     \r\n  Example 2: Write multiple objects to the bus (slow performance)  This is an example of what not to do. When you want to write many events to an RStreams queue, use the Load Stream pipe step.\n So, instead of reading one person from the public API we used in the example above, let\u0026rsquo;s say we get 100 people at a time from the public API and we want to write them to the bus.\nThe only difference in this example is that we pass in 100 to the public API, getting back 100 objects as an array. We then loop through them, making a connection to the RStreams Bus for each and every event. It\u0026rsquo;s simple and it works but this is bad. The putEvent API is really only meant to be called infrequently for one or maybe a handful of events. To understand why, consider what the RStreams SDK is doing when you call putEvent.\n It\u0026rsquo;s opening a connection to AWS Kinesis It sending the single event on that connection each time to Kinesis The event flows through Kinesis until an RStreams Kinesis processor reads the single event and writes it to the RStreams Dynamo DB queue table, putting the event in the correct queue  RStreams is designed to handle the continuos generation of data events that flow into a given queue, is read from that queue and mutated and then sent to other queues. It is today doing this with very large amounts of concurrently received events and has optimizations for sending lots of data. The Load Stream pipe step is a much better way to send large amounts of data to the bus, meaning to an RStreams queue.\nNote: Person types referenced in the examples\n\r\r Example 2 code\r 1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const people = await getRandomPeople(); 8 9 //HINT: this will have very bad performance. This is just to illustrate a point. 10 // Don\u0026#39;t use putEvent in a loop this way in practice, instead use sdk.load! 11 for (const person of people.results) { 12 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 13 } 14} 15 16async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 17 const NUM_EVENTS = 100; 18 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 19 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 20 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 21 22 if (status !== 200) { 23 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 24 } 25 26 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 27 28 return data; 29} 30 31(async () =\u0026gt; { 32 await main(); 33})() \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/source-streams/read/","title":"Read","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a source stream, fed by events from the specified RStreams queue, to act as the first step in a pipe. Just specify the RStreams queue and config to read efficiently and you\u0026rsquo;re done.\n  When would I use this?  I want to use a pipe to have a little more control over processing The data I want to process comes from an RStreams queue    Runnable Examples   Example 1 This example reads 100 events from the rstreams-example.peopleplus RStreams queue and then shuts down the pipe. The read stream sends the events to the devnull stream. illustrates code running as a bot with ID of rstreams-example.people-to-peopleplus and getting exactly two events from queue rstreams-example.people, starting at position z/2022/04/20, and then transforms each event\u0026rsquo;s JSON by dropping unwanted attributes and simplifying the JSON structure. It also calls a totally free, public API that given a country name returns the standard two-char country code which we tack on to the event after which we return the modified event which tells the SDK to push it to the rstreams-example.people-to-peopleplus queue.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\nSecond, there are actually three arguments to the transform function, even though in our example we are only using the first. What is stored in an RStreams queue is an instance of a ReadEvent where the payload attribute is the data the queue exists for. The first argument is just the payload pulled out since usually that\u0026rsquo;s all you need. The second argument is the full event from the queue with the event ID and other sometimes useful things. The third argument is only used in the callback version where you call done exactly once to trigger the callback. It\u0026rsquo;s there for backwared compat. Don\u0026rsquo;t use it on new things.\nThe devnull at the end just acts as a sink and passing in true tells it to log. That\u0026rsquo;s all it\u0026rsquo;s for, to act as a sink. See the doc on Devnull for more details.\n\r\r Example 1 code\r 1import { ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 const opts: ReadOptions = { 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 limit: 5 11 } 12 13 await rsdk.streams.pipeAsync( 14 rsdk.read\u0026lt;Person\u0026gt;(\u0026#39;rstreams-example.peopleplus-to-devnull\u0026#39;, 15 \u0026#39;rstreams-example.peopleplus\u0026#39;, opts), 16 rsdk.streams.devnull(true) 17 ); 18} 19 20(async () =\u0026gt; { 21 try { 22 await main(); 23 } catch(err) { 24 console.log(err); 25 } 26})() \r\r\r\r Example 1 console output\r 1➜ rstreams-runnable-examples ts-node apps/read-events-simple.ts 2Reading event from z/2022/04/20 3devnull { 4 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 5 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 6 \u0026#34;payload\u0026#34;: { 7 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 8 \u0026#34;firstName\u0026#34;: \u0026#34;Herman\u0026#34;, 9 \u0026#34;lastName\u0026#34;: \u0026#34;Morris\u0026#34;, 10 \u0026#34;email\u0026#34;: \u0026#34;herman.morris@example.com\u0026#34;, 11 \u0026#34;birthDate\u0026#34;: \u0026#34;1959-04-25T19:28:13.361Z\u0026#34;, 12 \u0026#34;nationality\u0026#34;: \u0026#34;IE\u0026#34;, 13 \u0026#34;addr\u0026#34;: { 14 \u0026#34;addr1\u0026#34;: \u0026#34;9393 Mill Lane\u0026#34;, 15 \u0026#34;city\u0026#34;: \u0026#34;Killarney\u0026#34;, 16 \u0026#34;state\u0026#34;: \u0026#34;Galway City\u0026#34;, 17 \u0026#34;country\u0026#34;: \u0026#34;Ireland\u0026#34;, 18 \u0026#34;postcode\u0026#34;: 34192, 19 \u0026#34;longitude\u0026#34;: \u0026#34;-48.3422\u0026#34;, 20 \u0026#34;latitude\u0026#34;: \u0026#34;23.2617\u0026#34;, 21 \u0026#34;tzOffset\u0026#34;: \u0026#34;-12:00\u0026#34;, 22 \u0026#34;tzDesc\u0026#34;: \u0026#34;Eniwetok, Kwajalein\u0026#34;, 23 \u0026#34;countryCode\u0026#34;: \u0026#34;IE\u0026#34; 24 } 25 }, 26 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 27 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/21/20/37/1650573479245-0000000\u0026#34;, 28 \u0026#34;correlation_id\u0026#34;: { 29 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 30 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000000\u0026#34;, 31 \u0026#34;units\u0026#34;: 1 32 }, 33 \u0026#34;timestamp\u0026#34;: 1650573479299 34} 35devnull { 36 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 37 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 38 \u0026#34;payload\u0026#34;: { 39 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 40 \u0026#34;firstName\u0026#34;: \u0026#34;Herman\u0026#34;, 41 \u0026#34;lastName\u0026#34;: \u0026#34;Morris\u0026#34;, 42 \u0026#34;email\u0026#34;: \u0026#34;herman.morris@example.com\u0026#34;, 43 \u0026#34;birthDate\u0026#34;: \u0026#34;1959-04-25T19:28:13.361Z\u0026#34;, 44 \u0026#34;nationality\u0026#34;: \u0026#34;IE\u0026#34;, 45 \u0026#34;addr\u0026#34;: { 46 \u0026#34;addr1\u0026#34;: \u0026#34;9393 Mill Lane\u0026#34;, 47 \u0026#34;city\u0026#34;: \u0026#34;Killarney\u0026#34;, 48 \u0026#34;state\u0026#34;: \u0026#34;Galway City\u0026#34;, 49 \u0026#34;country\u0026#34;: \u0026#34;Ireland\u0026#34;, 50 \u0026#34;postcode\u0026#34;: 34192, 51 \u0026#34;longitude\u0026#34;: \u0026#34;-48.3422\u0026#34;, 52 \u0026#34;latitude\u0026#34;: \u0026#34;23.2617\u0026#34;, 53 \u0026#34;tzOffset\u0026#34;: \u0026#34;-12:00\u0026#34;, 54 \u0026#34;tzDesc\u0026#34;: \u0026#34;Eniwetok, Kwajalein\u0026#34;, 55 \u0026#34;countryCode\u0026#34;: \u0026#34;IE\u0026#34; 56 } 57 }, 58 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 59 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645572667-0000134\u0026#34;, 60 \u0026#34;correlation_id\u0026#34;: { 61 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 62 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000000\u0026#34;, 63 \u0026#34;units\u0026#34;: 1 64 }, 65 \u0026#34;timestamp\u0026#34;: 1650645572513 66} 67devnull { 68 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 69 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 70 \u0026#34;payload\u0026#34;: { 71 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 72 \u0026#34;firstName\u0026#34;: \u0026#34;Tomothy\u0026#34;, 73 \u0026#34;lastName\u0026#34;: \u0026#34;Rogers\u0026#34;, 74 \u0026#34;email\u0026#34;: \u0026#34;tomothy.rogers@example.com\u0026#34;, 75 \u0026#34;birthDate\u0026#34;: \u0026#34;1967-01-22T18:32:59.793Z\u0026#34;, 76 \u0026#34;nationality\u0026#34;: \u0026#34;AU\u0026#34;, 77 \u0026#34;addr\u0026#34;: { 78 \u0026#34;addr1\u0026#34;: \u0026#34;6582 Adams St\u0026#34;, 79 \u0026#34;city\u0026#34;: \u0026#34;Kalgoorlie\u0026#34;, 80 \u0026#34;state\u0026#34;: \u0026#34;Australian Capital Territory\u0026#34;, 81 \u0026#34;country\u0026#34;: \u0026#34;Australia\u0026#34;, 82 \u0026#34;postcode\u0026#34;: 8157, 83 \u0026#34;longitude\u0026#34;: \u0026#34;33.3086\u0026#34;, 84 \u0026#34;latitude\u0026#34;: \u0026#34;49.2180\u0026#34;, 85 \u0026#34;tzOffset\u0026#34;: \u0026#34;+5:30\u0026#34;, 86 \u0026#34;tzDesc\u0026#34;: \u0026#34;Bombay, Calcutta, Madras, New Delhi\u0026#34;, 87 \u0026#34;countryCode\u0026#34;: \u0026#34;AU\u0026#34; 88 } 89 }, 90 \u0026#34;event_source_timestamp\u0026#34;: 1650415833985, 91 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645572667-0000135\u0026#34;, 92 \u0026#34;correlation_id\u0026#34;: { 93 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 94 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000001\u0026#34;, 95 \u0026#34;units\u0026#34;: 1 96 }, 97 \u0026#34;timestamp\u0026#34;: 1650645572690 98} 99devnull { 100 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 101 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 102 \u0026#34;payload\u0026#34;: { 103 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 104 \u0026#34;firstName\u0026#34;: \u0026#34;Herman\u0026#34;, 105 \u0026#34;lastName\u0026#34;: \u0026#34;Morris\u0026#34;, 106 \u0026#34;email\u0026#34;: \u0026#34;herman.morris@example.com\u0026#34;, 107 \u0026#34;birthDate\u0026#34;: \u0026#34;1959-04-25T19:28:13.361Z\u0026#34;, 108 \u0026#34;nationality\u0026#34;: \u0026#34;IE\u0026#34;, 109 \u0026#34;addr\u0026#34;: { 110 \u0026#34;addr1\u0026#34;: \u0026#34;9393 Mill Lane\u0026#34;, 111 \u0026#34;city\u0026#34;: \u0026#34;Killarney\u0026#34;, 112 \u0026#34;state\u0026#34;: \u0026#34;Galway City\u0026#34;, 113 \u0026#34;country\u0026#34;: \u0026#34;Ireland\u0026#34;, 114 \u0026#34;postcode\u0026#34;: 34192, 115 \u0026#34;longitude\u0026#34;: \u0026#34;-48.3422\u0026#34;, 116 \u0026#34;latitude\u0026#34;: \u0026#34;23.2617\u0026#34;, 117 \u0026#34;tzOffset\u0026#34;: \u0026#34;-12:00\u0026#34;, 118 \u0026#34;tzDesc\u0026#34;: \u0026#34;Eniwetok, Kwajalein\u0026#34;, 119 \u0026#34;countryCode\u0026#34;: \u0026#34;IE\u0026#34; 120 } 121 }, 122 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 123 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645583644-0000111\u0026#34;, 124 \u0026#34;correlation_id\u0026#34;: { 125 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 126 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000009\u0026#34;, 127 \u0026#34;units\u0026#34;: 10 128 }, 129 \u0026#34;timestamp\u0026#34;: 1650645583447 130} 131devnull { 132 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 133 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 134 \u0026#34;payload\u0026#34;: { 135 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 136 \u0026#34;firstName\u0026#34;: \u0026#34;Tomothy\u0026#34;, 137 \u0026#34;lastName\u0026#34;: \u0026#34;Rogers\u0026#34;, 138 \u0026#34;email\u0026#34;: \u0026#34;tomothy.rogers@example.com\u0026#34;, 139 \u0026#34;birthDate\u0026#34;: \u0026#34;1967-01-22T18:32:59.793Z\u0026#34;, 140 \u0026#34;nationality\u0026#34;: \u0026#34;AU\u0026#34;, 141 \u0026#34;addr\u0026#34;: { 142 \u0026#34;addr1\u0026#34;: \u0026#34;6582 Adams St\u0026#34;, 143 \u0026#34;city\u0026#34;: \u0026#34;Kalgoorlie\u0026#34;, 144 \u0026#34;state\u0026#34;: \u0026#34;Australian Capital Territory\u0026#34;, 145 \u0026#34;country\u0026#34;: \u0026#34;Australia\u0026#34;, 146 \u0026#34;postcode\u0026#34;: 8157, 147 \u0026#34;longitude\u0026#34;: \u0026#34;33.3086\u0026#34;, 148 \u0026#34;latitude\u0026#34;: \u0026#34;49.2180\u0026#34;, 149 \u0026#34;tzOffset\u0026#34;: \u0026#34;+5:30\u0026#34;, 150 \u0026#34;tzDesc\u0026#34;: \u0026#34;Bombay, Calcutta, Madras, New Delhi\u0026#34;, 151 \u0026#34;countryCode\u0026#34;: \u0026#34;AU\u0026#34; 152 } 153 }, 154 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 155 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645583644-0000112\u0026#34;, 156 \u0026#34;correlation_id\u0026#34;: { 157 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 158 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000009\u0026#34;, 159 \u0026#34;units\u0026#34;: 10 160 }, 161 \u0026#34;timestamp\u0026#34;: 1650645583448 162} \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/","title":"Standalone Ops","description":"","content":"\r\r\r ToC\r    \r\rThese powerful standalone operations, meaning without needing to use pipes and streams, do some heavy lifting for you to hide all the complexity of sending events to and getting events from the RStreams bus.\n put Operation A function that lets you write a single event to the specified RStreams queue enrich Operation A function that reads from the specified source RStreams queue, lets you transform the events and then sends the modified events to the specified destination RStreams queue offload Operation A function that reads from the specified RStreams queue and lets you do something with the events retrieved, perhaps save them in a DB  "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/transform-streams/stringify/","title":"Stringify Stream","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a transform stream, meaning a stream that exists to receive events after the source stream, do something with them and then send them on to the next pipe step, which must exist.\nIt takes each event, stringifies it and tacks on a newline character at the end and sends that string, with the newline, on to the next step in the pipe. It is used to create json lines content to either feed to an s3 file or just a file one the local file system.\n  When would I use this?  I want to make a JSON lines file from the events flowing through the stream    Runnable Examples   Example 1 This example reads 5 events from the rstreams-example.peopleplus RStreams queue. The pipe then creates a throughAsync stream step that just takes the ReadEvent\u0026lt;Person\u0026gt; events read from the bus and turns it into a PersonLight event and sends it on to the stringify stream to make a json line ready to stick in a json lines file.\nFinally, it writes the file using the Node standard filesystem fsmodule to create a sink that writes events that flow into the sink to a file. Pretty convenient.\n\r\r Example 1 code\r 1import { ReadEvent, ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import fs from \u0026#34;fs\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 const opts: ReadOptions = { 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 limit: 5 11 } 12 13 await rsdk.streams.pipeAsync( 14 rsdk.read\u0026lt;Person\u0026gt;(\u0026#39;rstreams-example.peopleplus-to-jsonlines\u0026#39;, 15 \u0026#39;rstreams-example.peopleplus\u0026#39;, opts), 16 rsdk.streams.throughAsync\u0026lt;ReadEvent\u0026lt;Person\u0026gt;, PersonLight\u0026gt;(async (p: ReadEvent\u0026lt;Person\u0026gt;) =\u0026gt; { 17 return { 18 firstName: p.payload.firstName, 19 lastName: p.payload.lastName, 20 email: p.payload.email 21 } 22 }), 23 rsdk.streams.stringify(), 24 fs.createWriteStream(\u0026#34;./output/people.jsonl\u0026#34;), 25 ); 26} 27 28interface PersonLight { 29 firstName: string; 30 lastName: string; 31 email: string; 32} 33 34(async () =\u0026gt; { 35 try { 36 await main(); 37 } catch(err) { 38 console.log(err); 39 } 40})() \r\r\r\r Generated people.jsonl file\r 1{\u0026#34;firstName\u0026#34;:\u0026#34;Herman\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Morris\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;herman.morris@example.com\u0026#34;} 2{\u0026#34;firstName\u0026#34;:\u0026#34;Herman\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Morris\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;herman.morris@example.com\u0026#34;} 3{\u0026#34;firstName\u0026#34;:\u0026#34;Tomothy\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Rogers\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;tomothy.rogers@example.com\u0026#34;} 4{\u0026#34;firstName\u0026#34;:\u0026#34;Herman\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Morris\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;herman.morris@example.com\u0026#34;} 5{\u0026#34;firstName\u0026#34;:\u0026#34;Tomothy\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Rogers\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;tomothy.rogers@example.com\u0026#34;} \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/transform-streams/tocsv/","title":"ToCSV Stream","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a transform stream, meaning a stream that exists to receive events after the source stream, do something with them and then send them on to the next pipe step, which must exist.\nIt takes each event, and turns it into a CSV line ready to be written to a CSV file.\n  When would I use this?  I want to generate a CSV file from the events flowing through the stream    Runnable Examples   Example 1 This example reads 5 events from the rstreams-example.peopleplus RStreams queue. The pipe then creates a throughAsync stream step that just takes the ReadEvent\u0026lt;Person\u0026gt; events read from the bus and turns it into a PersonLight event and sends it on to the toCSV stream to make a CSV line ready to stick in a CSV file.\nFinally, it writes the file using the Node standard filesystem fsmodule to create a sink that writes events that flow into the sink to a file. Pretty convenient.\nThe toCSV function\u0026rsquo;s first argument, if true, writes a CSV header as the first row. If the toCSV function\u0026rsquo;s first argument is an array of strings, it uses that as the CSV header first row. The second arg is options that come from the underlying fast-csv NPM module that generates the CSV file: fast-csv options.\n\r\r Example 1 code\r 1import { ReadEvent, ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import fs from \u0026#34;fs\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 const opts: ReadOptions = { 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 limit: 5 11 } 12 13 await rsdk.streams.pipeAsync( 14 rsdk.read\u0026lt;Person\u0026gt;(\u0026#39;rstreams-example.peopleplus-to-jsonlines\u0026#39;, 15 \u0026#39;rstreams-example.peopleplus\u0026#39;, opts), 16 rsdk.streams.throughAsync\u0026lt;ReadEvent\u0026lt;Person\u0026gt;, PersonLight\u0026gt;(async (p: ReadEvent\u0026lt;Person\u0026gt;) =\u0026gt; { 17 return { 18 firstName: p.payload.firstName, 19 lastName: p.payload.lastName, 20 email: p.payload.email 21 } 22 }), 23 rsdk.streams.toCSV(true, {quote: \u0026#39;\u0026#34;\u0026#39;}), 24 fs.createWriteStream(\u0026#34;./output/people.csv\u0026#34;), 25 ); 26} 27 28interface PersonLight { 29 firstName: string; 30 lastName: string; 31 email: string; 32} 33 34(async () =\u0026gt; { 35 try { 36 await main(); 37 } catch(err) { 38 console.log(err); 39 } 40})() \r\r\r\r Generated people.jsonl file\r 1firstName,lastName,email 2Herman,Morris,herman.morris@example.com 3Herman,Morris,herman.morris@example.com 4Tomothy,Rogers,tomothy.rogers@example.com 5Herman,Morris,herman.morris@example.com 6Tomothy,Rogers,tomothy.rogers@example.com \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-guides/core-concepts/event-streaming-primer/","title":"Event Streaming Primer","description":"In-depth guides and how-to&#39;s.","content":"One cannot understand the problems RStreams solves or reason about its implementation/usage without a fundamental understanding of event streaming compared to traditional microservices approaches.\n  Summary Some systems work with parties that are constantly generating new data. Client data flowing from these parties tends to flow in a sequential order that we call an event stream. The events in this stream get transformed, enriched, and used to trigger subsequent events. Event stream processing, in concert with general purpose messaging, is a loosely coupled, scalable pattern ideal for designing enterprise systems built to handle continuous data flow. RStreams is just such a system.\n"},{"url":"https://rstreams.org/rstreams-flow/","title":"RStreams Flow","description":"Be up and running in 10 minutes.","content":"RStreams Flow is an opinionated framework that makes choices on how to build, test, deploy, monitor and maintain RStreams microservices. Engineers don’t care about RStreams. They care about creating reactive microservices that leverage native AWS services at scale. RStreams Flow helps engineers just do their work.\n"},{"url":"https://rstreams.org/rstreams-flow/configuring-rstreams/","title":"Configuring RStreams","description":"","content":"TODO\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/source-streams/createsource/","title":"Create Source","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a source stream, fed by events from the array that you return from your CreateSourceFunction, to act as the first step in a pipe.\n  When would I use this?  I want to create a pipe and seed the pipe from data from a database at scale I want to create a pipe and seed the pipe with data from an API at scale I want to create a pipe and seed the pipe with data from [fill-in-the-blank] at scale    Runnable Examples   Example 1 There\u0026rsquo;s a fair bit going on here so stay with me. Here\u0026rsquo;s what we\u0026rsquo;re doing. We read events from from a public free API that generates random people, seeding a pipe by creating a source stream using the createSource API and then write them to the bus landing in the rstreams-example.people queue.\n Line 8 We define a type for the state we want the SDK to pass into each time it calls our function to generate more content to feed the stream. The createSource function lets you specify that you want state passed in with each invocation of your function and then lets you initialize that state for the first time your function is called. Then, you can change that state in your function and it will be passed to each subsequent invocation. Line 10 We are defining the options we want to pass into the createSource function. Here we are telling the SDK to close the source stream and thus shutdown the pipe after ten seconds. Line 11 We are creating an instance of our state. We only want to call out to the free public API that generates random people for us five times. So, we initialize our state to 5 and then in our actual function, below, we decrement that state. When it gets to zero, we simply return from the function which tells the SDK to close the stream and shut down the pipe. Line 14 We are creating a new source steam and specifying that the source stream will be returning arrays of PersonRaw objects and also that we are going to be asking the SDK to pass in a state object of type SourceState. Then we pass as the first argument to the createSource function an anonymous function of type CreateSourceFunction that wil be called each time the stream needs more data. There is an optional argument which is state that will be passed in by the SDK on our behalf each time our function is invoked. Lines 15 and 16 We grab the state into a local variable and then decrement that state number itself so that it will be changed on subsequent invocations to this function. Lines 17 and 18 If our counter is at 0, we don\u0026rsquo;t want to continue and so we return nothing which tells the SDK we\u0026rsquo;re don. It will close our stream which will cause the pipe to flush and then close down. Lines 20 and 21 We\u0026rsquo;re not done so lets get more data. Line 20 is a call to a function below that just makes a call out to get 100 random people objects from a public free API. Line 21 just returns the array of PersonRaw objects we got from the API. Line 23 The second argument and the third argument to the createSource function: the optional options and optional initial state respectively. Line 24 We create a write stream sink to write all events that make it to the sink to the rstreams-example.people queue doing so as the bot rstreams-example.load-people-faster.  \r\r Example 1 code\r 1import { CreateSourceOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults, PersonRaw } from \u0026#39;../lib/types\u0026#39;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 interface SourceState {numApiCalls: number;} 9 10 const opts: CreateSourceOptions = {milliseconds: 10000}; 11 const state: SourceState = {numApiCalls: 5}; 12 13 await rsdk.streams.pipeAsync( 14 rsdk.createSource\u0026lt;PersonRaw, SourceState\u0026gt;(async (state) =\u0026gt; { 15 const numApiCalls = state.numApiCalls; 16 state.numApiCalls--; 17 if (numApiCalls === 0) { 18 return; 19 } else { 20 const prr: PersonRawResults = await getRandomPeople(); 21 return prr.results; 22 } 23 }, opts, state), 24 rsdk.load(\u0026#39;rstreams-example.load-people-faster\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, 25 {records: 25, time: 5000, useS3: true}) 26 ); 27} 28 29async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 30 const NUM_EVENTS = 100; 31 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 32 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 33 34 if (status !== 200) { 35 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 36 } 37 38 return data; 39} 40 41(async () =\u0026gt; { 42 await main(); 43})() \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/enrich/","title":"Enrich","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1 Example 2        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API docs: async version | sync version\nA standalone function, meaning one that doesn\u0026rsquo;t use pipes and streams, that asks for the source and destination queues and then reads events from the source queue and writes to the destination queue, allowing you to insert a function in-between to transform the data on the way or do other computation.\n  When would I use this?  You want to read from a source queue, enrich or modify the event and send it to another queue You want to read from a source queue and aggregate events, perhaps reading one minute worth of events and then writing one event to another queue that summarizes the 1 minute of source events    Runnable Examples   Example 1 The first example illustrates code running as a bot with ID of rstreams-example.people-to-peopleplus and getting exactly two events from queue rstreams-example.people, starting at position z/2022/04/20, and then transforms each event\u0026rsquo;s JSON by dropping unwanted attributes and simplifying the JSON structure. It also calls a totally free, public API that given a country name returns the standard two-char country code which we tack on to the event after which we return the modified event which tells the SDK to push it to the rstreams-example.people-to-peopleplus queue.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\nSecond, there are actually three arguments to the transform function, even though in our example we are only using the first. What is stored in an RStreams queue is an instance of a ReadEvent where the payload attribute is the data the queue exists for. The first argument is just the payload pulled out since usually that\u0026rsquo;s all you need. The second argument is the full event from the queue with the event ID and other sometimes useful things. The third argument is only used in the callback version where you call done exactly once to trigger the callback. It\u0026rsquo;s there for backwared compat. Don\u0026rsquo;t use it on new things.\n\r\r\rReturning from an enrich async transform function  \r  throw Error\nIf you throw an error at anytime the pipe will error out and your upstream queue will not be checkpointed\n  return object\nWhatever object you return that isn\u0026rsquo;t of type Error will be treated as the event to emit\n  return Array\u0026lt;object\u0026gt;\nEach object in the array will be individually emitted as if you had called this.push(\u0026lt;object\u0026gt;, {partial: true} except the very last one in the array which will act like this this.push(\u0026lt;object\u0026gt;, {partial: false}. When you return a list of objects at once, we assume you mean for them to all work or none of them worked. So, the partial: false means the SDK will emit this events to the downstream queue but not checkpoint. Since the SDK sends the last one with partial: false the last one will both be emitted and the checkpoint updated to the event ID of that last event.\nIf you pass an empty array, that\u0026rsquo;s the same thing as if you called return true.\n  return true\nThis means I don\u0026rsquo;t want to emit an event with my return but I do want the SDK to checkpoint for me in the upstream queue. If we\u0026rsquo;re not batching, then this checkpoints the one event. If we\u0026rsquo;re batching, this checkpoints up to the final event in the batch.\n  return false\nThis means I don\u0026rsquo;t want to emint an event with my return AND I also don\u0026rsquo;t want the SDK to checkpoint for me\n  this.push\nYou may emit events by passing them in to this.push if you want to. More on this later in the Advanced use cases section* below.\n  \r \r\r\rAdvanced use cases  \rLet\u0026rsquo;s say I want to turn one event read from the upstream queue into many events in the downstream queue. Well, you can\u0026rsquo;t return multiple times from the transform function. There\u0026rsquo;s another way.\nIf your transform function uses transform: function() {} and not transform: () =\u0026gt; {} to create your function, then the this variable will be of type ProcessFunctionContext\u0026lt;U\u0026gt; - transform function type and ProcessFunctionContext types. Then you may call this.push as many times as you want to push events downstream that the SDK will pick up and send to the destination queue. Then, when you\u0026rsquo;re done, simply return true telling the SDK to checkpoint the upstream event now that you\u0026rsquo;re done.\nWe need to talk more about checkpointing. In the enrich operation the SDK assumes that for each event you consume from an upstream queue you will generate one event to send to the downstream queue. So, each time you call this.push from the transform function the SDK checkpoints the upstream event, marking that this bot has gone past that event in the upstream queue. Well, if you are turning one upstream event into multiple downstream events, you are going to call this.push multiple times to emit your many events and you don\u0026rsquo;t want to checkpoint the one upstream event until you\u0026rsquo;ve generated all the downstream events. You do this by calling the push method with the first arg as the event to emit and the second arg options partial set to true indicating that this event is one of many being emitted and it will send the partial event to the downstream queue but it won\u0026rsquo;t checkpoint. Then, when you\u0026rsquo;re done you simply return true; and it will checkpoint the event in the upstream queue.\nSee TypeScript this param typing.\n\r \r\r Example 1 code\r 1import { EnrichOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person, PersonRaw } from \u0026#34;../../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: EnrichOptions\u0026lt;PersonRaw, Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.people-to-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 outQueue: \u0026#39;rstreams-example.peopleplus\u0026#39;, 11 start: \u0026#39;z/2022/04/20\u0026#39;, 12 config: { 13 limit: 2 14 }, 15 transform: async (person: PersonRaw) =\u0026gt; { 16 const p: Person = translate(person); 17 await addCountryCode(p); 18 return p; 19 } 20 }; 21 22 await rsdk.enrichEvents\u0026lt;PersonRaw, Person\u0026gt;(opts); 23} 24 25// See next expand section for translate and addCountryCode functions 26 27(async () =\u0026gt; { 28 await main(); 29})() \r\r\r\r Example 1 addCountryCode and translate functions\r 1interface CountryCode {cca2: string;} 2 3/** 4* @param person The person to add addr.countryCode to by calling a public API to 5* turn a country name in a 2 digit country code (iso cca2) 6*/ 7async function addCountryCode(person: Person): Promise\u0026lt;void\u0026gt; { 8 const url = `https://restcountries.com/v3.1/name/${person.addr.country}?fullText=true\u0026amp;fields=cca2`; 9 const cc: CountryCode = await axios.get(url); 10 person.addr.countryCode = cc.cca2; 11} 12 13/** 14* @param p The type from the public API we want to modify 15* @returns The new type that is flatter and gets rid of some attributes don\u0026#39;t need 16*/ 17/** 18* @param p The type from the public API we want to modify 19* @returns The new type that is flatter and gets rid of some attributes don\u0026#39;t need 20*/ 21function translate(p: PersonRaw): Person { 22 return { 23 gender: p.gender, 24 firstName: p.name.first, 25 lastName: p.name.last, 26 email: p.email, 27 birthDate: p.dob.date, 28 nationality: p.nat, 29 addr: { 30 addr1: p.location.street.number + \u0026#39; \u0026#39; + p.location.street.name, 31 city: p.location.city, 32 state: p.location.state, 33 country: p.location.country, 34 postcode: p.location.postcode, 35 longitude: p.location.coordinates.longitude, 36 latitude: p.location.coordinates.latitude, 37 tzOffset: p.location.timezone.offset, 38 tzDesc: p.location.timezone.description 39 } 40 } 41} \r\rNote: Person types referenced in the examples\nAfter running this for the first time, the SDK created the restreams-exmaple.peopleplus queue and our bot showed up reading an event from the upstream queue and pushing it into the new queue and the modified event appeared in the new queue.        Example 2 This example is nearly identical to Example 1 above except that this time we are are going to use config to tell the SDK to batch up events for us so we can be more efficient. The calls out to a public API to enrich each event with the country code based on the country name. The free API we are using requires a separate API request for each country. Sure, we could try to make some kind of cache but there\u0026rsquo;s lots of cases where you can\u0026rsquo;t do this. So, we\u0026rsquo;re at risk of not being able to read and enrich events from the upstream queue fast enough to keep up if events are slamming into that upstream queue super fast.\nSo, we\u0026rsquo;re going to ask the SDK to micro-batch up events 10 at a time and then invoke our transform function with all ten at once and if it\u0026rsquo;s waited more than one second for 10 to show up then our config tells the SDK to just go ahead and invoke transform with whatever it\u0026rsquo;s got so far. Then in the enrich transform function we\u0026rsquo;re going to modify our addCountryCode function to make concurrent API requests for each person we are transforming, parallelizing the work and making it much faster so we can keep up. To make the example more interesting, we set config.limit now to 100 so we get a lot more events before we stop reading from the upstream queue. The config in the config attribute is important for specifying how long we\u0026rsquo;re meant to read from the upstream queue before we stop reading and close down shop. If you\u0026rsquo;re running in a lambda function, you\u0026rsquo;ve only got 15 min before AWS shuts down your lambda and that may sound like a long time unless you are reading from a queue that is forever getting new events shoved into it, a pretty common case. By default, if you don\u0026rsquo;t set any config to tell the SDK when to stop reading from the upstream queue, the SDK will read for up to 80% of the total time remaining for your lambda, if you are in fact running as a lambda. That then saves 20% of the time for you to finish processing.\nYou\u0026rsquo;ll notice that because we used the EnrichBatchOptions to batch things up that the transform function arguments change. That\u0026rsquo;s because the SDK isn\u0026rsquo;t invoking transform with just one object but with the batch: an array of objects.\nThe first argument is just the array of events direct from the upstream queue. The second arg is an event wrapper around the whole array of events directly from the upstream queue - not really needed except in rare use cases. The third argument is for backward compatability when using the enrich as a callback instead of using async. Please only use async going forward and so you don\u0026rsquo;t need the third arg.\nWhen we\u0026rsquo;re done enriching the events, we simply return the array of the new events to send them on their way to the destination RStreams queue. See Returning from an enrich async transform function above for more details.\n\r\r\r Example 2 code\r 1import { EnrichBatchOptions, ReadEvent, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person, PersonRaw } from \u0026#34;../../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: EnrichBatchOptions\u0026lt;PersonRaw, Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.people-to-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 outQueue: \u0026#39;rstreams-example.peopleplus\u0026#39;, 11 batch: { 12 count: 10, 13 time: 1000 14 }, 15 start: \u0026#39;z/2022/04/20\u0026#39;, 16 config: { 17 limit: 100, 18 }, 19 transform: async (people: ReadEvent\u0026lt;PersonRaw\u0026gt;[]) =\u0026gt; { 20 const newPeople: Person[] = people.map((p) =\u0026gt; translate(p.payload)); 21 await addCountryCode(newPeople); 22 return newPeople; 23 } 24 }; 25 26 await rsdk.enrichEvents\u0026lt;PersonRaw, Person\u0026gt;(opts); 27} 28 29(async () =\u0026gt; { 30 await main(); 31})() \r\r\r\r Example 2 addCountryCode and translate functions\r 1interface CountryCode {cca2: string;} 2 3/** 4* @param people The people to add addr.countryCode to by calling a public API to 5* turn a country name in a 2 digit country code (iso cca2) 6*/ 7async function addCountryCode(people: Person[]): Promise\u0026lt;void\u0026gt; { 8 const urls: string[] = people.map((el) =\u0026gt; { 9 return `https://restcountries.com/v3.1/name/${el.addr.country}?fullText=true\u0026amp;fields=cca2`; 10 }); 11 12 const ccs: CountryCode[] = (await Promise.all( 13 urls.map((url) =\u0026gt; axios.get(url)))).map((obj) =\u0026gt; (obj.data[0])); 14 15 people.forEach(function (person, i) { 16 person.addr.countryCode = ccs[i].cca2; 17 }); 18} 19 20/** 21* @param p The type from the public API we want to modify 22* @returns The new type that is flatter and gets rid of some attributes don\u0026#39;t need 23*/ 24 function translate(p: PersonRaw): Person { 25 return { 26 gender: p.gender, 27 firstName: p.name.first, 28 lastName: p.name.last, 29 email: p.email, 30 birthDate: p.dob.date, 31 nationality: p.nat, 32 addr: { 33 addr1: p.location.street.number + \u0026#39; \u0026#39; + p.location.street.name, 34 city: p.location.city, 35 state: p.location.state, 36 country: p.location.country, 37 postcode: p.location.postcode, 38 longitude: p.location.coordinates.longitude, 39 latitude: p.location.coordinates.latitude, 40 tzOffset: p.location.timezone.offset, 41 tzDesc: p.location.timezone.description 42 } 43 } 44} \r\rNote: Person types referenced in the examples\n   "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/source-streams/","title":"Source Streams","description":"","content":"\r\r\r ToC\r    \r\rThese are source streams meant to be the first step in a pipe that gets events to feed into the pipe, flowing from the source out to the next step in the pipe.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/streams-primer/","title":"Streams Primer","description":"","content":"\r\r\r ToC\r   Overview Pipes and Streams  Readable Writeable Duplex Transform or Through      \r\rThis primer provides exactly enough knowledge of streaming concepts for a developer to successfully write streaming applications using the RStreams SDK and bus. It is not intended as an exhaustive treatise on the vagaries of Node streams. We all have work to do.\n   Overview There truly is nothing new under the sun. Streaming is really nothing more than Unix pipes, albeit in a more distributed manner, invented more than 50 years ago. The RStreams Node SDK relies on streaming data in and out just exactly as Unix pipes stream together commands in POSIX-based systems.\nStreaming involves creating a series of steps in a pipe where the first step, the Source, generates the data to move through the pipe. The last step is the Sink, whose job it is to do something with the data moving through the pipe. The Sink is responsible for pulling data from the previous step, which causes data to flow in the pipe: no Sink step in the pipe means no data flows. In between the source step and the sink step may optionally be any number of Transform steps that can modify data that flows through the pipe on its way to the sink.   \r\r Care to hear why some think streams are too hard?\r Streams get a bad rap. There are some who claim learning to stream data is too hard for developers. Most who dis on streams were quoted some years ago, though you can still find some articles today. The negativity was a reaction to Java and Node and C# releasing streams and their functional programming approach, which was uber complex and often used when it shouldn\u0026rsquo;t have been.\nThis is in large measure because streams became synonymous with functional programming in Java and C# and elsewhere. Java’s streaming solution, which is how many got their first experience with streaming, is complicated, ill suited to streaming because of Java’s verbosity and feel to many developers like regular expressions: going back to one requires painstakingly decomposing what it is doing, having to understand code that is hard to read and understand.\nNode’s original streaming API was hard to understand and use and has been significantly improved over the years. Don\u0026rsquo;t worry, the RStreams Node SDK dramatically simplifies it for you.   \r\r\r\r Care to read why streams might be worth it for you?\r Why code in a series of chained steps? Sounds complicated. The answer is you turn to streaming when you are working with systems where you need to process data as it is coming in because so much data needs to flow in that you can\u0026rsquo;t wait to start sending it out.\nIt\u0026rsquo;s also applicable when you need to minimize the delay in processing lots of data. Finally, it\u0026rsquo;s a great way to creat a reactive system, where the data that flows are events that cause distributed event handlers to wake up and process them, moving and transforming them from one place to another.\n\r\r  Pipes and Streams  99% of the time, all you need to know is which RStreams SDK pipe step interface to use. This section helps you develop the mental model of a pipe so you can do that. Actually using a pipe step is brain dead simple. Don\u0026rsquo;t get overwhelmed as you don\u0026rsquo;t need to understand the actual functions of a Node Readable or Node Writable or the intracies of pipes as the SDK abstracts all that complexity for you. Here\u0026rsquo;s a good article if you want just a bit more detail but you you\u0026rsquo;ll be fine without it if you read the section below.\n As mentioned, a pipe is a set of steps that data flows through in sequence. Each step in the pipe is itself called a stream because they are meant to read/write data sequentially one after the other. Steps near the beginning of the pipe are upstream and the Sink downstream: data flows is the furthest step downstream. The pipe exists to daisy chain the stream steps together.\nIf you want the super short version, everything in a pipe must be linked together where start with a Readable followed by a Writable followed by a Readableand eventually end with a Writable as the Sink. Pipe step streams between the Source and the Sink have to be both a Writable and a Readable to allow the data to flow through the step: these in between steps are often called Through or Transform stream steps.     Readable In node, a pipe is a function and each argument is a step, thus a stream, in the pipe. The first step, the Source, must get or create data somehow. It might do this by continuosly querying a database and making the data available for the next step to grab it. Remember that each downstream step pulls data from the step before it. In other words, the Source step must be readable by the next step so it can pull data from it. So, Source steps will always be of the Node type Readable. For example, the fs.createReadStream() Node file function will create a source stream that reads data from a file.\nA Readable stream is an abstraction for a source from which data can be consumed.\nThe RStreams SDK provides extremely simple Readable interfaces to make getting data from an RStreams bus queue a breeze. These simplified RStreams SDK pipe steps are of the RStreams SDK type ReadableStream which inherits from the Node Readable.\n  Writeable The last step in a pipe, the Sink, needs to be able to do something with the data. In other words, it needs to be a step we can write to such as fs.createWriteStream() that creates a Sink stream that will write the data flowing through the pipe to a file.\nA Writable stream is an abstraction for a destination to which data can be written.\nThe RStreams SDK provides extremely simple Writable interfaces to make sending data to other resources, such as a database or Elastic Search, etc., a snap. These simplified RStreams SDK pipe steps are of the RStreams SDK type WritableStream which inherits from the Node Writable. That\u0026rsquo;s all you need to know.\n  Duplex A stream step that sits between the Source and the Sink is by definition a Duplex stream. Think of a Duplex stream like its really two streams smashed together. The input to the Duplex stream is a Writable so it can consume the data from the Readable in the step before it. The output from the Duplex stream is a Readable so the next step downstream can pull data from it.\nA Duplex stream is one that is both Readable and Writeable at the same time, e.g. a TCP socket.\n     Transform or Through A Transform stream is just a Duplex stream with a function that modifies the data or perhaps causes some other side effect and then sends the data downstream. A Transform stream is often called a Through stream.\nA Transform stream is a duplex stream that allows you to transform the data in between when it is written to the stream and later read from the stream.\nThe RStreams SDK provides extremely simple Transform interfaces to make moving data through a pipe easy. These simplified RStreams SDK pipe steps are of the RStreams SDK type TransformStream which inherits from the Node Duplex.\n   "},{"url":"https://rstreams.org/rstreams-guides/","title":"RStreams Guides","description":"In-depth guides and how-to&#39;s.","content":"This section includes guides that cover RStreams core concepts and common use cases.\n"},{"url":"https://rstreams.org/rstreams-bus/anatomy-of-bus/","title":"Anatomy of a Bus","description":"","content":"\r\r\r ToC\r    \r\rComing soon\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/offload/","title":"Offload","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1 Example 2        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API docs: async version | sync version\nA standalone function, meaning one that doesn\u0026rsquo;t use pipes and streams, that reads events from the specified source RStreams queue and then calls your transform function allowing you to do anything you want to with the data.\n  When would I use this?   You want to read from a source queue and then write it to a resource or system that isn\u0026rsquo;t another RStreams queue\n Write to a database Send data to an API    You want to read from a source queue and perform aggregations/analytics on data before sending to another system\n    Runnable Examples  This expects you\u0026rsquo;ve run the examples in the enrich Operation to populate queues with data.\n   Example 1 The first example illustrates code running as a bot with ID of rstreams-example.offload-one-peopleplus and getting exactly two events from queue rstreams-example.peopleplus, starting at position z/2022/04/20, and then simply saves each event to another system by calling that system\u0026rsquo;s API. The endpoint here is a free, public API that lets you mock out the response and just throws away your request, but works for our purposes.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\nSecond, there are actually three arguments to the transform function, even though in our example we are only using the first. What is stored in an RStreams queue is an instance of a ReadEvent where the payload attribute is the data the queue exists for. The first argument is just the payload pulled out since usually that\u0026rsquo;s all you need. The second argument is the full event from the queue with the event ID and other sometimes useful things. The third argument is only used in the callback version where you call done exactly once to trigger the callback. It\u0026rsquo;s there for backwared compat. Don\u0026rsquo;t use it on new things.\n\r\r\rReturning from an offload async transform function  \r throw Error\nIf you throw an error at anytime the pipe will error out and your upstream queue will not be checkpointed return true\nThis tells the SDK to checkpoint for me in the upstream queue read from. If we\u0026rsquo;re not batching, then this checkpoints the one event. If we\u0026rsquo;re batching, this checkpoints up to the final event in the batch return false\nThis tells the SDK not to checkpoint this event in the upstream queue read from  \r \r\r Example 1 code\r 1import { OffloadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import axios, { AxiosResponse } from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: OffloadOptions\u0026lt;Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.offload-one-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 start: \u0026#39;z/2022/04/20\u0026#39;, 11 limit: 2, 12 transform: async (person: Person) =\u0026gt; { 13 await savePerson(person); 14 return true; 15 } 16 }; 17 18 await rsdk.offloadEvents\u0026lt;Person\u0026gt;(opts); 19} 20 21interface PostResponse { 22 success: boolean; 23} 24 25/** 26* @param person Save the person to another system. 27*/ 28async function savePerson(person: Person): Promise\u0026lt;void\u0026gt; { 29 const url = `https://run.mocky.io/v3/83997150-ab13-43da-9fb9-66051ba06c10?mocky-delay=500ms`; 30 const {data, status}: AxiosResponse\u0026lt;PostResponse, any\u0026gt; = await axios.post\u0026lt;PostResponse\u0026gt;(url, person); 31 if (status !== 200 || !data || data.success !== true) { 32 throw new Error(\u0026#39;Saving person to external system failed\u0026#39;); 33 } 34} 35 36(async () =\u0026gt; { 37 await main(); 38})() \r\rNote: Person types referenced in the examples\n  Example 2 This example is nearly identical to Example 1 above except that this time we are are going to use config to tell the SDK to batch up events for us so we can be more efficient. The calls out to a public API to save the event elsewhere are intentionally delayed by 500ms each, a not uncommon API latency. So, we\u0026rsquo;re at risk of not being able to read and offload events from the upstream queue fast enough to keep up if events are slamming into that upstream queue super fast.\nSo, we\u0026rsquo;re going to ask the SDK to micro-batch up events 10 at a time and then invoke our transform function with all ten at once and if it\u0026rsquo;s waited more than one second for 10 to show up then our config tells the SDK to just go ahead and invoke transform with whatever it\u0026rsquo;s got so far. Then in the offload transform function we\u0026rsquo;re going to modify our savePerson function to make concurrent POST API calls for each person we are saving, parallelizing the work and making it much faster so we can keep up. To make the example more interesting, we set limit now to 100 so we get a lot more events before we stop reading from the upstream queue. The config that is inherited from the ReadOptions is important for specifying how long we\u0026rsquo;re meant to read from the upstream queue before we stop reading and close down shop. If you\u0026rsquo;re running in a lambda function, you\u0026rsquo;ve only got 15 min before AWS shuts down your lambda and that may sound like a long time unless you are reading from a queue that is forever getting new events shoved into it, a pretty common case. By default, if you don\u0026rsquo;t set any config to tell the SDK when to stop reading from the upstream queue, the SDK will read for up to 80% of the total time remaining for your lambda, if you are in fact running as a lambda. That then saves 20% of the time for you to finish processing.\nYou\u0026rsquo;ll notice that because we used the OffloadBatchOptions to batch things up that the transform function arguments change. That\u0026rsquo;s because the SDK isn\u0026rsquo;t invoking transform with just one object but with the batch: an array of objects.\nThe first argument is just the array of events direct from the upstream queue. The second arg is an event wrapper around the whole array of events directly from the upstream queue - not really needed except in rare use cases. The third argument is for backward compatability when using the offload as a callback instead of using async. Please only use async going forward and so you don\u0026rsquo;t need the third arg.\nWhen we\u0026rsquo;re done offloading the events, we simply return true telling the SDK to checkpoint for us in the upstream queue. See Returning from an offload async transform function above for more details.\nNote: Person types referenced in the examples\n\r\r Example 2 code\r 1import { OffloadBatchOptions, ReadEvent, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import axios, { AxiosResponse } from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: OffloadBatchOptions\u0026lt;Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.offload-one-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 batch: { 11 count: 10, 12 time: 1000 13 }, 14 start: \u0026#39;z/2022/04/20\u0026#39;, 15 limit: 2, 16 transform: async (people: ReadEvent\u0026lt;Person\u0026gt;[]) =\u0026gt; { 17 await savePeople(people); 18 return true; 19 } 20 }; 21 22 await rsdk.offloadEvents\u0026lt;Person\u0026gt;(opts); 23} 24 25interface PostResponse {success: boolean;} 26interface PostResponseStatus extends PostResponse {status: number} ; 27 28/** 29* @param person Save the person to another system. 30*/ 31async function savePeople(people: ReadEvent\u0026lt;Person\u0026gt;[]): Promise\u0026lt;void\u0026gt; { 32 const url = `https://run.mocky.io/v3/83997150-ab13-43da-9fb9-66051ba06c10?mocky-delay=500ms`; 33 34 const responses: PostResponseStatus[] = (await Promise.all( 35 people.map((person) =\u0026gt; axios.post\u0026lt;PostResponse\u0026gt;(url, person.payload)))).map((obj) =\u0026gt; { 36 return {status: obj.status, success: obj.data ? obj.data.success : false}; 37 }); 38 39 responses.forEach((resp) =\u0026gt; { 40 if (resp.status !== 200 || resp.success !== true) { 41 throw new Error(\u0026#39;Saving person to external system failed\u0026#39;); 42 } 43 }); 44} 45 46(async () =\u0026gt; { 47 await main(); 48})() \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/","title":"SDK APIs","description":"","content":"\r\r\r ToC\r   Overview Standalone Operations Functions to Create SDK Streams  checkpoint : WritableStream read : ReadableStream createSource : ReadableStream write : TransformStream load : WritableStream   Utility Functions    \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n \r\r Person types referenced in the examples\r 1export interface Person { 2 gender: string; 3 firstName: string; 4 lastName: string; 5 email: string; 6 birthDate: string; 7 nationality: string; 8 addr: { 9 addr1: string; 10 city: string; 11 state: string; 12 country: string; 13 countryCode?: string; 14 postcode: number; 15 longitude: string; 16 latitude: string; 17 tzOffset: string; 18 tzDesc: string; 19 } 20} 21 22export interface PersonRaw { 23 gender: string; 24 name: { 25 title: string; 26 first: string; 27 last: string; 28 } 29 location: { 30 street: { 31 number: number; 32 name: string; 33 } 34 city: string; 35 state: string; 36 country: string; 37 postcode: number; 38 coordinates: { 39 longitude: string; 40 latitude: string; 41 } 42 timezone: { 43 offset: string; 44 description: string; 45 } 46 } 47 email: string; 48 dob: { 49 date: string; 50 age: number; 51 } 52 nat: string; 53} 54 55export interface PersonRawResults { 56 results: PersonRaw[]; 57} \r\r  Overview The RStreams Node SDK includes a simple utility function to create to create pipes and nearly every kind of stream you\u0026rsquo;d need to work with massive amounts of continuously generated data in an instance of the RStreams bus. It also includes functions to allow you to skip the complexity of dealing with pipes and streams at all for the most common use cases: getting data from the bus and sending data to the bus.\n  Standalone Operations These powerful standalone operations, meaning without needing to use pipes and streams, do some heavy lifting for you to hide all the complexity of sending events to and getting events from the RStreams bus.\n put Operation A function that lets you write a single event to the specified RStreams queue enrich Operation A function that reads from the specified source RStreams queue, lets you transform the events and then sends the modified events to the specified destination RStreams queue offload Operation A function that reads from the specified RStreams queue and lets you do something with the events retrieved, perhaps save them in a DB    Functions to Create SDK Streams The following lists each SDK function that creates an instance of an SDK stream for you. If you need to do something with a pipe and its streams, there\u0026rsquo;s almost certainly a helper function to create the exact pipe step you need with the config you need to make it work in your use case.\n  checkpoint : WritableStream TODO A function that creates a Writable stream. TODO: link to checkpointing https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#checkpoint\n  read : ReadableStream https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#read\nA function that lets you create a source stream that is backed by events you specify from an RStreams queue, with additional config to make it flexible, intelligent and performant.\n  createSource : ReadableStream TODO\nhttps://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#createSource\nA function that creates a source stream that you can use to generate continuously generated, arbitrary content whether from a database, an API, a file or anything.\n  write : TransformStream https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#write\nA function that creates a pipe stream step that can sit somewhere between a source and sink stream and write the data that passes through the stream step to an RStreams queue while still allowing the data to pass through the step stream to the next downstream step in the pipe. This is useful if you want to siphon off some data to go to a given queue mid-pipe while you also want to send it on to do some other work.\n  load : WritableStream TODO\nA function that creates a sink step stream that takes the data flowing through the pipe and sends it to an RStreams queue in an intelligent manner.\nhttps://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#load\n  Utility Functions "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/sink-streams/","title":"Sink Streams","description":"","content":"\r\r\r ToC\r    \r\rThese are source streams that will be the final step in your pipe, consuming the events and doing something with them such as write them to Dynamo DB.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/","title":"RStreams Node SDK","description":"The smart SDK for Node/Typescript.","content":"This is the RStreams Node SDK, a client-side library designed to interact with instances of an RStreams Bus.\nThis assumes you\u0026rsquo;ve got an RStreams Bus instance running to connect to. If you don\u0026rsquo;t, head on over to the RStreams Bus section first. \nAlso, the RStreams Bus gets your Node/Typescript project setup in a jiffy with the right SDK config, ready to run local, debug and deploy if you haven\u0026rsquo;t already done that.\n   Are you setup to run the examples? \r\r\r\rExpand this section if you\u0026rsquo;re not sure  \rAll examples in the SDK documentation assume that when these apps run, the RStreams SDK can discover the configuration it needs. The config it needs is the AWS resource IDs of the RStreams Bus instance deployed in your AWS account. Things like the ID of the kinesis stream used by the bus and so on.\nOf course, in a production environment the SDK will get the config in an intelligent and safe manner, say from AWS Secrets Manager. See the RStreams Flow Configuring RStreams doc.\nHere\u0026rsquo;s the typescript type of the config.\n  Get the config You will first need to get this config. By default, the RStreams Bus puts a secret in secrets manager that is the JSON config blob. The secret will be named rstreams-\u0026lt;bus name\u0026gt;. Go get the JSON config from this secret.\n  Save the config   As a file Create a file named rstreams.config.json and put it in the same directory you are running your app in or in any parent director and the SDK will just find it and use it.\n  As an environment variable Create an environment variable named RSTREAMS_CONFIG whose value is the config JSON blob.\n  As an argument to the SDK itself Create a variable in the code that is the config and then pass it into the SDK\u0026rsquo;s constructor.\n1 2const RSTREAMS_BUS_CONFIG: ConfigurationResources = { 3 \u0026#34;Region\u0026#34;: \u0026#34;some-value\u0026#34;, 4 \u0026#34;LeoStream\u0026#34;: \u0026#34;some-value\u0026#34;, 5 \u0026#34;LeoCron\u0026#34;: \u0026#34;some-value\u0026#34;, 6 \u0026#34;LeoSettings\u0026#34;: \u0026#34;some-value\u0026#34;, 7 \u0026#34;LeoEvent\u0026#34;: \u0026#34;some-value\u0026#34;, 8 \u0026#34;LeoKinesisStream\u0026#34; : \u0026#34;some-value\u0026#34;, 9 \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;some-value\u0026#34;, 10 \u0026#34;LeoS3\u0026#34;: \u0026#34;some-value\u0026#34; 11}; 12 13const rsdk: RStreamsSdk = new RStreamsSdk(RSTREAMS_BUS_CONFIG); \r   Do you know how to access Botmon? \r\r\r\rExpand this section if you\u0026rsquo;re not sure  \rBotmon is a visualization, monitoring and debugging tool that installs with an instance of the RStreams as a website. Most examples will have you use Botmon to visualize what\u0026rsquo;s happening and to help diagnose issues.\nTODO: how do they know how to access botmon?\n\r The examples in this section are geared toward creating apps that use the RStreams SDK to interact with an RStreams bus instance regardless of how those apps are written. As such, the examples are simply standalone runnables node applications. The examples highlight the config that are available to control reading and writing in such apps.\nThe RStreams Flow section focuses on apps written specifically as bots that are deployed as lambda functions and go into great detail on the specific use cases applicable to serverless applications.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/sink-streams/devnull/","title":"Devnull","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a sink stream whose purpose is simply to pull events downstream and do nothing with them. All pipes have to have a sink or nothing flows in the pipe since the sink pulls data along from the upstream step before it and then that step pulls from its antecedent and so on. So, no sink means nothing moves in the pipe. However, you don\u0026rsquo;t always want your sink to actually do work like write to a file or to a database or another queue and so devnull is your answer.\n  When would I use this?  When you have a pipe where all you want to do is log data moving through the pipe When you have a pipe that does processing in one of the stream steps before the sink    Runnable Examples   Example 1 This example uses the very popular event-stream Node library, which is exported via the SDK it\u0026rsquo;s used so much, to turn a hard-coded array into a source stream to feed the pipe.\nThen, devnull is used since we don\u0026rsquo;t really want to do anything more than log the events moving through the stream.\nThe argument to devnull, if true, will log events that come to the sink. You can also pass a string in which tells the SDK to log events and starts each event in the console output with the string you provided and not the word \u0026ldquo;devnull\u0026rdquo; which is the default behavior.\n\r\r Example 1 code\r 1import { RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2 3async function main() { 4 const rsdk: RStreamsSdk = new RStreamsSdk(); 5 6 const DATA: SoftDrink[] = [ 7 {name: \u0026#39;Pepsi\u0026#39;, yearInvented: 1893}, 8 {name: \u0026#39;Coca-Cola\u0026#39;, yearInvented: 1886}, 9 {name: \u0026#39;Dr Pepper\u0026#39;, yearInvented: 1885}, 10 {name: \u0026#39;Hires Root Beer\u0026#39;, yearInvented: 1876}, 11 {name: \u0026#39;Vernors Ginger Ale\u0026#39;, yearInvented: 1866}, 12 {name: \u0026#39;Schweppes\u0026#39;, yearInvented: 1783} 13 ] 14 15 await rsdk.streams.pipeAsync( 16 rsdk.streams.eventstream.readArray(DATA), 17 rsdk.streams.devnull(true) 18 ); 19} 20 21interface SoftDrink { 22 name: string; 23 yearInvented: number; 24} 25 26(async () =\u0026gt; { 27 try { 28 await main(); 29 } catch(err) { 30 console.log(err); 31 } 32})() \r\r\r\r Example 1 console output\r 1➜ rstreams-runnable-examples ts-node apps/devnull-stream.ts 2devnull { 3 \u0026#34;name\u0026#34;: \u0026#34;Pepsi\u0026#34;, 4 \u0026#34;yearInvented\u0026#34;: 1893 5} 6devnull { 7 \u0026#34;name\u0026#34;: \u0026#34;Coca-Cola\u0026#34;, 8 \u0026#34;yearInvented\u0026#34;: 1886 9} 10devnull { 11 \u0026#34;name\u0026#34;: \u0026#34;Dr Pepper\u0026#34;, 12 \u0026#34;yearInvented\u0026#34;: 1885 13} 14devnull { 15 \u0026#34;name\u0026#34;: \u0026#34;Hires Root Beer\u0026#34;, 16 \u0026#34;yearInvented\u0026#34;: 1876 17} 18devnull { 19 \u0026#34;name\u0026#34;: \u0026#34;Vernors Ginger Ale\u0026#34;, 20 \u0026#34;yearInvented\u0026#34;: 1866 21} 22devnull { 23 \u0026#34;name\u0026#34;: \u0026#34;Schweppes\u0026#34;, 24 \u0026#34;yearInvented\u0026#34;: 1783 25} \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/read-write-scale/","title":"Read/Write at Scale","description":"","content":"\r\r\r ToC\r   Overview Reading at Scale  App Use Cases and Considerations Config to the Rescue   Writing at Scale  Considerations Config to the Rescue      \r\rYou need to understand what a pipe and stream step in a pipe are.\n   Overview The RStreams Node SDK includes a simple utility function to create pipes and nearly every kind of stream you\u0026rsquo;d need to work with massive amounts of continuously generated data in an instance of the RStreams bus. It also includes functions to allow you to skip the complexity of dealing with pipes and streams at all for the most common use cases: getting data from the bus and sending data to the bus.\n  Reading at Scale You want to read from an RStreams queue. What do you need to consider to ensure you do that efficiently and responsibly at massive scale?\n  App Use Cases and Considerations What kind of app are you making?\n  CASE 1: Are you writing an app that runs once in a while, pulling events from a specific start/end range in the queue?\nMaybe you are writing an app to recover from a failure somewhere in your enterprise and so your app gets a start/end date of events that needs to be re-processed from the queue and it is manually kicked off.\nOr maybe you\u0026rsquo;re writing an app to sample data in a queue as part of monitoring and health checks that gets kicked off on a cron every five minutes to read a few events and go back to sleep.\n  CASE 2: Are you writing an app that runs continously as a daemon, pulling new events from the queue as fast as they show up?\nYou care about each and every event and you want to get each one in order from the queue and process it. If events are pushed into the queue faster than you can read them and process them then you\u0026rsquo;re in trouble because the number of events in the queue that are waiting for you to grab and process will grow unbounded. This means that the data you are processing is forever getting older and older and isn\u0026rsquo;t being processed in near real-time, seconds to a few minutes typically.\nAlso, what happens if your daemon crashes? You will need to restart it and keep reading from where you left off.\n  CASE 3: Are you writing a serverless app that has to shut down every 15 minutes as an AWS lambda function and get restarted and keep going?\nLet\u0026rsquo;s assume that this is just CASE 2 above but instead of a daemon it\u0026rsquo;s a lambda function. You can\u0026rsquo;t miss an event and you need to process them efficiently. You need to make sure you leave enough time to complete processing the events you have and know for sure where you left off before your lambda gets restarted.\n  How much processing are you doing and what latency is acceptable?\nThe more events that are pushed into a queue per unit time the more efficiently your app needs to be able to read and process these events. Reading events from a queue is lightning, but what if you need to call out to an API to get data to enrich each and every event? What if you need to hit a database for each and every event? That\u0026rsquo;s going to slow everything down and could make you upside down in that you can\u0026rsquo;t process events as fast as they are being pushed into a queue.\nHow big are the data events you are reading?\nLarge events can\u0026rsquo;t flow through many of AWS\u0026rsquo;s services. The RStreams SDK will detect this and push them to S3 and write an event that flows into the stream that actually points to the events stored in a file in S3. The SDK handles all of this transparently and you won\u0026rsquo;t even be aware you are reading from S3. However, the larger the events the more this is going to happen and the more time it could take to read events from S3 if those events are striped to hell and back in individual S3 files.\n  Config to the Rescue RStreams includes config in read operations to let you tune reading based upon your specific uses cases.\nThe following applies to the enrich, offloadEvents and read operations.\nReadOptions Interface\nNote there are other options not listed below that are less often needed but might be interesting in some rare cases to fine tune performance such as stream_query_limit, size or loops.\n  fast_s3_read\n Problem reading events is slow, likely because there\u0026rsquo;s lots of small S3 files the SDK is reading events from Solution set this to true and the SDK will read concurrently from multiple S3 files and your reads will be much faster - will default to on in Q3 2022 (you can control how much is ready concurrently if you need fine-grained control, which you likely won\u0026rsquo;t, using fast_s3_read_parallel_fetch_max_bytes)    runTime | stopTime\n Problem your lambda function (bot) is shutting down after 15 minutes instead of ending gracefully because it is endlessly reading events from a queue Solution tell the RStreams read operation you are using to end after runTime number of milliseconds and set that to be 75-80% of the amount of time the lambda has left before it runs out of time before AWS shuts it down forcefully or calculate the exact stopTime that saves roughly 20% of the 15 min shutdown window for the pipe to complete processing, flush and checkpoint.    start\n Problem I don\u0026rsquo;t want to read the latest events, I want to start from a specific position in the queue Solution use the start attribute to specify the event ID of when to start    maxOverride\n Problem I don\u0026rsquo;t want to keep reading events forever, I want to stop at a certain time in the queue Solution use the maxOverride attribute to specify the event ID of when to stop    BatchOptions Interface\nThese don\u0026rsquo;t control reading from a queue but allow you to hold on to a group of events and present those events all at once to the next stream step in the pipe, a concept called micro-batching.\n bytes | count | time  Problem It\u0026rsquo;s taking me longer and longer to process events and I can\u0026rsquo;t keep up with new events coming into the queue and so reading is getting further and further behind Solution Try micro-batching using these attributes to group of events in small batches that are sent to the next pipe stream step all at once and then rewrite whatever your code is doing in that pipe stream step to do it in paralled: if writing to a DB write the entire batch in one SQL query; if reading from a DB, do one read to get all the data you need for all the events in the batch; if hitting an API use Promise.all to run each API request in parallel for the batch. NOTE, if you just can\u0026rsquo;t keep up no matter what, consider implementing Fanout    BufferOptions Interface\nThese serve the same purpose as the BatchOptions Interface above and solve the same problem. The difference is that BatchOptions are built into an RStreams operation to let you control it while BufferOptions is used with the Buffer pipe stream step operation that may be inserted into the pipe to choose to micro-batch events before flowing to the next pipe stream step. The attribute names are named slightly differently but are identical in purpose and function.\nToCheckpointOptions Interface\nHead over to the checkpointing article if you don\u0026rsquo;t know what a checkpoint is or what it\u0026rsquo;s used for.\n records | time  Problem I can\u0026rsquo;t ever re-process an event and so I need to checkpoint after I process each and every event Problem I\u0026rsquo;m OK if I re-process some events in the rare case of a failure and so I only want to checkpoint after so much time or so many records Solution Use these attributes to control checkpointing in a stream (see the checkpoint operation)      Writing at Scale You want to write to an RStreams queue. What do you need to consider to ensure you do that efficiently and responsibly at massive scale?\n  Considerations What\u0026rsquo;s really happening underneath the covers with a write?\nThe SDK is writing to either Kinesis, S3 and Firehose and S3 followed by Kinesis. See the Anatomy of a Bus article for more on this.\nSo, that means Kinesis has limitations on the size of events and how much data you can concurrently write to kinesis at once without having to jump through hoops.\nAm I getting data to write onesie twosie or all at once in big batches?\nPerhaps you are receiving a file from a customer where each row in the file is an object you want send into an RStreams queue or are you getting data in an event driven manner and the flow of those events can\u0026rsquo;t be predicted but is likely either coming one at a time or in a micro-batch.\n  Config to the Rescue RStreams includes config in write operations to let you tune writing based upon your specific uses cases.\nThe following applies to the load, offloadEvents and write operations.\nWriteOptions Interface\n  useS3\n Problem I have lots of events to send to an RStreams queue all at once and it\u0026rsquo;s slow Solution Set the useS3 option to true and the SDK will write a file chock full of events, many thousands is normal, and then send one event through kinesis that points back to the S3 file Problem It\u0026rsquo;s taking too long to read events Solution Wait. Why is this here in the write section? The reason is that how you write can affect how you read. If you write tons and tons and small S3 files, say with one event each, that\u0026rsquo;s going to affect read performance since the SDK will have to make many calls to S3 to read a small number of events. Yes, there\u0026rsquo;s a new fast_s3_read capability that will read multiple files at once that makes this much better but still it can be an issue. So the solution is to be smart about your use of the useS3 attribute. Be sure you micro-batch successfully if you use it, meaning that there is enough data available to be written all at once using the batch or buffer options listed above.    firehose\n Problem My event handler that writes to an RStreams queue is invoked one at a time by the nature of how it runs and the pace at which events come in that I want to write and so I\u0026rsquo;m writing lots of individual events that flow through kinesis and take up concurrent write bandwidth Solution Set firehose to true. Firehose will automatically micro-batch events for us in one minute increments, writing them to an S3 file which will then get sent to kinesis as one event. This does mean that ingestion will be delayed by up to a minute, so this will only work in use cases where this is acceptable.    records | size | time\n Problem I don\u0026rsquo;t want to inundate kinesis with events going one at a time but I need control over how group up events and send as a micro-batch to kinesis because ingestion time matters Solution Use one of these attributes, and probably all of them, to control how long to wait before the SDK micro-batches up events, zips them as a single blob and sends them to kinesis, which performs like a champ. Set number of events, the max size of the events and the max time to wait and the max number of events to wait and whichever is tripped first will cause the micro-batch to be sent as is.    "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/transform-streams/","title":"Transform Streams","description":"","content":"\r\r\r ToC\r    \r\rThese are streams that sit between the source and sink, consuming events from the upstream stream, doing something with the events, perhaps changing them, and then sending the events on to the next downstream pipe step.\n"},{"url":"https://rstreams.org/rstreams-bus/","title":"RStreams Bus","description":"RStreams event bus, built with native AWS services.","content":"The RStreams Bus is a light-weight framework that uses AWS Kinesis, S3, Lambda and Dynamo DB to create an event-streaming and general purpose messaging platform.\n"},{"url":"https://rstreams.org/rstreams-botmon/","title":"RStreams Monitoring","description":"Monitor, trace and debug events in the bus in real-time.","content":"RStreams includes a webapp called Botmon that provides real time monitoring, data visualization, tracing and debugging.\n"},{"url":"https://rstreams.org/rstreams-guides/core-concepts/","title":"Core Concepts","description":"In-depth guides and how-to&#39;s.","content":"Start here to get an understanding of core concepts that govern the RStreams platform. The Event Streaming Primer is a realy good place to start.\n"},{"url":"https://rstreams.org/","title":"RStreams","description":"","content":""},{"url":"https://rstreams.org/categories/","title":"Categories","description":"","content":""},{"url":"https://rstreams.org/changelog/","title":"Changelog posts","description":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.","content":"  February Updates Feb 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nChanged\r  Better support for using applying additional filters to posts_tax_query for categories for custom WordPress syncs\n  Reporting fine-tuning for speed improvements (up to 60% improvement in latency)\n  Replaced login / registration pre-app screens with a cleaner design\n   Removed\r Removed an issue with the sync autolinker only interlinking selectively. Removed up an issue with prematurely logging out users   \r  March Updates Mar 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nAdded\r Some scheduled changelogs, tweets, and slack messages queued up this weekend and were not published on time. We fixed the issue and all delayed publications should be out. We now prioritize keywords over title and body so customers can more effectively influence search results Support form in the Assistant is now protected with reCaptcha to reduce spam reinitializeOnUrlChange added to the JavaScript API to improve support for pages with turbolinks   Fixed\r Fixed an issue with the sync autolinker only interlinking selectively. Fixed up an issue with prematurely logging out users   \r  Changelog label \rAdded\r Changed\r Depricated\r Removed\r Fixed\r Security\r Unreleased\r "},{"url":"https://rstreams.org/contact/","title":"Got Any Questions","description":"this is meta description","content":""},{"url":"https://rstreams.org/rstreams-node-sdk/api-docs/","title":"SDK API Docs","description":"The generated RStreams Node SDK API Docs","content":"Jump over to the RStreams Node SDK API Docs.\n"},{"url":"https://rstreams.org/search/","title":"Search Result","description":"this is meta description","content":""},{"url":"https://rstreams.org/tags/","title":"Tags","description":"","content":""}]