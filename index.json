[{"url":"https://rstreams.org/rstreams-guides/core-concepts/fundamentals/","title":"Fundamentals","description":"Fundamental concepts.","content":"Events Event ID TODO\nPipeline (Pipe) TODO\nPipeline Step TODO\nStream Pipeline Step or Bots/Queues. TODO\nCheckpoint Writing last read event ID back to queue. TODO\nEvent source timestamp Ancestors and derivatives and the value it provides TODO\nStarted timestamp (bot) TODO\nEnded timestamp (bot) TODO\nCorrelation ID (bot) TODO\nUnits (bot) TODO\nCheckpoint TODO\n"},{"url":"https://rstreams.org/why-rstreams/","title":"Why RStreams?","description":"What is RStreams and why should I use it?","content":"Why RStreams? Why not just AWS services? Less Friction AWS services rock. Each service started life to serve a specific purpose and then grew in size and scope, becoming useful to more and more use cases. Yet, each service was born and iterated from a starting foundation, giving each a sweet spot for where it excels and step outside this sweet spot and friction emerges.\nTODO: talk about why we\u0026rsquo;d use queues to solve problems and the reactive mindset\n"},{"url":"https://rstreams.org/rstreams-guides/checkpointing/","title":"Checkpointing","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-guides/fanout/","title":"Fanout","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-botmon/getting-started/","title":"Getting Started","description":"","content":"Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet, tempus metus quis, pharetra turpis. Phasellus at massa sit amet ante semper fermentum sed eget lectus. Quisque id dictum magna turpis.\n Etiam vestibulum risus vel arcu elementum eleifend. Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet\n Etiam vestibulum risus vel arcu elementum eleifend. Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet, tempus metus quis, pharetra turpis. Phasellus at massa sit amet ante semper fermentum sed eget lectus. Quisque id dictum magna, et dapibus turpis.Etiam vestibulum risus vel arcu elementum eleifend. Cras at dolor eget urna varius faucibus tempus in elit. Cras a dui imperdiet, tempus metus quis, pharetra turpis. Phasellus at massa sit amet ante semper fermentum sed eget lectus. Quisque id dictum magna, et dapibus turpis.\n"},{"url":"https://rstreams.org/rstreams-bus/getting-started/","title":"Getting Started","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-flow/getting-started/","title":"Getting Started","description":"","content":"Coming soon\n"},{"url":"https://rstreams.org/rstreams-node-sdk/getting-started/","title":"Getting Started","description":"","content":"\r ToC\r   Write a single object to the bus Write multiple objects to the bus Stream multiple objects to the bus fast    \r Are you setup to run the examples? Expand this section if you\u0026rsquo;re not sure  \rAll examples in the SDK documentation assume that when these apps run, the RStreams SDK can discover the configuration it needs. The config it needs is the AWS resource IDs of the RStreams Bus instance deployed in your AWS account. Things like the ID of the kinesis stream used by the bus and so on.\nOf course, in a production environment the SDK will get the config in an intelligent and safe manner, say from AWS Secrets Manager. See the RStreams Flow Configuring RStreams doc.\nHere\u0026rsquo;s the typescript type of the config.\nGet the config You will first need to get this config. By default, the RStreams Bus puts a secret in secrets manager that is the JSON config blob. The secret will be named rstreams-\u0026lt;bus name\u0026gt;. Go get the JSON config from this secret.\nSave the config As a file Create a file named rstreams.config.json and put it in the same directory you are running your app in or in any parent director and the SDK will just find it and use it.\nAs an environment variable Create an environment variable named RSTREAMS_CONFIG whose value is the config JSON blob.\nAs an argument to the SDK itself Create a variable in the code that is the config and then pass it into the SDK\u0026rsquo;s constructor.\n1 2const RSTREAMS_BUS_CONFIG: ConfigurationResources = { 3 \u0026#34;Region\u0026#34;: \u0026#34;some-value\u0026#34;, 4 \u0026#34;LeoStream\u0026#34;: \u0026#34;some-value\u0026#34;, 5 \u0026#34;LeoCron\u0026#34;: \u0026#34;some-value\u0026#34;, 6 \u0026#34;LeoSettings\u0026#34;: \u0026#34;some-value\u0026#34;, 7 \u0026#34;LeoEvent\u0026#34;: \u0026#34;some-value\u0026#34;, 8 \u0026#34;LeoKinesisStream\u0026#34; : \u0026#34;some-value\u0026#34;, 9 \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;some-value\u0026#34;, 10 \u0026#34;LeoS3\u0026#34;: \u0026#34;some-value\u0026#34; 11}; 12 13const rsdk: RStreamsSdk = new RStreamsSdk(RSTREAMS_BUS_CONFIG); \r Principle Operations Write\nYou\u0026rsquo;re going to want to write to the bus, meaning send a data event to a specific queue of the bus. Queues maintain their order, with the newest at the front of the queue and the oldest data at the back of the queue.\nRead\nYou\u0026rsquo;re going to want to read from the bus, meaning read events from a queue of the bus. You typically read from the last place you read from last in a queue. Or, if this is your bot\u0026rsquo;s first time reading from a queue then the oldest event in the queue is the default. Or, you can read events in a specific range back in time in the queue.\nTransform\nYou\u0026rsquo;re going to want to read from the bus, change the data somehow or cause a side effect like writing to some database, and then write the changed data to a different queue.\nWrite to the bus TODO: include link to git project so can checkout and run\nWrite a single object to the bus Let\u0026rsquo;s say we want to populate an RStreams queue with people we retrieve from an API that generates random people. The steps to do that are\n Line 6 : Create an instance of the SDK Line 7 : Go get a single random person from a public API using the Axios library Line 8 : Call the putEvent SDK API to send an event up to the RStreams Bus  The first argument is the ID of the bot this code is running as The second argument is the ID of the RStreams queue to send the event to The third argument is the JSON object to send    1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRaw, PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const person = await getRandomPerson(); 8 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 9} 10 11async function getRandomPerson(): Promise\u0026lt;PersonRaw\u0026gt; { 12 const NUM_EVENTS = 1; 13 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 14 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 15 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 16 17 if (status !== 200) { 18 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 19 } 20 21 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 22 23 return data.results[0]; 24} 25 26(async () =\u0026gt; { 27 await main(); 28})() \r PersonRaw \u0026amp; PersonRawResults interfaces\r 1export interface PersonRaw { 2 gender: string; 3 name: { 4 title: string; 5 first: string; 6 last: string; 7 } 8 location: { 9 street: { 10 number: number; 11 name: string; 12 } 13 city: string; 14 state: string; 15 country: string; 16 postcode: number; 17 coordinates: { 18 longitude: string; 19 latitude: string; 20 } 21 timezone: { 22 offset: string; 23 description: string; 24 } 25 } 26 email: string; 27 dob: { 28 date: string; 29 age: number; 30 } 31 nat: string; 32} 33 34export interface PersonRawResults { 35 results: PersonRaw[]; 36} \r View results in Botmon\nIf you go to Botmon, you will see that the rstreams-example.people queue now has an event in it. Expand for Botmon screenshots  \r  Go to Botmon and search for rstreams-example.people in the search field   Botmon now shows a visual representation of the bot and the queue, click on the gear icon after hovering over the queue and then click on Events   Botmon now shows the events loaded into the queue   \r\nWrite multiple objects to the bus So, instead of reading one person from the public API we used in the example above, let\u0026rsquo;s say we get 100 people at a time from the public API and we want to write them to the bus. Here\u0026rsquo;s what that looks like.\n1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const people = await getRandomPeople(); 8 9 //HINT: this will have very bad performance. This is just to illustrate a point. 10 // Don\u0026#39;t use putEvent in a loop this way in practice! 11 for (const person of people.results) { 12 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 13 } 14} 15 16async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 17 const NUM_EVENTS = 100; 18 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 19 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 20 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 21 22 if (status !== 200) { 23 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 24 } 25 26 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 27 28 return data; 29} 30 31(async () =\u0026gt; { 32 await main(); 33})() The only difference in this example is that we pass in 100 to the public API, getting back 100 objects as an array. We then loop through them, making a connection to the RStreams Bus for each and every event. It\u0026rsquo;s simple and it works but this is bad. The putEvent API is really only meant for one or maybe a handful of events. To understand why, consider what the RStreams SDK is doing when you call putEvent.\n It\u0026rsquo;s opening a connection to AWS Kinesis It sending the single event on that connection each time to Kinesis The event flows through Kinesis until an RStreams Kinesis processor reads the single event and writes it to the RStreams Dynamo DB queue table, putting the event in the correct queue  RStreams is designed to handle the continuos generation of data events that flow into a given queue, is read from that queue and mutated and then sent to other queues. It is today doing this with very large amounts of concurrently received events. The RStreams SDK has a better way to work with sending larger amounts of data to the bus, meaning to an RStreams queue.\nStream multiple objects to the bus fast It\u0026rsquo;s time to tackle the idea of streams. If you aren\u0026rsquo;t well versed on streams, jump over and read the Streams Primer. It\u0026rsquo;s short and sweet and may well convert you to streams if you aren\u0026rsquo;t already.\n1import { RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const es = rsdk.streams.eventstream; 8 const people = await getRandomPeople(); 9 10 rsdk.streams.pipeAsync( 11 es.readArray(people.results), 12 rsdk.load(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, 13 {records: 25, time: 5000, useS3: true}) 14 ); 15} 16 17async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 18 const NUM_EVENTS = 100; 19 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 20 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 21 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 22 23 if (status !== 200) { 24 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 25 } 26 27 return data; 28} 29 30(async () =\u0026gt; { 31 await main(); 32})() "},{"url":"https://rstreams.org/why-rstreams/getting-started/","title":"Getting Started","description":"","content":"RStreams is a server-side bus installed in an AWS account and an SDK for creating reactive applications that use the bus and a monitoring web app called Botmon. RStreams Flow is an opinionated framework to make getting a client-side project up and running super easy.\n"},{"url":"https://rstreams.org/rstreams-guides/core-concepts/event-streaming-primer/","title":"Event Streaming Primer","description":"In-depth guides and how-to&#39;s.","content":"One cannot understand the problems RStreams solves or reason about its implementation/usage without a fundamental understanding of event streaming compared to traditional microservices approaches.\nSummary Some systems work with parties that are constantly generating new data. Client data flowing from these parties tends to flow in a sequential order that we call an event stream. The events in this stream get transformed, enriched, and used to trigger subsequent events. Event stream processing, in concert with general purpose messaging, is a loosely coupled, scalable pattern ideal for designing enterprise systems built to handle continuous data flow. RStreams is just such a system.\n"},{"url":"https://rstreams.org/rstreams-flow/","title":"RStreams Flow","description":"Be up and running in 10 minutes.","content":"RStreams Flow is an opinionated framework that makes choices on how to build, test, deploy, monitor and maintain RStreams microservices. Engineers don’t care about RStreams. They care about creating reactive microservices that leverage native AWS services at scale. RStreams Flow helps engineers just do their work.\n"},{"url":"https://rstreams.org/rstreams-flow/configuring-rstreams/","title":"Configuring RStreams","description":"","content":"TODO\n"},{"url":"https://rstreams.org/rstreams-node-sdk/streams-primer/","title":"Streams Primer","description":"","content":"\r ToC\r   Readable Writeable Duplex Transform or Through    \r This primer provides exactly enough knowledge of streaming concepts for a developer to successfully write streaming applications using the RStreams SDK and bus. It is not intended as an exhaustive treatise on the vagaries of Node streams. We all have work to do.\n Overview There truly is nothing new under the sun. Streaming is really nothing more than Unix pipes, albeit in a more distributed manner, invented more than 50 years ago. The RStreams Node SDK relies on streaming data in and out just exactly as Unix pipes stream together commands in POSIX-based systems.\nStreaming involves creating a series of steps in a pipe where the first step, the Source, generates the data to move through the pipe. The last step is the Sink, whose job it is to do something with the data moving through the pipe. The Sink is responsible for pulling data from the previous step, which causes data to flow in the pipe: no Sink step in the pipe means no data flows. In between the source step and the sink step may optionally be any number of Transform steps that can modify data that flows through the pipe on its way to the sink.  Care to hear why some think streams are too hard?\r Streams get a bad rap. There are some who claim learning to stream data is too hard for developers. Most who dis on streams were quoted some years ago, though you can still find some articles today. The negativity was a reaction to Java and Node and C# releasing streams and their functional programming approach, which was uber complex and often used when it shouldn\u0026rsquo;t have been.\nThis is in large measure because streams became synonymous with functional programming in Java and C# and elsewhere. Java’s streaming solution, which is how many got their first experience with streaming, is complicated, ill suited to streaming because of Java’s verbosity and feel to many developers like regular expressions: going back to one requires painstakingly decomposing what it is doing, having to understand code that is hard to read and understand.\nNode’s original streaming API was hard to understand and use and has been significantly improved over the years. Don\u0026rsquo;t worry, the RStreams Node SDK dramatically simplifies it for you. \r  Care to read why streams might be worth it for you?\r Why code in a series of chained steps? Sounds complicated. The answer is you turn to streaming when you are working with systems where you need to process data as it is coming in because so much data needs to flow in that you can\u0026rsquo;t wait to start sending it out.\nIt\u0026rsquo;s also applicable when you need to minimize the delay in processing lots of data. Finally, it\u0026rsquo;s a great way to creat a reactive system, where the data that flows are events that cause distributed event handlers to wake up and process them, moving and transforming them from one place to another.\n\r Pipes and Streams 99% of the time, all you need to know is which RStreams SDK pipe step interface to use. This section helps you develop the mental model of a pipe so you can do that. Actually using a pipe step is brain dead simple. Don\u0026rsquo;t get overwhelmed as you don\u0026rsquo;t need to understand the actual functions of a Node Readable or Node Writable or the intracies of pipes as the SDK abstracts all that complexity for you. Here\u0026rsquo;s a good article if you want just a bit more detail but you you\u0026rsquo;ll be fine without it if you read the section below.\n As mentioned, a pipe is a set of steps that data flows through in sequence. Each step in the pipe is itself called a stream because they are meant to read/write data sequentially one after the other. Steps near the beginning of the pipe are upstream and the Sink downstream: data flows is the furthest step downstream. The pipe exists to daisy chain the stream steps together.\nIf you want the super short version, everything in a pipe must be linked together where start with a Readable followed by a Writable followed by a Readableand eventually end with a Writable as the Sink. Pipe step streams between the Source and the Sink have to be both a Writable and a Readable to allow the data to flow through the step: these in between steps are often called Through or Transform stream steps. Readable In node, a pipe is a function and each argument is a step, thus a stream, in the pipe. The first step, the Source, must get or create data somehow. It might do this by continuosly querying a database and making the data available for the next step to grab it. Remember that each downstream step pulls data from the step before it. In other words, the Source step must be readable by the next step so it can pull data from it. So, Source steps will always be of the Node type Readable. For example, the fs.createReadStream() Node file function will create a source stream that reads data from a file.\nA Readable stream is an abstraction for a source from which data can be consumed.\nThe RStreams SDK provides extremely simple Readable interfaces to make getting data from an RStreams bus queue a breeze. These simplified RStreams SDK pipe steps are of the RStreams SDK type ReadableStream which inherits from the Node Readable.\nWriteable The last step in a pipe, the Sink, needs to be able to do something with the data. In other words, it needs to be a step we can write to such as fs.createWriteStream() that creates a Sink stream that will write the data flowing through the pipe to a file.\nA Writable stream is an abstraction for a destination to which data can be written.\nThe RStreams SDK provides extremely simple Writable interfaces to make sending data to other resources, such as a database or Elastic Search, etc., a snap. These simplified RStreams SDK pipe steps are of the RStreams SDK type WritableStream which inherits from the Node Writable. That\u0026rsquo;s all you need to know.\nDuplex A stream step that sits between the Source and the Sink is by definition a Duplex stream. Think of a Duplex stream like its really two streams smashed together. The input to the Duplex stream is a Writable so it can consume the data from the Readable in the step before it. The output from the Duplex stream is a Readable so the next step downstream can pull data from it.\nA Duplex stream is one that is both Readable and Writeable at the same time, e.g. a TCP socket.\n Transform or Through A Transform stream is just a Duplex stream with a function that modifies the data or perhaps causes some other side effect and then sends the data downstream. A Transform stream is often called a Through stream.\nA Transform stream is a duplex stream that allows you to transform the data in between when it is written to the stream and later read from the stream.\nThe RStreams SDK provides extremely simple Transform interfaces to make moving data through a pipe easy. These simplified RStreams SDK pipe steps are of the RStreams SDK type TransformStream which inherits from the Node Duplex.\n "},{"url":"https://rstreams.org/rstreams-guides/","title":"RStreams Guides","description":"In-depth guides and how-to&#39;s.","content":"This section includes guides that cover RStreams core concepts and common use cases.\n"},{"url":"https://rstreams.org/rstreams-bus/anatomy-of-bus/","title":"Anatomy of a Bus","description":"","content":"\r ToC\r    \r Coming soon\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-streams/","title":"SDK Streams","description":"","content":"\r ToC\r   enrichEvent offload putEvent   checkpoint : WritableStream read : ReadableStream createSource : ReadableStream write : TransformStream load : WritableStream    \r You need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n Overview The RStreams Node SDK includes a simple utility function to create to create pipes and nearly every kind of stream you\u0026rsquo;d need to work with massive amounts of continuously generated data in an instance of the RStreams bus. It also includes functions to allow you to skip the complexity of dealing with pipes and streams at all for the most common use cases: getting data from the bus and sending data to the bus.\nPowerful Helper Functions without Pipes These functions do some heavy lifting for you, hiding all the complexity behind a single function that doesn\u0026rsquo;t require that you use pipes and streams at all.\nenrichEvent API docs: async version | callback version\nA function that asks for the source and destination queues and then reads events from the source queue and writes to the destination queue, allowing you to insert a function in-between to transform the data on the way or do other computation.\nWhen would I use this?\n You want to read from a source queue, enrich or modify the event and send it to another queue You want to read from a source queue and aggregate events, perhaps reading one minute worth of event and then writing one event to another queue that summarizes the 1 minute of source events  What do I need to consider when using this?\noffload https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#load https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#offloadEvents\nA function where you specify the source queue and give it a function and your function is called with events from the source queue, allowing you to save them elsewhere or do other computation.\nputEvent https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#put https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#putEvent\nA function that asks you to specify the destination queue and then you can write a single event to a given RStreams queue on the bus.\nFunctions to Create SDK Streams The following lists each SDK function that creates an instance of an SDK stream for you. If you need to do something with a pipe and its streams, there\u0026rsquo;s almost certainly a helper function to create the exact pipe step you need with the config you need to make it work in your use case.\ncheckpoint : WritableStream TODO A function that creates a Writable stream. TODO: link to checkpointing https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#checkpoint\nread : ReadableStream https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#read\nA function that lets you create a source stream that is backed by events you specify from an RStreams queue, with additional config to make it flexible, intelligent and performant.\ncreateSource : ReadableStream TODO\nhttps://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#createSource\nA function that creates a source stream that you can use to generate continuously generated, arbitrary content whether from a database, an API, a file or anything.\nwrite : TransformStream https://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#write\nA function that creates a pipe stream step that can sit somewhere between a source and sink stream and write the data that passes through the stream step to an RStreams queue while still allowing the data to pass through the step stream to the next downstream step in the pipe. This is useful if you want to siphon off some data to go to a given queue mid-pipe while you also want to send it on to do some other work.\nload : WritableStream TODO\nA function that creates a sink step stream that takes the data flowing through the pipe and sends it to an RStreams queue in an intelligent manner.\nhttps://leoplatform.github.io/Nodejs/classes/index.RStreamsSdk.html#load\nUtility Functions "},{"url":"https://rstreams.org/rstreams-node-sdk/","title":"RStreams Node SDK","description":"The smart SDK for Node/Typescript.","content":"This is the RStreams Node SDK, a client-side library designed to interact with instances of an RStreams Bus.\nThis assumes you\u0026rsquo;ve got an RStreams Bus instance running to connect to. If you don\u0026rsquo;t, head on over to the RStreams Bus section first. \nAlso, the RStreams Bus gets your Node/Typescript project setup in a jiffy with the right SDK config, ready to run local, debug and deploy if you haven\u0026rsquo;t already done that.\n Are you setup to run the examples? Expand this section if you\u0026rsquo;re not sure  \rAll examples in the SDK documentation assume that when these apps run, the RStreams SDK can discover the configuration it needs. The config it needs is the AWS resource IDs of the RStreams Bus instance deployed in your AWS account. Things like the ID of the kinesis stream used by the bus and so on.\nOf course, in a production environment the SDK will get the config in an intelligent and safe manner, say from AWS Secrets Manager. See the RStreams Flow Configuring RStreams doc.\nHere\u0026rsquo;s the typescript type of the config.\nGet the config You will first need to get this config. By default, the RStreams Bus puts a secret in secrets manager that is the JSON config blob. The secret will be named rstreams-\u0026lt;bus name\u0026gt;. Go get the JSON config from this secret.\nSave the config As a file Create a file named rstreams.config.json and put it in the same directory you are running your app in or in any parent director and the SDK will just find it and use it.\nAs an environment variable Create an environment variable named RSTREAMS_CONFIG whose value is the config JSON blob.\nAs an argument to the SDK itself Create a variable in the code that is the config and then pass it into the SDK\u0026rsquo;s constructor.\n1 2const RSTREAMS_BUS_CONFIG: ConfigurationResources = { 3 \u0026#34;Region\u0026#34;: \u0026#34;some-value\u0026#34;, 4 \u0026#34;LeoStream\u0026#34;: \u0026#34;some-value\u0026#34;, 5 \u0026#34;LeoCron\u0026#34;: \u0026#34;some-value\u0026#34;, 6 \u0026#34;LeoSettings\u0026#34;: \u0026#34;some-value\u0026#34;, 7 \u0026#34;LeoEvent\u0026#34;: \u0026#34;some-value\u0026#34;, 8 \u0026#34;LeoKinesisStream\u0026#34; : \u0026#34;some-value\u0026#34;, 9 \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;some-value\u0026#34;, 10 \u0026#34;LeoS3\u0026#34;: \u0026#34;some-value\u0026#34; 11}; 12 13const rsdk: RStreamsSdk = new RStreamsSdk(RSTREAMS_BUS_CONFIG); \r Do you know how to access Botmon? Expand this section if you\u0026rsquo;re not sure  \rBotmon is a visualization, monitoring and debugging tool that installs with an instance of the RStreams as a website. Most examples will have you use Botmon to visualize what\u0026rsquo;s happening and to help diagnose issues.\nTODO: how do they know how to access botmon?\n\r "},{"url":"https://rstreams.org/rstreams-node-sdk/read-write-scale/","title":"Read/Write at Scale","description":"","content":"\r ToC\r   App Use Cases and Considerations Config to the Rescue   Considerations Config to the Rescue    \r You need to understand what a pipe and stream step in a pipe are.\n Overview The RStreams Node SDK includes a simple utility function to create to create pipes and nearly every kind of stream you\u0026rsquo;d need to work with massive amounts of continuously generated data in an instance of the RStreams bus. It also includes functions to allow you to skip the complexity of dealing with pipes and streams at all for the most common use cases: getting data from the bus and sending data to the bus.\nReading at Scale You want to read from an RStreams queue. What do you need to consider to ensure you do that efficiently and responsibly at massive scale?\nApp Use Cases and Considerations What kind of app are you making?\n  CASE 1: Are you writing an app that runs once in a while, pulling events from a specific start/end range in the queue?\nMaybe you are writing an app to recover from a failure somewhere in your enterprise and so your app gets a start/end date of events that needs to be re-processed from the queue and it is manually kicked off.\nOr maybe you\u0026rsquo;re writing an app to sample data in a queue as part of monitoring and health checks that gets kicked off on a cron every five minutes to read a few events and go back to sleep.\n  CASE 2: Are you writing an app that runs continously as a daemon, pulling new events from the queue as fast as they show up?\nYou care about each and every event and you want to get each one in order from the queue and process it. If events are pushed into the queue faster than you can read them and process them then you\u0026rsquo;re in trouble because the number of events in the queue that are waiting for you to grab and process will grow unbounded. This means that the data you are processing is forever getting older and older and isn\u0026rsquo;t being processed in near real-time, seconds to a few minutes typically.\nAlso, what happens if your daemon crashes? You will need to restart it and keep reading from where you left off.\n  CASE 3: Are you writing a serverless app that has to shut down every 15 minutes as an AWS lambda function and get restarted and keep going?\nLet\u0026rsquo;s assume that this is just CASE 2 above but instead of a daemon it\u0026rsquo;s a lambda function. You can\u0026rsquo;t miss an event and you need to process them efficiently. You need to make sure you leave enough time to complete processing the events you have and know for sure where you left off before your lambda gets restarted.\n  How much processing are you doing and what latency is acceptable?\nThe more events that are pushed into a queue per unit time the more efficiently your app needs to be able to read and process these events. Reading events from a queue is lightning, but what if you need to call out to an API to get data to enrich each and every event? What if you need to hit a database for each and every event? That\u0026rsquo;s going to slow everything down and could make you upside down in that you can\u0026rsquo;t process events as fast as they are being pushed into a queue.\nHow big are the data events you are reading?\nLarge events can\u0026rsquo;t flow through many of AWS\u0026rsquo;s services. The RStreams SDK will detect this and push them to S3 and write an event that flows into the stream that actually points to the events stored in a file in S3. The SDK handles all of this transparently and you won\u0026rsquo;t even be aware you are reading from S3. However, the larger the events the more this is going to happen and the more time it could take to read events from S3 if those events are striped to hell and back in individual S3 files.\nConfig to the Rescue RStreams includes config in read operations to let you tune reading based upon your specific uses cases.\nThe following applies to the enrich, offloadEvents and read operations.\nReadOptions Interface\nNote there are other options not listed below that are less often needed but might be interesting in some rare cases to fine tune performance such as stream_query_limit, size or loops.\n  fast_s3_read\n Problem reading events is slow, likely because there\u0026rsquo;s lots of small S3 files the SDK is reading events from Solution set this to true and the SDK will read concurrently from multiple S3 files and your reads will be much faster - will default to on in Q3 2022 (you can control how much is ready concurrently if you need fine-grained control, which you likely won\u0026rsquo;t, using fast_s3_read_parallel_fetch_max_bytes)    runTime | stopTime\n Problem your lambda function (bot) is shutting down after 15 minutes instead of ending gracefully because it is endlessly reading events from a queue Solution tell the RStreams read operation you are using to end after runTime number of milliseconds and set that to be 75-80% of the amount of time the lambda has left before it runs out of time before AWS shuts it down forcefully or calculate the exact stopTime that saves roughly 20% of the 15 min shutdown window for the pipe to complete processing, flush and checkpoint.    start\n Problem I don\u0026rsquo;t want to read the latest events, I want to start from a specific position in the queue Solution use the start attribute to specify the event ID of when to start    maxOverride\n Problem I don\u0026rsquo;t want to keep reading events forever, I want to stop at a certain time in the queue Solution use the maxOverride attribute to specify the event ID of when to stop    BatchOptions Interface\nThese don\u0026rsquo;t control reading from a queue but allow you to hold on to a group of events and present those events all at once to the next stream step in the pipe, a concept called micro-batching.\n bytes | count | time  Problem It\u0026rsquo;s taking me longer and longer to process events and I can\u0026rsquo;t keep up with new events coming into the queue and so reading is getting further and further behind Solution Try micro-batching using these attributes to group of events in small batches that are sent to the next pipe stream step all at once and then rewrite whatever your code is doing in that pipe stream step to do it in paralled: if writing to a DB write the entire batch in one SQL query; if reading from a DB, do one read to get all the data you need for all the events in the batch; if hitting an API use Promise.all to run each API request in parallel for the batch. NOTE, if you just can\u0026rsquo;t keep up no matter what, consider implementing Fanout    BufferOptions Interface\nThese serve the same purpose as the BatchOptions Interface above and solve the same problem. The difference is that BatchOptions are built into an RStreams operation to let you control it while BufferOptions is used with the Buffer pipe stream step operation that may be inserted into the pipe to choose to micro-batch events before flowing to the next pipe stream step. The attribute names are named slightly differently but are identical in purpose and function.\nToCheckpointOptions Interface\nHead over to the checkpointing article if you don\u0026rsquo;t know what a checkpoint is or what it\u0026rsquo;s used for.\n records | time  Problem I can\u0026rsquo;t ever re-process an event and so I need to checkpoint after I process each and every event Problem I\u0026rsquo;m OK if I re-process some events in the rare case of a failure and so I only want to checkpoint after so much time or so many records Solution Use these attributes to control checkpointing in a stream (see the checkpoint operation)    Writing at Scale You want to write to an RStreams queue. What do you need to consider to ensure you do that efficiently and responsibly at massive scale?\nConsiderations What\u0026rsquo;s really happening underneath the covers with a write?\nThe SDK is writing to either Kinesis, S3 and Firehose and S3 followed by Kinesis. See the Anatomy of a Bus article for more on this.\nSo, that means Kinesis has limitations on the size of events and how much data you can concurrently write to kinesis at once without having to jump through hoops.\nAm I getting data to write onesie twosie or all at once in big batches?\nPerhaps you are receiving a file from a customer where each row in the file is an object you want send into an RStreams queue or are you getting data in an event driven manner and the flow of those events can\u0026rsquo;t be predicted but is likely either coming one at a time or in a micro-batch.\nConfig to the Rescue RStreams includes config in write operations to let you tune writing based upon your specific uses cases.\nThe following applies to the load, offloadEvents and write operations.\nWriteOptions Interface\n  useS3\n Problem I have lots of events to send to an RStreams queue all at once and it\u0026rsquo;s slow Solution Set the useS3 option to true and the SDK will write a file chock full of events, many thousands is normal, and then send one event through kinesis that points back to the S3 file Problem It\u0026rsquo;s taking too long to read events Solution Wait. Why is this here in the write section? The reason is that how you write can affect how you read. If you write tons and tons and small S3 files, say with one event each, that\u0026rsquo;s going to affect read performance since the SDK will have to make many calls to S3 to read a small number of events. Yes, there\u0026rsquo;s a new fast_s3_read capability that will read multiple files at once that makes this much better but still it can be an issue. So the solution is to be smart about your use of the useS3 attribute. Be sure you micro-batch successfully if you use it, meaning that there is enough data available to be written all at once using the batch or buffer options listed above.    firehose\n Problem My event handler that writes to an RStreams queue is invoked one at a time by the nature of how it runs and the pace at which events come in that I want to write and so I\u0026rsquo;m writing lots of individual events that flow through kinesis and take up concurrent write bandwidth Solution Set firehose to true. Firehose will automatically micro-batch events for us in one minute increments, writing them to an S3 file which will then get sent to kinesis as one event. This does mean that ingestion will be delayed by up to a minute, so this will only work in use cases where this is acceptable.    records | size | time\n Problem I don\u0026rsquo;t want to inundate kinesis with events going one at a time but I need control over how group up events and send as a micro-batch to kinesis because ingestion time matters Solution Use one of these attributes, and probably all of them, to control how long to wait before the SDK micro-batches up events, zips them as a single blob and sends them to kinesis, which performs like a champ. Set number of events, the max size of the events and the max time to wait and the max number of events to wait and whichever is tripped first will cause the micro-batch to be sent as is.    "},{"url":"https://rstreams.org/rstreams-bus/","title":"RStreams Bus","description":"RStreams event bus, built with native AWS services.","content":"The RStreams Bus is a light-weight framework that uses AWS Kinesis, S3, Lambda and Dynamo DB to create an event-streaming and general purpose messaging platform.\n"},{"url":"https://rstreams.org/rstreams-botmon/","title":"RStreams Monitoring","description":"Monitor, trace and debug events in the bus in real-time.","content":"RStreams includes a webapp called Botmon that provides real time monitoring, data visualization, tracing and debugging.\n"},{"url":"https://rstreams.org/rstreams-guides/core-concepts/","title":"Core Concepts","description":"In-depth guides and how-to&#39;s.","content":"Start here to get an understanding of core concepts that govern the RStreams platform. The Event Streaming Primer is a realy good place to start.\n"},{"url":"https://rstreams.org/","title":"RStreams","description":"","content":""},{"url":"https://rstreams.org/categories/","title":"Categories","description":"","content":""},{"url":"https://rstreams.org/changelog/","title":"Changelog posts","description":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.","content":"February Updates Feb 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nChanged\r  Better support for using applying additional filters to posts_tax_query for categories for custom WordPress syncs\n  Reporting fine-tuning for speed improvements (up to 60% improvement in latency)\n  Replaced login / registration pre-app screens with a cleaner design\n   Removed\r Removed an issue with the sync autolinker only interlinking selectively. Removed up an issue with prematurely logging out users   \rMarch Updates Mar 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nAdded\r Some scheduled changelogs, tweets, and slack messages queued up this weekend and were not published on time. We fixed the issue and all delayed publications should be out. We now prioritize keywords over title and body so customers can more effectively influence search results Support form in the Assistant is now protected with reCaptcha to reduce spam reinitializeOnUrlChange added to the JavaScript API to improve support for pages with turbolinks   Fixed\r Fixed an issue with the sync autolinker only interlinking selectively. Fixed up an issue with prematurely logging out users   \rChangelog label Added\r Changed\r Depricated\r Removed\r Fixed\r Security\r Unreleased\r "},{"url":"https://rstreams.org/contact/","title":"Got Any Questions","description":"this is meta description","content":""},{"url":"https://rstreams.org/rstreams-node-sdk/api-docs/","title":"SDK API Docs","description":"The generated RStreams Node SDK API Docs","content":"Jump over to the RStreams Node SDK API Docs.\n"},{"url":"https://rstreams.org/search/","title":"Search Result","description":"this is meta description","content":""},{"url":"https://rstreams.org/tags/","title":"Tags","description":"","content":""}]