[{"url":"https://rstreams.org/rstreams-guides/core-concepts/","title":"Core Concepts","description":"In-depth guides and how-to&#39;s.","content":"Start here to get an understanding of core concepts that govern the RStreams platform. The Event Streaming Primer is a realy good place to start.\n"},{"url":"https://rstreams.org/rstreams-guides/core-concepts/event-streaming-primer/","title":"Event Streaming Primer","description":"In-depth guides and how-to&#39;s.","content":"\r\r\r ToC\r   Summary What\u0026rsquo;s a Stream Continuous data systems before event streaming Event streaming compared to APIs    \r\rOne cannot understand the problems RStreams solves or reason about its implementation/usage without a fundamental understanding of event streaming compared to traditional microservices approaches.\n  Summary Some systems work with parties that are constantly generating new data. Client data flowing from these parties tends to flow in a sequential order that we call an event stream. The events in this stream get transformed, enriched, and used to trigger subsequent events. Event stream processing, in concert with general purpose messaging, is a loosely coupled, scalable pattern ideal for designing enterprise systems built to handle continuous data flow. RStreams is just such a system.\n  What\u0026rsquo;s a Stream The traditional definition of a stream is a sequence of data elements that move from a source to a sink.   Sources generate the data that feeds the start of the stream. A source may get the data to feed into the stream from a file, from an HTTP request, from a database or by reading from another stream.\nThe transform step(s) of a stream processes and possibly modifies the data that moves through a stream. Transform steps may parse data, filter data, zip/unzip content, enrich data form other sources or aggregate data.\nThe sink is the final step in a stream, which collects and stores the final transformed data. A sink can write a new file, update a database, collect data into an array or write to another stream.\nStreams may be implemented with a push or pull model. The main difference between the two models is which end of the stream is in control of data moving through the stream. In a push model the source will push data to the next transform step until it reaches the sink. In a pull model the sink will pull/request data from the previous steps until it reaches the source. Node and Rust streams APIs use the pull model.\n  Continuous data systems before event streaming Before the cloud and before products were designed to work well with big data problems, batch processing was the preferred method to create solutions for continuous data in/out. The producer of the data would batch up and send a file to the System. The Core microservice of that system, often running on a periodic timer, would look for and find the newly deposited file and batch up the ingestion of the file into the source-of-truth database it encapsulates.\nSome other microservice, say the data warehouse microservice, running on a periodic timer of its own, would hit the Core microservice interface and query for change. It would suck the data over in batch and plant a file of its own somewhere for the data warehouse microservice to act on.\nPerhaps there would also be another microservice to generate analytics for interactive dashboards. That analytics microservice would likewise call the Core’s API to pull change from the core database, perhaps on a daily basis, and batch that into the analytics microservice’s database.\nChallenges with this approach:\n  Batching and time\nBatching in this manner introduces big latencies in processing time at every level of the system and across all microservices since each is responsible to reach out and ask for the data it needs and does so with some large batch interval.\n  Batching, load and visibility\nA continuous stream of large batches of data, processed one at a time as a thing, introduces large operational challenges because of the complexity and the amount of time it takes to replay a batch when things go wrong with the batch system or with a customer’s own system. It is difficult to get visibility into what is happening and debug issues with large batches.\n  Microservices and cross-cutting change\nIn a microservices world where each service encapsulates its data and requires that others ask for it, it makes cross-cutting change across the microservices of the system very challenging, say adding a new fundamental attribute to a core data type used in many of the system’s services.\n  Microservices coupling\nA recognized drawback with conventional microservice architectures is the coupling that occurs between microservices. Each service reaches out and asks another microservice for the data or state it needs, creating a network of dependencies. When a given microservice is down or overloaded, retries and queuing must be implemented at each point in the graph of calls made to another service.\n  Still, this is far better than monolithic, pre-microservice architectures to be certain, but there is a better way to organize services for a system designed to receive and generate a near continuous stream of data events.\n  Event streaming compared to APIs The non-event streaming way has microservices reach out and pull data in that they need using microservice APIs, creating a directed graph of dependent API calls between microservices. This approach tightly couples various microservice API interfaces with the API data inputs/outputs. It puts code in the driver’s seat of the architecture.   Event streaming turns the microservices approach on its head. With event streaming, the structure of the flow of data causes data to be pushed to the microservices that need them via queues of data, creating a directed graph of data streams, not API calls.\n   The structure of the directed graph of streams is the application. This structure causes data to be pushed from where it is to where a microservice needs it, without tight coupling. Nothing is free in life and event streaming has its own drawbacks. Change doesn’t revolve around modifying APIs as in the traditional microservices approach, instead it revolves around changing data contracts, say adding an attribute to an object that flows everywhere.\nThis cost is easier to bear, however, because it more closely aligns behavior where the focus should be in today\u0026rsquo; world - on the data. It means that data is the feature and the crux of where architecture and thought goes, not code. A large part of making an interesting change, such as adding a new major capability to a system, is just supporting receiving and streaming the new data attributes of system objects. for the new capabilities. The code to do something interesting with the data is often trivial by comparison to getting the data changes and contracts right and propagated.\n"},{"url":"https://rstreams.org/what-is-rstreams/","title":"What is RStreams?","description":"What is RStreams and why should I use it?","content":"\r\r\r ToC\r   RStreams Defined Use Cases    \r\r  RStreams Defined  RStreams Bus: a server-side framework that is a thin veneer using AWS Kinesis, Firehose, S3, Lambda and DynamoDB to create a general-puropose streaming and messaging platform RStreams Node SDK: a node/typescript SDK that developers use in their apps and lambda functions to get data from and send data to queues of an instance of an RStreams Bus in a streaming fashion with massive concurrent scale RStreams Flow: a project template that generates an opinionated way to craft one or more related bots, lambda queue event handlers, to develop a reactive app using RStreams locally as a developer, running it, debugging it, deploying it, and monitoring it RStreams Monitoring: a webapp, Botmon, that provides a visual graph of all queues and bots (queue lambda event handlers), provides statistics on the time it takes to process and move data from queue to queue and enables the extraction of these metrics to one\u0026rsquo;s own monitoring solution for reporting and alerting    Use Cases If these use cases sound useful, RStreams might make sense for you.\n Queues and Event Handlers I want to put data events into many different queues and have a single event handler lambda wake up when events come in and process them 1 to Many Event Handling I want to have N different event handlers listening for change off the same queue without stepping on each other Many to 1 Event Reducing I want to aggregate events and emit a single event for a group while keeping track of where the one event came from as I send it to another queue Automatic Persistence I want all events to persist, even after I\u0026rsquo;ve handled the events, without risk of re-processing data Stateful Queues I want queues to remember where I last read events from when my program restarts and let me safely keep reading in the queue Ordered Queues and Ordered Processing I want events in my queues to enter in time-sequence order and I want event handlers to be able to read events exactly once in either total order or partial order; partial order to ensure I can read from a queue in parallel but not get events that matter out of order Event Playback I want to be able to replay events or rewind back to a moment in time and play from there for failure recovery scenarios Parallelization with Fanout I want N copies of the same event handler intelligently reading from a single queue so I can process events super fast and keep up with a vast amount of data events coming in Event Push I want my lambda queue event handler to be invoked when there are new events to be handled in the queue Event Pull I want my lambad to be invoked on a cron and then to be able to pull data from whatever queue I want Event Transformation I want a super easy way to read data events from one queue, transform them and push them to another queue at massive scale Zero-infrastructure Scale-out I don\u0026rsquo;t want to have to deploy new AWS infrastructure and create new resources just to create new queues Data-flow Visualization I want to visualize the queues and event handlers that put data in and take data out of queues to understand the system and to monitor/diagnose issues visually Develop Locally I want to develop and debug locally despite the complexity of a streaming system  "},{"url":"https://rstreams.org/rstreams-botmon/getting-started/","title":"Getting Started","description":"","content":"\r\r\r ToC\r   Summary Accessing Botmon    \r\rThis article assumes you have a working installation of an RStreams Bus.\n The URL for accessing Botmon and the security around it may well have been changed at your company. This is more for a brand new installation of an RStreams bus. The next article goes deep into what Botmon is, the problems it solves and how to use it.\n   Summary This article explains how to access Botmon, a webapp that provides queue and bot data visualization, data tracing, monitoring and debugging.\n  Accessing Botmon After having installed your new RStreams Bus you need to figure out how to access it.\nFirst, be aware that the Botmon website is deployed wide open. TODO: Follow these instructions on securing Botmon.\n Go to CloudFormation Search on the name of your RStreams bus instance - I searched on PlaygroundBus, the bus I created in the RStreams Bus getting started guide You should see a stack named {busName}-Botmon. Go into its details. This is the stack that installed the Botmon web app just FYI Go to ApiGateway and search on the name of your bus You should see an API for {busName}-botmon, go into its detail and see something like this    Click on Settings on the left nav Scroll down to Default Endpoint and copy the URL embedded in the paragraph This is your default endpoint to Botmon, feel free to search on how to use your own domain name if you\u0026rsquo;d prefer for an API Gateway API  "},{"url":"https://rstreams.org/rstreams-bus/getting-started/","title":"Getting Started","description":"","content":"\r\r\r ToC\r   Summary AWS Cost The Stack of Stacks Create the RStreams Stack How do you access the new RStreams Bus instance?  Check SecretsManager for the new Secret Make your own RStreams Config Secret  Get Your RStreams Config Values Create the Secret     What do you Have?    \r\rAs elsewhere in this documentation, you will find reference to leo which is the old brand for RStreams. These references are being gradually changed over time.\n   Summary Learn how to install a new RStreams Bus instance.\n  AWS Cost Be aware that following these instructions will result in AWS resources being created that cost between 50 and 100 dollars a month.\nOf course, after creating the bus, you can dial down the capacity of the resources and get that down well below ten dollars a month. The more data you push into and read out of the bus, the more it costs. During testing and evaluation, all bus resources plus read/write capacity will easily remain in the 50 - 100 dollar a month range. You should pay close attention to cost as you ramp up use. The greatest expense is DynamoDB reads/writes which powers queues. The second greatest expense is DynamoDB storage. Kinesis and S3 costs are relatively minor by comparison.\n  The Stack of Stacks As stated by AWS, \u0026ldquo;A stack is a collection of AWS resources that you manage as a single unit.\u0026rdquo; Installing RStreams involves creating a new stack that itself contains four additional stacks:\n The Bus Stack : the actual RStreams bus instance itself The Auth Stack : sets up authentication tables for managing access to bots/queues The Cognito Stack : used for access to RStreams resources The Botmon Stack : the monitoring, tracing and visual debugging website that accompanies an RStreams Bus instance  From AWS CloudFormation\u0026rsquo;s designer.     Create the RStreams Stack Create a new stack using this RStreams Stack CloudFormation Template URL.\nhttps://leo-cli-publishbucket-abb4i613j9y9.s3.amazonaws.com/leo/2.0.0/cloudformation-1652216325999.json \r\r\r\rClick here for the long version with all the gory details  \r\r  Go to CloudFormation in the AWS Console and be sure you\u0026rsquo;re in the region you want to create the RStreams Bus in. Click the Create stack button    Select the With new resources (standard) option since we\u0026rsquo;re creating a stack with new resources Step 1 - Specify Template\nPopulate the Amazon S3 URL column with the RStreams CloudFormation Template URL at the top of this section which is a public URL containing the definition of the four sub-stacks that together create an instance of an RStreams bus. Click Next.    Step 2 - Specify Stack Details  Give the stack a name. By convention, RStreams bus instances are title cases with two words, the first being the name of the bus or its environment and the second being the word bus as in: ProdBus, StagingBus, TestBus, ProdIntegrationBus, PlaygroundBus, etc. Name the environment. This isn\u0026rsquo;t actually used today but it may be in the future, so go ahead and fill it in and then click the Next button.      Step 3 - Configure Stack Options\nThere are no stack options necessary, but it\u0026rsquo;s usually a good idea to add tags in conformance with AWS best practices. Scroll down to the bottom of the form and click the Next button.    Step 4 - Review\nReview everything and then scroll to the bottom. You will have to acknowledge that clicking the Create Stack button will possibly do some the things listed by checking the two checkboxes. If you agree, check them and click Create Stack. Then wait 10 - 20 minutes, refreshing as you go to see the proress.       How do you access the new RStreams Bus instance? If you are on a new version of the RStreams Bus, then creating the RStreams Stack will have published a new Secrets Manager secret that contains all the resources necessary for the RStreams Node SDK to connect to and use the new bus. It will be named like this: rstreams-{busName} where {busName} is the name you gave your RStreams bus instance when you created it.\n  Check SecretsManager for the new Secret  Go to AWS Secrets Manager In the filter text area type \u0026ldquo;rstreams\u0026rdquo; and hit enter If it\u0026rsquo;s there click on it and then click the Retrieve secret value button and you should see something like this:  {  \u0026#34;LeoStream\u0026#34;: \u0026#34;PlaygroundBus-Bus-123456-LeoStream-123456\u0026#34;,  \u0026#34;LeoCron\u0026#34;: \u0026#34;PlaygroundBus-Bus-123456-LeoCron-123456\u0026#34;,  \u0026#34;LeoEvent\u0026#34;: \u0026#34;PlaygroundBus-Bus-123456-LeoEvent-123456\u0026#34;,  \u0026#34;LeoSettings\u0026#34;: \u0026#34;PlaygroundBus-Bus-123456-LeoSettings-123456\u0026#34;,  \u0026#34;LeoSystem\u0026#34;: \u0026#34;PlaygroundBus-Bus-123456-LeoSystem-123456\u0026#34;,  \u0026#34;LeoKinesisStream\u0026#34;: \u0026#34;PlaygroundBus-Bus-123456-LeoKinesisStream-123456\u0026#34;,  \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;PlaygroundBus-Bus-123456-LeoFirehoseStream-123456\u0026#34;,  \u0026#34;LeoS3\u0026#34;: \u0026#34;playgroundbus-bus-123456-leos3-123456\u0026#34;,  \u0026#34;Region\u0026#34;: \u0026#34;us-east-1\u0026#34; }  Note there\u0026rsquo;s nothing actually secret in the RStreams Config secret. There are no passwords just the connection name needed for the various AWS resources just created that comprise the RStreams Bus.\n   Make your own RStreams Config Secret If your secret isn\u0026rsquo;t there, feel free to create your own to make working with the SDK easier. The RStreams Flow deployment process will automatically look for the secret name specified by the developer for a given environment and associate the correct environment-specific RStreams config JSON structure with your bot when it deploys. Note that you\u0026rsquo;ll see environment called stage in RStreams Flow since that\u0026rsquo;s what the Serverless Framework calls it.\n  Get Your RStreams Config Values The bus stack, one of the four stack, contains all the configuration as exported stack values. Go get them.\n Go to CloudFormation Filter by the name you selected for the RStreams bus stack, I chose PlaygroundBus You should see five entries in the filtered results, one each for the five stacks that were created and here are mine  PlaygroundBus : the main RStreams Bus stack that caused the other four stacks to be created PlaygroundBus-Botmon : the monitoring, tracing, debugging web app stack PlaygroundBus-Auth : the authentication stack PlaygroundBus-Bus : the actual RStreams bus instance stack itself which created kinesis, firehose, s3 and other resources PlaygroundBus-Cognito : the cognito stack   Select your {myBusName}-Bus stack that is the actual RStreams bus instance stack to see its detail Select the Outputs tab along the top You are now looking at all the values the SDK needs to connect to your bus instance  The SDK needs a structure that looks like this in order to connect to your RStreams bus instance where the keys in the JSON structure match the keys on the Outputs tab of the Bus stack we just landed on and the values come from the corresponding Value column.\n\r\r SDK RStreams Config JSON Structure\r 1{ 2 \u0026#34;LeoStream\u0026#34;: \u0026#34;abc\u0026#34;, 3 \u0026#34;LeoCron\u0026#34;: \u0026#34;abc\u0026#34;, 4 \u0026#34;LeoEvent\u0026#34;: \u0026#34;abc\u0026#34;, 5 \u0026#34;LeoSettings\u0026#34;: \u0026#34;abc\u0026#34;, 6 \u0026#34;LeoSystem\u0026#34;: \u0026#34;abc\u0026#34;, 7 \u0026#34;LeoKinesisStream\u0026#34;: \u0026#34;abc\u0026#34;, 8 \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;abc\u0026#34;, 9 \u0026#34;LeoS3\u0026#34;: \u0026#34;abc\u0026#34;, 10 \u0026#34;Region\u0026#34;: \u0026#34;abc\u0026#34; 11} \r\r  Create the Secret  Go to secrets manager Click Store a new secret For Secret type select Other type of secret For Key/value pairs select Plaintext and populate the text area with the SDK RStreams Config JSON Structure listed above Follow the instructions in the previous section to populate the values in that JSON structure with the correct values for you bus instance Name your secret exactly this: rstreams-{busName} where {busName} is the name of your bus (the bus I created was named PlaygroundBus so my secret should be named rstreams-PlaygroundBus) Save it and continue on to use your bus below    What do you Have? The next article explains in detail what was created but if you\u0026rsquo;re ready to give it a spin, head over to the RStreams Flow Getting Started Guide and Running Locally articles which will demonstrate using the RStreams Bus, creating RStreams queues on the bus instance and bots that interact with them.\n"},{"url":"https://rstreams.org/rstreams-flow/getting-started/","title":"Getting Started","description":"","content":"\r\r\r ToC\r   Summary Install Node and NPM Install the Serverless Framework Get RStreams Bus Config Secret AWS Access Checkout the RStreams Flow Example Project Project Commands using NPM            \r\rRStreams flow is powered by the popular Serverless Framework. It has a very large community and many plugins that provide a scaffold for RStreams Flow to build on. The Serverless Framework does have some deficiencies, however, it is believed that it is extensible enough that these can be improved or worked around.\n   Summary Get up an running to develop with RStreams with the sample app in less than 30 minutes.\n  Install Node and NPM Install Node and NPM. RStreams Flow supports whatever version of Node JS is deployable to AWS lambda. At the time of this article, that\u0026rsquo;s Node 14.\n  Install the Serverless Framework Install the Serverless Framework as a global package on your development box.\nnpm install -g serverless   Get RStreams Bus Config Secret You must have an RStreams Bus instance to access. Each bus instance installs a secret in AWS secrets manager named rstreams-{busName}. If you go to AWS secrets manager and search on rstreams, you can see if one is installed. All you need is the name of this secret that points to a non-production instance of an RStreams bus that you can work with safely.\nMany companies will install a bus in their dev or test environments. If you\u0026rsquo;re unsure, reach out to IT/Devops in your company. You can also standup your own bus instance but be warned this creates some AWS resources: Kinesis, S3, a few DynamoDB tables, a secret in Secrets Manager. It\u0026rsquo;s not much money to just stand them up and have them turned on but if you push lots of data through your bus, it will become real money. Be sure to contact your IT/Devops/Engineering management on their policies around this.\n  AWS Access You are going to need to access from your laptop to the resources listed in the AWS Secrets Manager secret named for the RStreams bus instance you wish to use. Those resources are six DynamoDB tables, a Kinesis stream, a Firehose stream and a private S3 bucket. Please reach out to your IT/Devops/Eng mgt to follow your company\u0026rsquo;s proscribed policy for gaining access to these resources. If IT/Devops/Eng mgt is just you, there are lots of online resources on creating IAM policies. Here are the contents of rstreams-TestBus that might exist in a secret for say a test environment.\n\r\r rstreams-TestBus Secret\r 1{ 2 // These are DynamoDB tables 3 \u0026#34;LeoStream\u0026#34;:\u0026#34;TestBus-LeoStream-1234567\u0026#34;, 4 \u0026#34;LeoCron\u0026#34;:\u0026#34;TestBus-LeoCron-1234567\u0026#34;, 5 \u0026#34;LeoSettings\u0026#34;:\u0026#34;TestBus-LeoSettings-1234567\u0026#34;, 6 \u0026#34;LeoEvent\u0026#34;:\u0026#34;TestBus-LeoEvent-1234567\u0026#34;, 7 \u0026#34;LeoSystem\u0026#34;:\u0026#34;TestBus-LeoSystem-1234567\u0026#34;, 8 \u0026#34;LeoArchive\u0026#34;:\u0026#34;TestBus-LeoArchive-1234567\u0026#34;, 9 10 // A Kinesis stream 11 \u0026#34;LeoKinesisStream\u0026#34;:\u0026#34;TestBus-LeoKinesisStream-1234567\u0026#34;, 12 13 // A Kinesis Firehose stream 14 \u0026#34;LeoFirehoseStream\u0026#34;:\u0026#34;TestBus-LeoFirehoseStream-1234567\u0026#34;, 15 16 // An S3 bucket 17 \u0026#34;LeoS3\u0026#34;:\u0026#34;testbus-leos3-1234567\u0026#34;, 18 19 // The region this is installed in 20 \u0026#34;Region\u0026#34;:\u0026#34;us-east-1\u0026#34; 21} \r\r  Checkout the RStreams Flow Example Project  RStreams Flow will provide a command line tool for generating a sample project from a template by June 30, 2022. For now, let\u0026rsquo;s just checkout the sample project and customize it.\n Here\u0026rsquo;s the git project or just check it out:\ngit clone https://github.com/LeoPlatform/rstreams-flow-example.git   Project Commands using NPM   Check code coverage on all files in the project Run code coverage for all files. More info here.\nnpm run coverage-all   Check code coverage on only source files visited during a test Run code coverage for only select files. More info here.\nnpm run coverage   Run project unit tests using Mocha More info here.\nnpm run utest   Package your bots into artifacts ready to be pushed to CI/CD or directly released More info here.\nnpm run package   Deploy directly to your dev environment using serverless deploy More info here.\nnpm run deploy-dev   Use the Serverless RSF plugin to run locally You will need to provide the name of your bot to run - see Running Locally for a lot more detail.\nnpm run test-sls npm run test   Automatically re-compile typescript to javascript as source files change More info here.\nnpm run watch   Automatically re-compile project config JSON into typescript interface when it changes This is being released as a preview. The feature will be released officially soon.\nnpm run watch-config   Bundle the project with webpack More info here.\nnpm run webpack   Lint the project More info here.\nnpm run lint "},{"url":"https://rstreams.org/rstreams-node-sdk/getting-started/","title":"Getting Started","description":"","content":"\r\r\r ToC\r   Are you setup to run the examples? Principle Operations Write to the bus  Write a single object to the bus Write multiple objects to the bus Stream multiple objects to the bus fast      \r\rThis primer provides exactly enough knowledge of streaming concepts for a developer to successfully\n This primer provides exactly enough knowledge of streaming concepts for a developer to successfully write streaming applications using the RStreams SDK and bus. It is not intended as an exhaustive treatise on the vagaries of Node streams. We all have work to do.\n   Are you setup to run the examples? \r\r\r\rExpand this section if you\u0026rsquo;re not sure  \rAll examples in the SDK documentation assume that when these apps run, the RStreams SDK can discover the configuration it needs. The config it needs is the AWS resource IDs of the RStreams Bus instance deployed in your AWS account. Things like the ID of the kinesis stream used by the bus and so on.\nOf course, in a production environment the SDK will get the config in an intelligent and safe manner, say from AWS Secrets Manager. See the RStreams Flow Configuring RStreams doc.\nHere\u0026rsquo;s the typescript type of the config.\n  Get the config You will first need to get this config. By default, the RStreams Bus puts a secret in secrets manager that is the JSON config blob. The secret will be named rstreams-\u0026lt;bus name\u0026gt;. Go get the JSON config from this secret.\n  Save the config   As a file Create a file named rstreams.config.json and put it in the same directory you are running your app in or in any parent director and the SDK will just find it and use it.\n  As an environment variable Create an environment variable named RSTREAMS_CONFIG whose value is the config JSON blob.\n  As an argument to the SDK itself Create a variable in the code that is the config and then pass it into the SDK\u0026rsquo;s constructor.\n1 2const RSTREAMS_BUS_CONFIG: ConfigurationResources = { 3 \u0026#34;Region\u0026#34;: \u0026#34;some-value\u0026#34;, 4 \u0026#34;LeoStream\u0026#34;: \u0026#34;some-value\u0026#34;, 5 \u0026#34;LeoCron\u0026#34;: \u0026#34;some-value\u0026#34;, 6 \u0026#34;LeoSettings\u0026#34;: \u0026#34;some-value\u0026#34;, 7 \u0026#34;LeoEvent\u0026#34;: \u0026#34;some-value\u0026#34;, 8 \u0026#34;LeoKinesisStream\u0026#34; : \u0026#34;some-value\u0026#34;, 9 \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;some-value\u0026#34;, 10 \u0026#34;LeoS3\u0026#34;: \u0026#34;some-value\u0026#34; 11}; 12 13const rsdk: RStreamsSdk = new RStreamsSdk(RSTREAMS_BUS_CONFIG); \r   Principle Operations Write\nYou\u0026rsquo;re going to want to write to the bus, meaning send a data event to a specific queue of the bus. Queues maintain their order, with the newest at the front of the queue and the oldest data at the back of the queue.\nRead\nYou\u0026rsquo;re going to want to read from the bus, meaning read events from a queue of the bus. You typically read from the last place you read from last in a queue. Or, if this is your bot\u0026rsquo;s first time reading from a queue then the oldest event in the queue is the default. Or, you can read events in a specific range back in time in the queue.\nTransform\nYou\u0026rsquo;re going to want to read from the bus, change the data somehow or cause a side effect like writing to some database, and then write the changed data to a different queue.\n  Write to the bus You want to write data to an RStreams qeuue.\nTODO: include link to git project so can checkout and run\n  Write a single object to the bus Let\u0026rsquo;s say we want to populate an RStreams queue with people we retrieve from an API that generates random people. The steps to do that are\n Line 6 : Create an instance of the SDK Line 7 : Go get a single random person from a public API using the Axios library Line 8 : Call the putEvent SDK API to send an event up to the RStreams Bus  The first argument is the ID of the bot this code is running as The second argument is the ID of the RStreams queue to send the event to The third argument is the JSON object to send    1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRaw, PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const person = await getRandomPerson(); 8 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 9} 10 11async function getRandomPerson(): Promise\u0026lt;PersonRaw\u0026gt; { 12 const NUM_EVENTS = 1; 13 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 14 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 15 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 16 17 if (status !== 200) { 18 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 19 } 20 21 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 22 23 return data.results[0]; 24} 25 26(async () =\u0026gt; { 27 await main(); 28})() \r\r\r PersonRaw \u0026amp; PersonRawResults interfaces\r 1export interface PersonRaw { 2 gender: string; 3 name: { 4 title: string; 5 first: string; 6 last: string; 7 } 8 location: { 9 street: { 10 number: number; 11 name: string; 12 } 13 city: string; 14 state: string; 15 country: string; 16 postcode: number; 17 coordinates: { 18 longitude: string; 19 latitude: string; 20 } 21 timezone: { 22 offset: string; 23 description: string; 24 } 25 } 26 email: string; 27 dob: { 28 date: string; 29 age: number; 30 } 31 nat: string; 32} 33 34export interface PersonRawResults { 35 results: PersonRaw[]; 36} \r\rView results in Botmon\nIf you go to Botmon, you will see that the rstreams-example.people queue now has an event in it. \r\r\rExpand for Botmon screenshots  \r  Go to Botmon and search for rstreams-example.people in the search field     Botmon now shows a visual representation of the bot and the queue, click on the gear icon after hovering over the queue and then click on Events     Botmon now shows the events loaded into the queue     \r\n  Write multiple objects to the bus So, instead of reading one person from the public API we used in the example above, let\u0026rsquo;s say we get 100 people at a time from the public API and we want to write them to the bus. Here\u0026rsquo;s what that looks like.\n1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const people = await getRandomPeople(); 8 9 //HINT: this will have very bad performance. This is just to illustrate a point. 10 // Don\u0026#39;t use putEvent in a loop this way in practice! 11 for (const person of people.results) { 12 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 13 } 14} 15 16async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 17 const NUM_EVENTS = 100; 18 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 19 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 20 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 21 22 if (status !== 200) { 23 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 24 } 25 26 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 27 28 return data; 29} 30 31(async () =\u0026gt; { 32 await main(); 33})() The only difference in this example is that we pass in 100 to the public API, getting back 100 objects as an array. We then loop through them, making a connection to the RStreams Bus for each and every event. It\u0026rsquo;s simple and it works but this is bad. The putEvent API is really only meant for one or maybe a handful of events. To understand why, consider what the RStreams SDK is doing when you call putEvent.\n It\u0026rsquo;s opening a connection to AWS Kinesis It sending the single event on that connection each time to Kinesis The event flows through Kinesis until an RStreams Kinesis processor reads the single event and writes it to the RStreams Dynamo DB queue table, putting the event in the correct queue  RStreams is designed to handle the continuos generation of data events that flow into a given queue, is read from that queue and mutated and then sent to other queues. It is today doing this with very large amounts of concurrently received events. The RStreams SDK has a better way to work with sending larger amounts of data to the bus, meaning to an RStreams queue.\n  Stream multiple objects to the bus fast It\u0026rsquo;s time to tackle the idea of streams. If you aren\u0026rsquo;t well versed on streams, jump over and read the Streams Primer. It\u0026rsquo;s short and sweet and may well convert you to streams if you aren\u0026rsquo;t already.\n1import { RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const es = rsdk.streams.eventstream; 8 const people = await getRandomPeople(); 9 10 await rsdk.streams.pipeAsync( 11 es.readArray(people.results), 12 rsdk.load(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, 13 {records: 25, time: 5000, useS3: true}) 14 ); 15} 16 17async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 18 const NUM_EVENTS = 100; 19 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 20 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 21 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 22 23 if (status !== 200) { 24 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 25 } 26 27 return data; 28} 29 30(async () =\u0026gt; { 31 await main(); 32})() "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/sink-streams/load/","title":"Load","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a sink stream whose purpose is simply to pull events downstream and then write them to one or more RStreams queues. The second argument of the load function is the queue to send events to that don\u0026rsquo;t designate themselves, via an attribute named event, which queue to push the data to. Note that load cannot today push to multiple queues with the useS3 option set to true.\nThe load stream is often used in conjunction with a through stream that may precede it.\n  When would I use this?  When you want to push events to a single queue at the end of a pipe When you want to push events to multiple queues at the end of a pipe    Runnable Examples   Example 1 Please see Through Example 2 which uses a load stream.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/put/","title":"Put","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1: Write a Single Object to the Bus Example 2: Write multiple objects to the bus (slow performance)        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nA stand-alone function, meaning one that doesn\u0026rsquo;t use pipes and streams, that reads events from the specified source RStreams queue and then calls your transform function allowing you to do anything you want to with the data.\n  When would I use this?  You\u0026rsquo;re in an app and want to send a single event to an RStreams queue on a very infrequent basis You\u0026rsquo;ve got a pipe that does something and you want to enhance it, as the side effect of a given stream step function, to send events to another RStreams queue    Runnable Examples   Example 1: Write a Single Object to the Bus The first example is a naive example that sends data to an RStreams queue one at a time. The code makes a call out to a free API that returns random people, gets a single person back and then on line 6 uses putEvent to send that person to the rstreams-example.people queue, doing so as a bot with ID of rstreams-example.load-people.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\n\r\r Example 1 code\r 1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRaw, PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const person = await getRandomPerson(); 8 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 9} 10 11async function getRandomPerson(): Promise\u0026lt;PersonRaw\u0026gt; { 12 const NUM_EVENTS = 1; 13 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 14 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 15 16 if (status !== 200) { 17 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 18 } 19 20 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 21 22 return data.results[0]; 23} 24 25(async () =\u0026gt; { 26 await main(); 27})() \r\rNote: Person types referenced in the examples\n  View results in Botmon If you go to Botmon, you will see that the rstreams-example.people queue now has an event in it. \r\r\rExpand for Botmon screenshots  \r  Go to Botmon and search for rstreams-example.people in the search field     Botmon now shows a visual representation of the bot and the queue, click on the gear icon after hovering over the queue and then click on Events     Botmon now shows the events loaded into the queue     \r\n  Example 2: Write multiple objects to the bus (slow performance)  This is an example of what not to do. When you want to write many events to an RStreams queue, use the Load Stream pipe step.\n So, instead of reading one person from the public API we used in the example above, let\u0026rsquo;s say we get 100 people at a time from the public API and we want to write them to the bus.\nThe only difference in this example is that we pass in 100 to the public API, getting back 100 objects as an array. We then loop through them, making a connection to the RStreams Bus for each and every event. It\u0026rsquo;s simple and it works but this is bad. The putEvent API is really only meant to be called infrequently for one or maybe a handful of events. To understand why, consider what the RStreams SDK is doing when you call putEvent.\n It\u0026rsquo;s opening a connection to AWS Kinesis It sending the single event on that connection each time to Kinesis The event flows through Kinesis until an RStreams Kinesis processor reads the single event and writes it to the RStreams Dynamo DB queue table, putting the event in the correct queue  RStreams is designed to handle the continuos generation of data events that flow into a given queue, is read from that queue and mutated and then sent to other queues. It is today doing this with very large amounts of concurrently received events and has optimizations for sending lots of data. The Load Stream pipe step is a much better way to send large amounts of data to the bus, meaning to an RStreams queue.\nNote: Person types referenced in the examples\n\r\r Example 2 code\r 1import { ConfigurationResources, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const people = await getRandomPeople(); 8 9 //HINT: this will have very bad performance. This is just to illustrate a point. 10 // Don\u0026#39;t use putEvent in a loop this way in practice, instead use sdk.load! 11 for (const person of people.results) { 12 await rsdk.putEvent(\u0026#39;rstreams-example.load-people\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, person); 13 } 14} 15 16async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 17 const NUM_EVENTS = 100; 18 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;` + 19 `exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 20 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 21 22 if (status !== 200) { 23 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 24 } 25 26 console.log(\u0026#39;Person: \u0026#39; + data.results[0].name.first + \u0026#39; \u0026#39; + data.results[0].name.last); 27 28 return data; 29} 30 31(async () =\u0026gt; { 32 await main(); 33})() \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/source-streams/read/","title":"Read","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a source stream, fed by events from the specified RStreams queue, to act as the first step in a pipe. Just specify the RStreams queue and config to read efficiently and you\u0026rsquo;re done.\n  When would I use this?  I want to use a pipe to have a little more control over processing The data I want to process comes from an RStreams queue    Runnable Examples   Example 1 This example reads 100 events from the rstreams-example.peopleplus RStreams queue and then shuts down the pipe. The read stream sends the events to the devnull stream. illustrates code running as a bot with ID of rstreams-example.people-to-peopleplus and getting exactly two events from queue rstreams-example.people, starting at position z/2022/04/20, and then transforms each event\u0026rsquo;s JSON by dropping unwanted attributes and simplifying the JSON structure. It also calls a totally free, public API that given a country name returns the standard two-char country code which we tack on to the event after which we return the modified event which tells the SDK to push it to the rstreams-example.people-to-peopleplus queue.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\nSecond, there are actually three arguments to the transform function, even though in our example we are only using the first. What is stored in an RStreams queue is an instance of a ReadEvent where the payload attribute is the data the queue exists for. The first argument is just the payload pulled out since usually that\u0026rsquo;s all you need. The second argument is the full event from the queue with the event ID and other sometimes useful things. The third argument is only used in the callback version where you call done exactly once to trigger the callback. It\u0026rsquo;s there for backwared compat. Don\u0026rsquo;t use it on new things.\nThe devnull at the end just acts as a sink and passing in true tells it to log. That\u0026rsquo;s all it\u0026rsquo;s for, to act as a sink. See the doc on Devnull for more details.\n\r\r Example 1 code\r 1import { ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 const opts: ReadOptions = { 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 limit: 5 11 } 12 13 await rsdk.streams.pipeAsync( 14 rsdk.read\u0026lt;Person\u0026gt;(\u0026#39;rstreams-example.peopleplus-to-devnull\u0026#39;, 15 \u0026#39;rstreams-example.peopleplus\u0026#39;, opts), 16 rsdk.streams.devnull(true) 17 ); 18} 19 20(async () =\u0026gt; { 21 try { 22 await main(); 23 } catch(err) { 24 console.log(err); 25 } 26})() \r\r\r\r Example 1 console output\r 1➜ rstreams-runnable-examples ts-node apps/read-events-simple.ts 2Reading event from z/2022/04/20 3devnull { 4 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 5 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 6 \u0026#34;payload\u0026#34;: { 7 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 8 \u0026#34;firstName\u0026#34;: \u0026#34;Herman\u0026#34;, 9 \u0026#34;lastName\u0026#34;: \u0026#34;Morris\u0026#34;, 10 \u0026#34;email\u0026#34;: \u0026#34;herman.morris@example.com\u0026#34;, 11 \u0026#34;birthDate\u0026#34;: \u0026#34;1959-04-25T19:28:13.361Z\u0026#34;, 12 \u0026#34;nationality\u0026#34;: \u0026#34;IE\u0026#34;, 13 \u0026#34;addr\u0026#34;: { 14 \u0026#34;addr1\u0026#34;: \u0026#34;9393 Mill Lane\u0026#34;, 15 \u0026#34;city\u0026#34;: \u0026#34;Killarney\u0026#34;, 16 \u0026#34;state\u0026#34;: \u0026#34;Galway City\u0026#34;, 17 \u0026#34;country\u0026#34;: \u0026#34;Ireland\u0026#34;, 18 \u0026#34;postcode\u0026#34;: 34192, 19 \u0026#34;longitude\u0026#34;: \u0026#34;-48.3422\u0026#34;, 20 \u0026#34;latitude\u0026#34;: \u0026#34;23.2617\u0026#34;, 21 \u0026#34;tzOffset\u0026#34;: \u0026#34;-12:00\u0026#34;, 22 \u0026#34;tzDesc\u0026#34;: \u0026#34;Eniwetok, Kwajalein\u0026#34;, 23 \u0026#34;countryCode\u0026#34;: \u0026#34;IE\u0026#34; 24 } 25 }, 26 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 27 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/21/20/37/1650573479245-0000000\u0026#34;, 28 \u0026#34;correlation_id\u0026#34;: { 29 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 30 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000000\u0026#34;, 31 \u0026#34;units\u0026#34;: 1 32 }, 33 \u0026#34;timestamp\u0026#34;: 1650573479299 34} 35devnull { 36 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 37 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 38 \u0026#34;payload\u0026#34;: { 39 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 40 \u0026#34;firstName\u0026#34;: \u0026#34;Herman\u0026#34;, 41 \u0026#34;lastName\u0026#34;: \u0026#34;Morris\u0026#34;, 42 \u0026#34;email\u0026#34;: \u0026#34;herman.morris@example.com\u0026#34;, 43 \u0026#34;birthDate\u0026#34;: \u0026#34;1959-04-25T19:28:13.361Z\u0026#34;, 44 \u0026#34;nationality\u0026#34;: \u0026#34;IE\u0026#34;, 45 \u0026#34;addr\u0026#34;: { 46 \u0026#34;addr1\u0026#34;: \u0026#34;9393 Mill Lane\u0026#34;, 47 \u0026#34;city\u0026#34;: \u0026#34;Killarney\u0026#34;, 48 \u0026#34;state\u0026#34;: \u0026#34;Galway City\u0026#34;, 49 \u0026#34;country\u0026#34;: \u0026#34;Ireland\u0026#34;, 50 \u0026#34;postcode\u0026#34;: 34192, 51 \u0026#34;longitude\u0026#34;: \u0026#34;-48.3422\u0026#34;, 52 \u0026#34;latitude\u0026#34;: \u0026#34;23.2617\u0026#34;, 53 \u0026#34;tzOffset\u0026#34;: \u0026#34;-12:00\u0026#34;, 54 \u0026#34;tzDesc\u0026#34;: \u0026#34;Eniwetok, Kwajalein\u0026#34;, 55 \u0026#34;countryCode\u0026#34;: \u0026#34;IE\u0026#34; 56 } 57 }, 58 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 59 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645572667-0000134\u0026#34;, 60 \u0026#34;correlation_id\u0026#34;: { 61 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 62 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000000\u0026#34;, 63 \u0026#34;units\u0026#34;: 1 64 }, 65 \u0026#34;timestamp\u0026#34;: 1650645572513 66} 67devnull { 68 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 69 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 70 \u0026#34;payload\u0026#34;: { 71 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 72 \u0026#34;firstName\u0026#34;: \u0026#34;Tomothy\u0026#34;, 73 \u0026#34;lastName\u0026#34;: \u0026#34;Rogers\u0026#34;, 74 \u0026#34;email\u0026#34;: \u0026#34;tomothy.rogers@example.com\u0026#34;, 75 \u0026#34;birthDate\u0026#34;: \u0026#34;1967-01-22T18:32:59.793Z\u0026#34;, 76 \u0026#34;nationality\u0026#34;: \u0026#34;AU\u0026#34;, 77 \u0026#34;addr\u0026#34;: { 78 \u0026#34;addr1\u0026#34;: \u0026#34;6582 Adams St\u0026#34;, 79 \u0026#34;city\u0026#34;: \u0026#34;Kalgoorlie\u0026#34;, 80 \u0026#34;state\u0026#34;: \u0026#34;Australian Capital Territory\u0026#34;, 81 \u0026#34;country\u0026#34;: \u0026#34;Australia\u0026#34;, 82 \u0026#34;postcode\u0026#34;: 8157, 83 \u0026#34;longitude\u0026#34;: \u0026#34;33.3086\u0026#34;, 84 \u0026#34;latitude\u0026#34;: \u0026#34;49.2180\u0026#34;, 85 \u0026#34;tzOffset\u0026#34;: \u0026#34;+5:30\u0026#34;, 86 \u0026#34;tzDesc\u0026#34;: \u0026#34;Bombay, Calcutta, Madras, New Delhi\u0026#34;, 87 \u0026#34;countryCode\u0026#34;: \u0026#34;AU\u0026#34; 88 } 89 }, 90 \u0026#34;event_source_timestamp\u0026#34;: 1650415833985, 91 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645572667-0000135\u0026#34;, 92 \u0026#34;correlation_id\u0026#34;: { 93 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 94 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000001\u0026#34;, 95 \u0026#34;units\u0026#34;: 1 96 }, 97 \u0026#34;timestamp\u0026#34;: 1650645572690 98} 99devnull { 100 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 101 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 102 \u0026#34;payload\u0026#34;: { 103 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 104 \u0026#34;firstName\u0026#34;: \u0026#34;Herman\u0026#34;, 105 \u0026#34;lastName\u0026#34;: \u0026#34;Morris\u0026#34;, 106 \u0026#34;email\u0026#34;: \u0026#34;herman.morris@example.com\u0026#34;, 107 \u0026#34;birthDate\u0026#34;: \u0026#34;1959-04-25T19:28:13.361Z\u0026#34;, 108 \u0026#34;nationality\u0026#34;: \u0026#34;IE\u0026#34;, 109 \u0026#34;addr\u0026#34;: { 110 \u0026#34;addr1\u0026#34;: \u0026#34;9393 Mill Lane\u0026#34;, 111 \u0026#34;city\u0026#34;: \u0026#34;Killarney\u0026#34;, 112 \u0026#34;state\u0026#34;: \u0026#34;Galway City\u0026#34;, 113 \u0026#34;country\u0026#34;: \u0026#34;Ireland\u0026#34;, 114 \u0026#34;postcode\u0026#34;: 34192, 115 \u0026#34;longitude\u0026#34;: \u0026#34;-48.3422\u0026#34;, 116 \u0026#34;latitude\u0026#34;: \u0026#34;23.2617\u0026#34;, 117 \u0026#34;tzOffset\u0026#34;: \u0026#34;-12:00\u0026#34;, 118 \u0026#34;tzDesc\u0026#34;: \u0026#34;Eniwetok, Kwajalein\u0026#34;, 119 \u0026#34;countryCode\u0026#34;: \u0026#34;IE\u0026#34; 120 } 121 }, 122 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 123 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645583644-0000111\u0026#34;, 124 \u0026#34;correlation_id\u0026#34;: { 125 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 126 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000009\u0026#34;, 127 \u0026#34;units\u0026#34;: 10 128 }, 129 \u0026#34;timestamp\u0026#34;: 1650645583447 130} 131devnull { 132 \u0026#34;id\u0026#34;: \u0026#34;rstreams-example.people-to-peopleplus\u0026#34;, 133 \u0026#34;event\u0026#34;: \u0026#34;rstreams-example.peopleplus\u0026#34;, 134 \u0026#34;payload\u0026#34;: { 135 \u0026#34;gender\u0026#34;: \u0026#34;male\u0026#34;, 136 \u0026#34;firstName\u0026#34;: \u0026#34;Tomothy\u0026#34;, 137 \u0026#34;lastName\u0026#34;: \u0026#34;Rogers\u0026#34;, 138 \u0026#34;email\u0026#34;: \u0026#34;tomothy.rogers@example.com\u0026#34;, 139 \u0026#34;birthDate\u0026#34;: \u0026#34;1967-01-22T18:32:59.793Z\u0026#34;, 140 \u0026#34;nationality\u0026#34;: \u0026#34;AU\u0026#34;, 141 \u0026#34;addr\u0026#34;: { 142 \u0026#34;addr1\u0026#34;: \u0026#34;6582 Adams St\u0026#34;, 143 \u0026#34;city\u0026#34;: \u0026#34;Kalgoorlie\u0026#34;, 144 \u0026#34;state\u0026#34;: \u0026#34;Australian Capital Territory\u0026#34;, 145 \u0026#34;country\u0026#34;: \u0026#34;Australia\u0026#34;, 146 \u0026#34;postcode\u0026#34;: 8157, 147 \u0026#34;longitude\u0026#34;: \u0026#34;33.3086\u0026#34;, 148 \u0026#34;latitude\u0026#34;: \u0026#34;49.2180\u0026#34;, 149 \u0026#34;tzOffset\u0026#34;: \u0026#34;+5:30\u0026#34;, 150 \u0026#34;tzDesc\u0026#34;: \u0026#34;Bombay, Calcutta, Madras, New Delhi\u0026#34;, 151 \u0026#34;countryCode\u0026#34;: \u0026#34;AU\u0026#34; 152 } 153 }, 154 \u0026#34;event_source_timestamp\u0026#34;: 1650415833983, 155 \u0026#34;eid\u0026#34;: \u0026#34;z/2022/04/22/16/39/1650645583644-0000112\u0026#34;, 156 \u0026#34;correlation_id\u0026#34;: { 157 \u0026#34;source\u0026#34;: \u0026#34;rstreams-example.people\u0026#34;, 158 \u0026#34;start\u0026#34;: \u0026#34;z/2022/04/20/00/50/1650415834886-0000009\u0026#34;, 159 \u0026#34;units\u0026#34;: 10 160 }, 161 \u0026#34;timestamp\u0026#34;: 1650645583448 162} \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/","title":"Standalone Ops","description":"","content":"\r\r\r ToC\r    \r\rThese powerful standalone operations, meaning without needing to use pipes and streams, do some heavy lifting for you to hide all the complexity of sending events to and getting events from the RStreams bus.\n put Operation A function that lets you write a single event to the specified RStreams queue enrich Operation A function that reads from the specified source RStreams queue, lets you transform the events and then sends the modified events to the specified destination RStreams queue offload Operation A function that reads from the specified RStreams queue and lets you do something with the events retrieved, perhaps save them in a DB  "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/transform-streams/stringify/","title":"Stringify","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a transform stream, meaning a stream that exists to receive events after the source stream, do something with them and then send them on to the next pipe step, which must exist.\nIt takes each event, stringifies it and tacks on a newline character at the end and sends that string, with the newline, on to the next step in the pipe. It is used to create json lines content to either feed to an s3 file or just a file one the local file system.\n  When would I use this?  I want to make a JSON lines file from the events flowing through the stream    Runnable Examples   Example 1 This example reads 5 events from the rstreams-example.peopleplus RStreams queue. The pipe then creates a throughAsync stream step that just takes the ReadEvent\u0026lt;Person\u0026gt; events read from the bus and turns it into a PersonLight event and sends it on to the stringify stream to make a json line ready to stick in a json lines file.\nFinally, it writes the file using the Node standard filesystem fsmodule to create a sink that writes events that flow into the sink to a file. Pretty convenient.\n\r\r Example 1 code\r 1import { ReadEvent, ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import fs from \u0026#34;fs\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 const opts: ReadOptions = { 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 limit: 5 11 } 12 13 await rsdk.streams.pipeAsync( 14 rsdk.read\u0026lt;Person\u0026gt;(\u0026#39;rstreams-example.peopleplus-to-jsonlines\u0026#39;, 15 \u0026#39;rstreams-example.peopleplus\u0026#39;, opts), 16 rsdk.streams.throughAsync\u0026lt;ReadEvent\u0026lt;Person\u0026gt;, PersonLight\u0026gt;(async (p: ReadEvent\u0026lt;Person\u0026gt;) =\u0026gt; { 17 return { 18 firstName: p.payload.firstName, 19 lastName: p.payload.lastName, 20 email: p.payload.email 21 } 22 }), 23 rsdk.streams.stringify(), 24 fs.createWriteStream(\u0026#34;./output/people.jsonl\u0026#34;), 25 ); 26} 27 28interface PersonLight { 29 firstName: string; 30 lastName: string; 31 email: string; 32} 33 34(async () =\u0026gt; { 35 try { 36 await main(); 37 } catch(err) { 38 console.log(err); 39 } 40})() \r\r\r\r Generated people.jsonl file\r 1{\u0026#34;firstName\u0026#34;:\u0026#34;Herman\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Morris\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;herman.morris@example.com\u0026#34;} 2{\u0026#34;firstName\u0026#34;:\u0026#34;Herman\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Morris\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;herman.morris@example.com\u0026#34;} 3{\u0026#34;firstName\u0026#34;:\u0026#34;Tomothy\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Rogers\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;tomothy.rogers@example.com\u0026#34;} 4{\u0026#34;firstName\u0026#34;:\u0026#34;Herman\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Morris\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;herman.morris@example.com\u0026#34;} 5{\u0026#34;firstName\u0026#34;:\u0026#34;Tomothy\u0026#34;,\u0026#34;lastName\u0026#34;:\u0026#34;Rogers\u0026#34;,\u0026#34;email\u0026#34;:\u0026#34;tomothy.rogers@example.com\u0026#34;} \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/transform-streams/through/","title":"Through","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1 Example 2        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n Only use the throughAsync SDK method and not through as it is not needed and is deprecated.\n API Doc\nThis function creates a transform stream, meaning a stream that exists to receive events after the source stream, do something with them and then send them on to the next pipe step, which must exist.\nNote that through intentionally doesn\u0026rsquo;t know anything about RStreams events. All it does it take in what it is given from the previous pipe step and then send on what you return from the function to the next pipe step.\nFor example, if your source pipe step produces an object of type Person then you are going to get an object of type Person and not ReadEvent\u0026lt;Person\u0026gt;. The small example below uses the popular event-stream library exported by the RStreams SDK to turn an array into a source stream to seed the pipe with content from the array.\n1const arr: Person[] = [{name: \u0026#39;jane doe\u0026#39;}]; 2await rsdk.streams.pipeAsync( 3 rsdk.streams.eventstream.readArray(people.results), 4 rsdk.throughAsync\u0026lt;Person\u0026gt;(event) { 5 event.name = \u0026#39;john doe\u0026#39;; 6 return event; 7 }, 8 rsdk.streams.devnull() 9);   When would I use this?  I want to do something more involved so enrich doesn\u0026rsquo;t work for me I want to take data events provided and transform them and send them through to the next pipe step    Runnable Examples   Example 1 This example reads 5 events from the rstreams-example.peopleplus RStreams queue. The pipe then creates a throughAsync stream step that just takes the ReadEvent\u0026lt;Person\u0026gt; events read from the bus and turns it into a PersonLight event and sends it on to the toCSV stream to make a CSV line ready to stick in a CSV file.\nFinally, it writes the file using the Node standard filesystem fsmodule to create a sink that writes events that flow into the sink to a file. Pretty convenient.\nThe toCSV function\u0026rsquo;s first argument, if true, writes a CSV header as the first row. If the toCSV function\u0026rsquo;s first argument is an array of strings, it uses that as the CSV header first row. The second arg is options that come from the underlying fast-csv NPM module that generates the CSV file: fast-csv options.\n\r\r Example 1 code\r 1import { ReadEvent, ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import fs from \u0026#34;fs\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 const opts: ReadOptions = { 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 limit: 5 11 } 12 13 await rsdk.streams.pipeAsync( 14 rsdk.read\u0026lt;Person\u0026gt;(\u0026#39;rstreams-example.peopleplus-to-jsonlines\u0026#39;, 15 \u0026#39;rstreams-example.peopleplus\u0026#39;, opts), 16 rsdk.throughAsync\u0026lt;ReadEvent\u0026lt;Person\u0026gt;, PersonLight\u0026gt;(async (p: ReadEvent\u0026lt;Person\u0026gt;) =\u0026gt; { 17 return { 18 firstName: p.payload.firstName, 19 lastName: p.payload.lastName, 20 email: p.payload.email 21 } 22 }), 23 rsdk.streams.toCSV(true, {quote: \u0026#39;\u0026#34;\u0026#39;}), 24 fs.createWriteStream(\u0026#34;./output/people.csv\u0026#34;), 25 ); 26} 27 28interface PersonLight { 29 firstName: string; 30 lastName: string; 31 email: string; 32} 33 34(async () =\u0026gt; { 35 try { 36 await main(); 37 } catch(err) { 38 console.log(err); 39 } 40})() \r\r\r\r Generated people json lines file\r firstName,lastName,email Herman,Morris,herman.morris@example.com Herman,Morris,herman.morris@example.com Tomothy,Rogers,tomothy.rogers@example.com Herman,Morris,herman.morris@example.com Tomothy,Rogers,tomothy.rogers@example.com \r\rNote: Person types referenced in the examples\n  Example 2 Here we see the real power of a through. We are reading data from a stream populated with people that are of type PersonRaw. We want to read them, translate them from PersonRaw to a simpler type named Person and then look at the country on each object and push those that live in the US to a queue named rstreams-example.peopleplus-us and the others to a queue named rstreams-example.peopleplus.\nThings to learn from this example:\n  Reading Data We are only reading up to 5 objects and then closing the source stream which closes down the pipe\nWe don\u0026rsquo;t have a batch stream step between the read and the throughAsync since we don\u0026rsquo;t need it because the logic we have in the throughAsync doesn\u0026rsquo;t do anything that takes time. If we reached out to say an API or database here to further enrich the person with external data, then we would absolutely want to use the batch stream step to micro-batch and get arrays of data sent to the throughAsync with a type of Array\u0026lt;ReadEvent\u0026lt;PersonRaw\u0026gt;\u0026gt;. This would then let you hit the database once per group of events, perhaps, building a single SQL statement to get all the data so your through doesn\u0026rsquo;t slow down.\n  Creating a BaseEvent Note that throughAsync doesn\u0026rsquo;t wrap your return value in a BaseEvent which is what the load needs so we make one ourself and return it. Normally, there are only three attributes you need to care about:\n payload: the data this event exists to wrap event_source_timestamp: this is when the very first event hit any queue of the bus that eventually led to this event, no matter how many queues it flowed through to get here and how the event was transformed along the way. So, copying the source event\u0026rsquo;s (PersonRaw) event_source_timestamp is almost always the right thing to do so we can carry forward this date. It is used to understand how long it has taken for an object to propagate through the system and is very important. correlation_id: The short answer is that you need to have this so be sure to just use the helper API rsdk.streams.createCorrelation to make one for you. What is correlation_id?  Normally, we let the SDK set the other things on the BaseEvent for us. If we don\u0026rsquo;t include the event attribute, which is what queue to send the event to, then the downstream load stream\u0026rsquo;s default queue value will be put on the event and used to send the event to that queue. The second argument to the load on Line 28 is the default queue to send events to that don\u0026rsquo;t have the event attribute to tell the SDK which queue to send things to.\nrstreams-example.peopleplus\n\r\r Example 2 code\r 1import { BaseEvent, ReadEvent, ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRaw, Person } from \u0026#39;../lib/types\u0026#39;; 3 4async function main() { 5 const rsdk: RStreamsSdk = new RStreamsSdk(); 6 const es = rsdk.streams.eventstream; 7 const botId = \u0026#39;rstreams-example.people-to-peopleplusandus\u0026#39;; 8 9 const readOpts: ReadOptions = { 10 start: \u0026#39;z/2022/04/20\u0026#39;, 11 limit: 5 12 } 13 14 await rsdk.streams.pipeAsync( 15 rsdk.read\u0026lt;PersonRaw\u0026gt;(botId, \u0026#39;rstreams-example.people\u0026#39;, readOpts), 16 rsdk.throughAsync\u0026lt;ReadEvent\u0026lt;PersonRaw\u0026gt;, BaseEvent\u0026lt;Person\u0026gt;\u0026gt;((event) =\u0026gt; { 17 const queue = event.payload.location.country === \u0026#39;United States\u0026#39; ? 18 \u0026#39;rstreams-example.peopleplus\u0026#39; : 19 \u0026#39;rstreams-example.peopleplus-us\u0026#39; 20 const result: BaseEvent\u0026lt;Person\u0026gt; = { 21 event: queue, 22 payload: translate(event.payload), 23 event_source_timestamp: event.event_source_timestamp, 24 correlation_id: rsdk.streams.createCorrelation(event) 25 }; 26 return result; 27 }), 28 rsdk.load(botId, \u0026#39;rstreams-example.peopleplus\u0026#39;, {force: true}) 29 ); 30} 31 32/** 33* @param p The type from the public API we want to modify 34* @returns The new type that is flatter and gets rid of some attributes don\u0026#39;t need 35*/ 36 function translate(p: PersonRaw): Person { 37 return { 38 gender: p.gender, 39 firstName: p.name.first, 40 lastName: p.name.last, 41 email: p.email, 42 birthDate: p.dob.date, 43 nationality: p.nat, 44 addr: { 45 addr1: p.location.street.number + \u0026#39; \u0026#39; + p.location.street.name, 46 city: p.location.city, 47 state: p.location.state, 48 country: p.location.country, 49 postcode: p.location.postcode, 50 longitude: p.location.coordinates.longitude, 51 latitude: p.location.coordinates.latitude, 52 tzOffset: p.location.timezone.offset, 53 tzDesc: p.location.timezone.description 54 } 55 } 56 } 57 58(async () =\u0026gt; { 59 await main(); 60})() \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/transform-streams/tocsv/","title":"ToCSV","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a transform stream, meaning a stream that exists to receive events after the source stream, do something with them and then send them on to the next pipe step, which must exist.\nIt takes each event, and turns it into a CSV line ready to be written to a CSV file.\n  When would I use this?  I want to generate a CSV file from the events flowing through the stream    Runnable Examples   Example 1 This example reads 5 events from the rstreams-example.peopleplus RStreams queue. The pipe then creates a throughAsync stream step that just takes the ReadEvent\u0026lt;Person\u0026gt; events read from the bus and turns it into a PersonLight event and sends it on to the toCSV stream to make a CSV line ready to stick in a CSV file.\nFinally, it writes the file using the Node standard filesystem fsmodule to create a sink that writes events that flow into the sink to a file. Pretty convenient.\nThe toCSV function\u0026rsquo;s first argument, if true, writes a CSV header as the first row. If the toCSV function\u0026rsquo;s first argument is an array of strings, it uses that as the CSV header first row. The second arg is options that come from the underlying fast-csv NPM module that generates the CSV file: fast-csv options.\n\r\r Example 1 code\r 1import { ReadEvent, ReadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import fs from \u0026#34;fs\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 const opts: ReadOptions = { 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 limit: 5 11 } 12 13 await rsdk.streams.pipeAsync( 14 rsdk.read\u0026lt;Person\u0026gt;(\u0026#39;rstreams-example.peopleplus-to-jsonlines\u0026#39;, 15 \u0026#39;rstreams-example.peopleplus\u0026#39;, opts), 16 rsdk.streams.throughAsync\u0026lt;ReadEvent\u0026lt;Person\u0026gt;, PersonLight\u0026gt;(async (p: ReadEvent\u0026lt;Person\u0026gt;) =\u0026gt; { 17 return { 18 firstName: p.payload.firstName, 19 lastName: p.payload.lastName, 20 email: p.payload.email 21 } 22 }), 23 rsdk.streams.toCSV(true, {quote: \u0026#39;\u0026#34;\u0026#39;}), 24 fs.createWriteStream(\u0026#34;./output/people.csv\u0026#34;), 25 ); 26} 27 28interface PersonLight { 29 firstName: string; 30 lastName: string; 31 email: string; 32} 33 34(async () =\u0026gt; { 35 try { 36 await main(); 37 } catch(err) { 38 console.log(err); 39 } 40})() \r\r\r\r Generated people.jsonl file\r 1firstName,lastName,email 2Herman,Morris,herman.morris@example.com 3Herman,Morris,herman.morris@example.com 4Tomothy,Rogers,tomothy.rogers@example.com 5Herman,Morris,herman.morris@example.com 6Tomothy,Rogers,tomothy.rogers@example.com \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/what-is-rstreams/why-rstreams/","title":"Why RStreams?","description":"","content":"\r\r\r ToC\r   Why RStreams? Why not just AWS services? Design Priorities  Data is the Feature Favor AWS Services over Proprietary Solutions Use the Right Tool/Database/Service for the Job Time Matters      \r\r  Why RStreams? Why not just AWS services? RStreams provides a near real-time messaging and streaming platform that knits together underlying AWS services to do the heavy lifting. Why bother with this? Why not just use the AWS services themselves?\nAWS is hard. It’s not just that there’s a lot to learn on a given service, it’s that each service has its own sweet spot for the use cases it solves and none provide a one stop shop for event streaming and general messaging. When you step outside that sweet spot of given service, you run smack into one or more serious limitations: latency, message size, write throughput, concurrent read limits, lack of durability, read and write performance coupling, no persistence after event handling, lack of backpressure, etc. AWS expects developers to tie the different services together to get around the natural limitations of a given service: SNS tied to SQS queues, Kinesis tied into EventBridge tied into SQS.\nBridging across AWS’ services to create a comprehensive solution that works in all circumstances is the right thing to do. The problem is that it means that every team or project has to learn these non-obvious pitfalls/limitations and how to overcome them by connecting many AWS services together at scale. This will often make a conceptually very simple task quite hard to implement at scale with good debugging/monitoring/visibility across the entire thing.\nThis is the problem RStreams solves. It makes conceptually simple things remain simple for developers by connecting the AWS services together with a very thin veneer using a simple, consistent abstraction. The fundamental premise is that it should be easier to learn RStreams than to have to learn how and when to tie all these AWS services together for the features a project needs. However, when problems occur, it should not get in the way of the underlying AWS services with their large community support.\n  Design Priorities With more than one right way to solve problems, design priorities make clear why tech decisions on RStreams were made. RStreams chose event streaming based on queues as the foundational pattern that drives everything else and much of what follows won’t make sense without context on event streaming. Read the event streaming article for a lot more on this.\n  Data is the Feature Big data has changed the world. RStreams is a tech stack and patterns that best supports the movement, transformation, persistence and ability to react to data when creating solutions and not necessarily the actual features that product management asks for. The features product management asks for should naturally and easily fall out as a side effect of the data architecture of a project. If the features don’t fall out naturally and easily from the data architecture, new features will be very hard, suffer performance/scale issues, add immense complexity and result in the creation of vast silos of data/functionality to do what would otherwise be easy.\n  Favor AWS Services over Proprietary Solutions Those who built RStreams believed strongly that if an engineering team leverages cloud services, and software that natively takes advantage of these services, that this will allow the engineering teams to get an outsized outcome for their efforts. This is one reason that Kafka, a leading event streaming platform, wasn’t chosen at the time - it wasn’t built on AWS' most scaleable services.\n  Use the Right Tool/Database/Service for the Job Data storage is far cheaper than the opportunity cost of not using the right tool/database/service for the job. So, don’t fear replicating data from the source-of-truth database to purpose-built tools/databases/AWS services that excel at a specific job.\n  Time Matters The time ordering of events in systems that are continuously receiving and propagating events matters and if ignored will push complexity to individual engineers and teams, resulting in an entire class of bugs for CommerceHub developers and its customers that would otherwise just go away.\n"},{"url":"https://rstreams.org/rstreams-guides/core-concepts/fundamentals/","title":"Fundamentals","description":"Fundamental concepts.","content":"\r\r\r ToC\r       Event Queue The Bus Bot Event ID Pipe Pipe Stream Step Stream Checkpoint Event source timestamp Correlation ID Stage System        \r\r  Event All data that is sent to an RStreams queue is an event. Each event exists to wrap the data contained in the payload attribute. Queues hold events of a consistent type, such as all Employee objects or all Change Order request objects for example.\n  Queue RStreams is factored around queues. A queue is a named set of events in time sequence order, ordered by their event ID. An event in a queue is persisted by virtue of being in a queue. Bots register themselves to be invoked when events arrive in a queue. Bots read data from queues and send new data to other queues.\n  The Bus A term used to mean a single instance of RStreams with its associated queues and the bots that react to and populate them.\n  Bot A bot as a logical wrapper around a unit of work that will read data from an RStreams queue or write data to an RStreams queue or both. Bots may be registered to be invoked when new events show up in a given queue. Bots may be scheduled to run periodically using standard cron syntax.\n  Event ID Every event in a queue has an event ID, which uniquely identifies it in the queue. The event ID is also its position in the queue because event IDs are actually a form of date/time (UTC) value. Here\u0026rsquo;s one: z/2021/06/14/17/19/1623691151425-0000013\n z/ : all event IDs start with this to identify them as event IDs 2021 : the year of the event 06 : the month of the event 14 : the day of the event 17 : the hour of the event 19 : the minute of the event 1623691151425 : the millisecond of the event 0000013 : the sequence number in that millisecond, so this is the 13 event in this one millisecond that came into the queue    Pipe A function that takes as its arguments a set of stream steps where the pipe begins with a source stream that produces data to seed the pipe with, an optional set of transform streams that take in data, do something with it and send it to the next stream in the pipe and the sink stream that ends the pipe.\n  Pipe Stream Step A single stream in a pipe. It\u0026rsquo;s called a step because each stream in a pipe receives data from the previous step in the pipe and sends data to the next step in the pipe.\n  Stream A set of RStreams queues and bots that chain together to create a directed graph of moving data with upstream queues visualized as the sources that initially receive events, think leftmost if visualizing, and downstream queues getting data from the previous stream step, think rightmost if visualizing.\n  Checkpoint A checkpoint is a saved position in a stream. The Node SDK maintains the read checkpoint for all bots that read from a given queue. The Node SDK maintains a write checkpoing for all bots that write to a given queue. When a bot is restarted and starts reading from a queue, it will by default begin reading from its checkpoint (think position) in the queue.\n  Event source timestamp Every event that hits a queue came from somewhere originally. Initially, perhaps it was loaded into a queue from a database. Then, a bot read from the queue and let\u0026rsquo;s say transformed the event and put it in another queue. We want all derived events to reference when the source event that led to the derived events was put into and RStreams queue so we can track overall transit times for that source event. It\u0026rsquo;s a simple idea but very important. It requires that when engineers manually craft their own events that flow through the bus that they simply take the effort to copy the parent event\u0026rsquo;s event_source_timestamp onto their new derived event and put it in the new event\u0026rsquo;s event_source_timestamp so the value propagates through.\n  Correlation ID Every event that hits a queue came from somewhere originally and we want to be able to trace the movement of an event through the various queues of the bus, knowing the parent queue and the exact event in the parent queue that a given event was derived from. We accomplish this by keeping track of what parent queue/event ID an event was derived from in a bookkeeping object on the event named the correlation_id. Also, the SDK cannot checkpoint for you if your events don\u0026rsquo;t have a valid correldation_id.\nThis is so important that when developers need to craft an event by hand that they should simply use the rsdk.streams.createCorrelation helper API from the SDK to create this object for them by passing in the parent event to the createCorrelation API as they are building a new derived event.\nThe helper API is useful because in the case where a bot turns N upstream events into 1 downstream event, perhaps aggregating or reducing data, there isn\u0026rsquo;t one parent event a single event was derived from but there are N events. Let\u0026rsquo;s say that we have a through pipe step that aggregates every 10 parent queue events into 1 new event. Well, then in that case the correlation_id object will need to include the parent queue\u0026rsquo;s event ID of the first event of the ten and the last event ID of the ten that this one new event derived from.\nHere\u0026rsquo;s an example object that might represent those ten.\n1{ 2 source: \u0026#39;rstreams-example.people\u0026#39;, 3 start: \u0026#39;z/2022/04/20/00/50/1650415834886-0000002\u0026#39;, 4 end: \u0026#39;z/2022/04/20/00/50/1650415834886-00000012\u0026#39;, 5 units: 10 6}  source: the upstream queue this event derived from start: the event ID of the starting event that this event derived from end: if present and if different than start then this it\u0026rsquo;s the event ID of the last event that this event derived from in the upstream queue and if different than start it means that this one event was derived from the number of events between start and end units: the number of parent queue events this one event was derived from  Note that when an event was derived from an external system, say a database, that has been setup as a System in RStreams that this bookkeeping may point to the database table as the source as the start/end may be where in the database it came from.\n  Stage One of the set of independent environments supported for deployment, e.g. production, staging, development, etc.\n  System A resource external to RStreams, such as a database, that has been registered in RStreams so it may be visualized as botmon as a queue of data and optionally used to designate correlation information specific to that external system.\n"},{"url":"https://rstreams.org/rstreams-flow/","title":"RStreams Flow","description":"Be up and running in 10 minutes.","content":"RStreams Flow is a project template that creates a new empty project to act as a new microservice for you that has everything an engineer needs to hit the ground running.\nProjects created from the RStreams Flow template include libraries and tech that solve common development needs out of the box. Thus, RStreams Flow prjoects include the RStreams Flow opinionated development decisions to makes choices on how to build, test, deploy, monitor and maintain RStreams microservices. Engineers don’t care about RStreams. They care about creating reactive microservices that leverage native AWS services at scale. RStreams Flow helps engineers just do their work.\nIn order to make things just work, RStreams Flow projects have an opinion on:\n Running locally Configuration management Building, deploying an CI/CD Project organization    Released or enhanced 05/09/2022  Full type support in VS Code Invoke a bot locally Invoke a series of bots/queues locally Invoke a bot locally hitting queues in the cloud Invoke a bot pulling mock data from a file acting as a queue Unit testing Debugging locally Project organization    Coming in the next 60 days (end of June 2022)  Project configuration : Coming Early June 2022 Building / bundling artifacts : Coming Early June 2022 Deploying : Coming June 2022 Monitoring : Coming June 2022  RStreams Flow will include a tool to help an RStreams Flow project migrate from one version to a newer version in order to keep getting new capabilities and features.\n"},{"url":"https://rstreams.org/rstreams-guides/checkpointing/","title":"Checkpointing","description":"","content":"\r\r\r ToC\r   Summary Checkpointing and Correlation Read Checkpoints Read Checkpointing for Offload and Enrich Read Checkpointing for a Pipe Read Checkpoint Examples  Example 1 Example 2 Example 3 Example 4      \r\r  Summary A checkpoint is a saved position in an RStreams queue. The Node SDK maintains the read checkpoint for all bots that read from a given queue. The Node SDK maintains a write checkpoing for all bots that write to a given queue. When a bot is restarted and starts reading from a queue, it will by default begin reading from its checkpoint (think position) in the queue.\nSo, \u0026ldquo;to checkpoint\u0026rdquo; or the act of \u0026ldquo;checkpointing\u0026rdquo; means using the SDK to write the read or write checkpoint for a bot for the stream it is reading from or writing to.\nMost of the time, the SDK automatically checkpoints for you and you don\u0026rsquo;t need to care. However, you need to understand generally what it is so you don\u0026rsquo;t get in trouble.\n   Checkpointing and Correlation IF YOU HAVE AN EVENT THAT DOESN\u0026rsquo;T INCLUDE A VALID CORRELATION_ID THEN THE SDK CANNOT CHECKPOINT FOR YOU.\nIf the above sentence is all you remember from this article, then victory is assured :)\nAn event has an object called correlation_id. Jump over to the fundamentals doc and read the brief section on Correlation ID if you haven\u0026rsquo;t already.\nThe SDK will checkpoint for you if your event has a correlation_id and without it the SDK can\u0026rsquo;t. Why? There is no magic to how the SDK works. The correlation information tells the SDK the source queue and exact event that an event came from and without this it simply can\u0026rsquo;t save your bot\u0026rsquo;s read or write position in a queue.\n  Read Checkpoints When your bot reads an event from an RStreams queue, you are going to do something with that event and then want to read the next event in the queue. What if you read event A and before you process the event your bot crashes and shuts down.\nWhen your bot restarts, you want to read event A. The good news is that all events in RStreams queues are persisted so no worries, your event is still there. But, where was event A in the queue? Each event has an ID that both uniquely identifies that event and marks it time-based position in that queue. So, if you know know the time of the last event in the queue was written to that queue, you could start reading from about that position by crafting an event ID that maybe gets you close to where you were reading. Here\u0026rsquo;s an event Id that will start reading the first event in the 13th minute of April 13 at 5PM UTC.\nz/2022/04/13/17/13\nBut, that\u0026rsquo;s not great. So, the SDK keeps track of this for you. Here\u0026rsquo;s how?\n  Read Checkpointing for Offload and Enrich When you have an operation that reads from a queue, such as offload or enrich, the SDK will checkpoint events read from the source queue when events flow out of the sink step that is hidden within these operations. In the case of offload, that\u0026rsquo;s when the offload function returns. In the cae of enrich, it\u0026rsquo;s when an event is written to the destination queue.\nHere\u0026rsquo;s exactly what \u0026ldquo;checkpoint events\u0026rdquo; means. It means that periodically, it will send an update back to the RStreams Bus to have it save this bot\u0026rsquo;s read position in the queue the bot is reading data from using the correlation_id of events that have flowed into the offload and the function has returned without error and in the case of enrich for the events that have been successfully written to the destination queue.\nSo, each time your bot starts up it will automatically call out to the RStreams Bus and ask for this bot\u0026rsquo;s current read position, the checkpoint, in the given queue it\u0026rsquo;s reading from and will continue pulling events from that position forward in time through the queue.\n  Read Checkpointing for a Pipe If your last pipe stream step, your sink, writes to a queue using a load stream, then periodically as the SDK writes the events to the desination queue it will also updatethis bot\u0026rsquo;s checkpoint in the queue that you read from in your source stream.\n  Read Checkpoint Examples   Example 1 Here we are creating a source stream that goes out and gets data not yet in a queue of the RStreams Busand we write it to the RStreams queue named my-destination-queue acting as a bot named my-cool-bot. The SDK will not checkpoint what you are reading from since your source isn\u0026rsquo;t an RStreams queue. May sound obvious but just need to do a gut check that folks are getting it.\n\r\r Example 1 code\r 1import { RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2 3async function main() { 4 const rsdk: RStreamsSdk = new RStreamsSdk(); 5 6 await rsdk.streams.pipeAsync( 7 rsdk.createSource(async () =\u0026gt; { 8 // Call out to a database to feed data into the pipe 9 }, opts, state), 10 rsdk.load(\u0026#39;my-cool-bot\u0026#39;, \u0026#39;my-destination-queue\u0026#39;) 11 ); 12} \r\r  Example 2 Here we are reading events as a bot named my-cool-bot and writing them to another system, in this case some database external to RStreams. Notice that on line 12 we return true after saving the event to the database. This tells the SDK that processing was successful and that we should checkpoint the position of this event for the my-cool-bot in the my-source-queue queue.\n\r\r Example 2 code\r 1import { RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2 3async function main() { 4 const rsdk: RStreamsSdk = new RStreamsSdk(); 5 const opts: OffloadOptions\u0026lt;Person\u0026gt; = { 6 id: \u0026#39;my-cool-bot\u0026#39;, 7 inQueue: \u0026#39;my-source-queue\u0026#39;, 8 start: \u0026#39;z/2022/04/20\u0026#39;, 9 limit: 2, 10 transform: async (person: Person) =\u0026gt; { 11 // Save the person to a database 12 return true; 13 } 14 }; 15 16 await rsdk.offloadEvents\u0026lt;Person\u0026gt;(opts); 17} \r\r  Example 3 Here we are reading events as a bot named my-cool-bot from a queue named my-source-queue, translating them from a PersonRaw to a Person object and then writing them a queue named my-dest-queue. You\u0026rsquo;ll notice that on line 15 we are returning the newly translated person to be sent to the my-dest-queue queue. The SDK will checkpoint the original event from my-source-queue for you after it\u0026rsquo;s sure that the derived event that you returned from the transform function was successfully written to the my-dest-queue queue.\nLet\u0026rsquo;s say that you wanted to skip one person event and not write it to the dest queue but you still wanted to checkpoint the source event from the souce queue? Well, in that case you\u0026rsquo;d just do this for that one event return true; which tells the SDK not to send an event to the destination but to go ahead and checkpoint in the source queue that the bot handled the event.\n\r\r Example 3 code\r 1import { EnrichOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2 3async function main() { 4 const rsdk: RStreamsSdk = new RStreamsSdk(); 5 const opts: EnrichOptions\u0026lt;PersonRaw, Person\u0026gt; = { 6 id: \u0026#39;my-cool-bot\u0026#39;, 7 inQueue: \u0026#39;my-source-queue\u0026#39;, 8 outQueue: \u0026#39;my-dest-queue\u0026#39;, 9 start: \u0026#39;z/2022/04/20\u0026#39;, 10 config: { 11 limit: 2 12 }, 13 transform: async (person: PersonRaw) =\u0026gt; { 14 const p: Person = translate(person); 15 return p; 16 } 17 }; 18 19 await rsdk.enrichEvents\u0026lt;PersonRaw, Person\u0026gt;(opts); 20} \r\r  Example 4 Here we are reading events from queue my-source-queue as a bot named my-cool-bot and then transforming the event from a PersonRaw to a Person object and then sending the new Person object to the queue named my-dest-queue doing so as a bot named my-cool-bot.\nThe SDK will checkpoint that we\u0026rsquo;ve read and handled the event in my-source-queue for us only after it\u0026rsquo;s sure that the new derived event has been written to my-dest-queue. That\u0026rsquo;s why it is so important that derived events have the correct correlation_id correlation bookkeeping information. The SDK uses that in derived events to know what source event correlates to the new event so it can checkpoint for us.\n\r\r Example 4 code\r 1import { BaseEvent, ReadEvent, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRaw, Person } from \u0026#39;../lib/types\u0026#39;; 3 4async function main() { 5 const rsdk: RStreamsSdk = new RStreamsSdk(); 6 const botId = \u0026#39;rstreams-example.people-to-peopleplusandus\u0026#39;; 7 8 await rsdk.streams.pipeAsync( 9 rsdk.read\u0026lt;PersonRaw\u0026gt;(\u0026#39;my-cool-bot\u0026#39;, \u0026#39;my-source-queue\u0026#39;), 10 rsdk.throughAsync\u0026lt;ReadEvent\u0026lt;PersonRaw\u0026gt;, BaseEvent\u0026lt;Person\u0026gt;\u0026gt;((event) =\u0026gt; { 11 const result: BaseEvent\u0026lt;Person\u0026gt; = { 12 payload: translate(event.payload), 13 event_source_timestamp: event.event_source_timestamp, 14 correlation_id: rsdk.streams.createCorrelation(event) 15 }; 16 17 return result; 18 }), 19 rsdk.load(\u0026#39;my-cool-bot\u0026#39;, \u0026#39;my-dest-queue\u0026#39;, {force: true}) 20 ); 21} \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/source-streams/createsource/","title":"Create Source","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a source stream, fed by events from the array that you return from your CreateSourceFunction, to act as the first step in a pipe.\n  When would I use this?  I want to create a pipe and seed the pipe from data from a database at scale I want to create a pipe and seed the pipe with data from an API at scale I want to create a pipe and seed the pipe with data from [fill-in-the-blank] at scale    Runnable Examples   Example 1 There\u0026rsquo;s a fair bit going on here so stay with me. We read events from from a public free API that generates random people, seeding a pipe by creating a source stream using the createSource API and then write them to the bus landing in the rstreams-example.people queue.\n Line 8 We define a type for the state we want the SDK to pass into each time it calls our function to generate more content to feed the stream. The createSource function lets you specify that you want state passed in with each invocation of your function and then lets you initialize that state for the first time your function is called. Then, you can change that state in your function and it will be passed to each subsequent invocation. Line 10 We are defining the options we want to pass into the createSource function. Here we are telling the SDK to close the source stream and thus shutdown the pipe after ten seconds. Line 11 We are creating an instance of our state. We only want to call out to the free public API that generates random people for us five times. So, we initialize our state to 5 and then in our actual function, below, we decrement that state. When it gets to zero, we simply return from the function which tells the SDK to close the stream and shut down the pipe. Line 14 We are creating a new source steam and specifying that the source stream will be returning arrays of PersonRaw objects and also that we are going to be asking the SDK to pass in a state object of type SourceState. Then we pass as the first argument to the createSource function an anonymous function of type CreateSourceFunction that wil be called each time the stream needs more data. There is an optional argument which is state that will be passed in by the SDK on our behalf each time our function is invoked. Lines 15 and 16 We grab the state into a local variable and then decrement that state number itself so that it will be changed on subsequent invocations to this function. Lines 17 and 18 If our counter is at 0, we don\u0026rsquo;t want to continue and so we return nothing which tells the SDK we\u0026rsquo;re don. It will close our stream which will cause the pipe to flush and then close down. Lines 20 and 21 We\u0026rsquo;re not done so lets get more data. Line 20 is a call to a function below that just makes a call out to get 100 random people objects from a public free API. Line 21 just returns the array of PersonRaw objects we got from the API. Line 23 The second argument and the third argument to the createSource function: the optional options and optional initial state respectively. Line 24 We create a write stream sink to write all events that make it to the sink to the rstreams-example.people queue doing so as the bot rstreams-example.load-people-faster.  \r\r Example 1 code\r 1import { CreateSourceOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { PersonRawResults, PersonRaw } from \u0026#39;../lib/types\u0026#39;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 8 interface SourceState {numApiCalls: number;} 9 10 const opts: CreateSourceOptions = {milliseconds: 10000}; 11 const state: SourceState = {numApiCalls: 5}; 12 13 await rsdk.streams.pipeAsync( 14 rsdk.createSource\u0026lt;PersonRaw, SourceState\u0026gt;(async (state) =\u0026gt; { 15 const numApiCalls = state.numApiCalls; 16 state.numApiCalls--; 17 if (numApiCalls === 0) { 18 return; 19 } else { 20 const prr: PersonRawResults = await getRandomPeople(); 21 return prr.results; 22 } 23 }, opts, state), 24 rsdk.load(\u0026#39;rstreams-example.load-people-faster\u0026#39;, \u0026#39;rstreams-example.people\u0026#39;, 25 {records: 25, time: 5000, useS3: true}) 26 ); 27} 28 29async function getRandomPeople(): Promise\u0026lt;PersonRawResults\u0026gt; { 30 const NUM_EVENTS = 100; 31 const url = `https://randomuser.me/api/?results=${NUM_EVENTS}\u0026amp;exc=login,registered,phone,cell,picture,id\u0026amp;noinfo`; 32 const {data, status} = await axios.get\u0026lt;PersonRawResults\u0026gt;(url); 33 34 if (status !== 200) { 35 throw new Error(\u0026#39;Unable to get randomPeople from https://randomuser.me API: \u0026#39; + status); 36 } 37 38 return data; 39} 40 41(async () =\u0026gt; { 42 await main(); 43})() \r\rNote: Person types referenced in the examples\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/enrich/","title":"Enrich","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1 Example 2        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nA standalone function, meaning one that doesn\u0026rsquo;t use pipes and streams, that asks for the source and destination queues and then reads events from the source queue and writes to the destination queue, allowing you to insert a function in-between to transform the data on the way or do other computation.\n  When would I use this?  You want to read from a source queue, enrich or modify the event and send it to another queue You want to read from a source queue and aggregate events, perhaps reading one minute worth of events and then writing one event to another queue that summarizes the 1 minute of source events    Runnable Examples   Example 1 The first example illustrates code running as a bot with ID of rstreams-example.people-to-peopleplus and getting exactly two events from queue rstreams-example.people, starting at position z/2022/04/20, and then transforms each event\u0026rsquo;s JSON by dropping unwanted attributes and simplifying the JSON structure. It also calls a totally free, public API that given a country name returns the standard two-char country code which we tack on to the event after which we return the modified event which tells the SDK to push it to the rstreams-example.people-to-peopleplus queue.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\nSecond, there are actually three arguments to the transform function, even though in our example we are only using the first. What is stored in an RStreams queue is an instance of a ReadEvent where the payload attribute is the data the queue exists for. The first argument is just the payload pulled out since usually that\u0026rsquo;s all you need. The second argument is the full event from the queue with the event ID and other sometimes useful things. The third argument is only used in the callback version where you call done exactly once to trigger the callback. It\u0026rsquo;s there for backwared compat. Don\u0026rsquo;t use it on new things.\n\r\r\rReturning from an enrich async transform function  \r  throw Error\nIf you throw an error at anytime the pipe will error out and your upstream queue will not be checkpointed\n  return object\nWhatever object you return that isn\u0026rsquo;t of type Error will be treated as the event to emit\n  return Array\u0026lt;object\u0026gt;\nEach object in the array will be individually emitted as if you had called this.push(\u0026lt;object\u0026gt;, {partial: true} except the very last one in the array which will act like this this.push(\u0026lt;object\u0026gt;, {partial: false}. When you return a list of objects at once, we assume you mean for them to all work or none of them worked. So, the partial: false means the SDK will emit this events to the downstream queue but not checkpoint. Since the SDK sends the last one with partial: false the last one will both be emitted and the checkpoint updated to the event ID of that last event.\nIf you pass an empty array, that\u0026rsquo;s the same thing as if you called return true.\n  return true\nThis means I don\u0026rsquo;t want to emit an event with my return but I do want the SDK to checkpoint for me in the upstream queue. If we\u0026rsquo;re not batching, then this checkpoints the one event. If we\u0026rsquo;re batching, this checkpoints up to the final event in the batch.\n  return false\nThis means I don\u0026rsquo;t want to emint an event with my return AND I also don\u0026rsquo;t want the SDK to checkpoint for me\n  this.push\nYou may emit events by passing them in to this.push if you want to. More on this later in the Advanced use cases section* below.\n  \r \r\r\rAdvanced use cases  \rLet\u0026rsquo;s say I want to turn one event read from the upstream queue into many events in the downstream queue. Well, you can\u0026rsquo;t return multiple times from the transform function. There\u0026rsquo;s another way.\nIf your transform function uses transform: function() {} and not transform: () =\u0026gt; {} to create your function, then the this variable will be of type ProcessFunctionContext\u0026lt;U\u0026gt; - transform function type and ProcessFunctionContext types. Then you may call this.push as many times as you want to push events downstream that the SDK will pick up and send to the destination queue. Then, when you\u0026rsquo;re done, simply return true telling the SDK to checkpoint the upstream event now that you\u0026rsquo;re done.\nWe need to talk more about checkpointing. In the enrich operation the SDK assumes that for each event you consume from an upstream queue you will generate one event to send to the downstream queue. So, each time you call this.push from the transform function the SDK checkpoints the upstream event, marking that this bot has gone past that event in the upstream queue. Well, if you are turning one upstream event into multiple downstream events, you are going to call this.push multiple times to emit your many events and you don\u0026rsquo;t want to checkpoint the one upstream event until you\u0026rsquo;ve generated all the downstream events. You do this by calling the push method with the first arg as the event to emit and the second arg options partial set to true indicating that this event is one of many being emitted and it will send the partial event to the downstream queue but it won\u0026rsquo;t checkpoint. Then, when you\u0026rsquo;re done you simply return true; and it will checkpoint the event in the upstream queue.\nSee TypeScript this param typing.\n\r \r\r Example 1 code\r 1import { EnrichOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person, PersonRaw } from \u0026#34;../../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: EnrichOptions\u0026lt;PersonRaw, Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.people-to-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 outQueue: \u0026#39;rstreams-example.peopleplus\u0026#39;, 11 start: \u0026#39;z/2022/04/20\u0026#39;, 12 config: { 13 limit: 2 14 }, 15 transform: async (person: PersonRaw) =\u0026gt; { 16 const p: Person = translate(person); 17 await addCountryCode(p); 18 return p; 19 } 20 }; 21 22 await rsdk.enrichEvents\u0026lt;PersonRaw, Person\u0026gt;(opts); 23} 24 25// See next expand section for translate and addCountryCode functions 26 27(async () =\u0026gt; { 28 await main(); 29})() \r\r\r\r Example 1 addCountryCode and translate functions\r 1interface CountryCode {cca2: string;} 2 3/** 4* @param person The person to add addr.countryCode to by calling a public API to 5* turn a country name in a 2 digit country code (iso cca2) 6*/ 7async function addCountryCode(person: Person): Promise\u0026lt;void\u0026gt; { 8 const url = `https://restcountries.com/v3.1/name/${person.addr.country}?fullText=true\u0026amp;fields=cca2`; 9 const cc: CountryCode = await axios.get(url); 10 person.addr.countryCode = cc.cca2; 11} 12 13/** 14* @param p The type from the public API we want to modify 15* @returns The new type that is flatter and gets rid of some attributes don\u0026#39;t need 16*/ 17/** 18* @param p The type from the public API we want to modify 19* @returns The new type that is flatter and gets rid of some attributes don\u0026#39;t need 20*/ 21function translate(p: PersonRaw): Person { 22 return { 23 gender: p.gender, 24 firstName: p.name.first, 25 lastName: p.name.last, 26 email: p.email, 27 birthDate: p.dob.date, 28 nationality: p.nat, 29 addr: { 30 addr1: p.location.street.number + \u0026#39; \u0026#39; + p.location.street.name, 31 city: p.location.city, 32 state: p.location.state, 33 country: p.location.country, 34 postcode: p.location.postcode, 35 longitude: p.location.coordinates.longitude, 36 latitude: p.location.coordinates.latitude, 37 tzOffset: p.location.timezone.offset, 38 tzDesc: p.location.timezone.description 39 } 40 } 41} \r\rNote: Person types referenced in the examples\nAfter running this for the first time, the SDK created the restreams-exmaple.peopleplus queue and our bot showed up reading an event from the upstream queue and pushing it into the new queue and the modified event appeared in the new queue.        Example 2 This example is nearly identical to Example 1 above except that this time we are are going to use config to tell the SDK to batch up events for us so we can be more efficient. The calls out to a public API to enrich each event with the country code based on the country name. The free API we are using requires a separate API request for each country. Sure, we could try to make some kind of cache but there\u0026rsquo;s lots of cases where you can\u0026rsquo;t do this. So, we\u0026rsquo;re at risk of not being able to read and enrich events from the upstream queue fast enough to keep up if events are slamming into that upstream queue super fast.\nSo, we\u0026rsquo;re going to ask the SDK to micro-batch up events 10 at a time and then invoke our transform function with all ten at once and if it\u0026rsquo;s waited more than one second for 10 to show up then our config tells the SDK to just go ahead and invoke transform with whatever it\u0026rsquo;s got so far. Then in the enrich transform function we\u0026rsquo;re going to modify our addCountryCode function to make concurrent API requests for each person we are transforming, parallelizing the work and making it much faster so we can keep up. To make the example more interesting, we set config.limit now to 100 so we get a lot more events before we stop reading from the upstream queue. The config in the config attribute is important for specifying how long we\u0026rsquo;re meant to read from the upstream queue before we stop reading and close down shop. If you\u0026rsquo;re running in a lambda function, you\u0026rsquo;ve only got 15 min before AWS shuts down your lambda and that may sound like a long time unless you are reading from a queue that is forever getting new events shoved into it, a pretty common case. By default, if you don\u0026rsquo;t set any config to tell the SDK when to stop reading from the upstream queue, the SDK will read for up to 80% of the total time remaining for your lambda, if you are in fact running as a lambda. That then saves 20% of the time for you to finish processing.\nYou\u0026rsquo;ll notice that because we used the EnrichBatchOptions to batch things up that the transform function arguments change. That\u0026rsquo;s because the SDK isn\u0026rsquo;t invoking transform with just one object but with the batch: an array of objects.\nThe first argument is just the array of events direct from the upstream queue. The second arg is an event wrapper around the whole array of events directly from the upstream queue - not really needed except in rare use cases. The third argument is for backward compatability when using the enrich as a callback instead of using async. Please only use async going forward and so you don\u0026rsquo;t need the third arg.\nWhen we\u0026rsquo;re done enriching the events, we simply return the array of the new events to send them on their way to the destination RStreams queue. See Returning from an enrich async transform function above for more details.\n\r\r\r Example 2 code\r 1import { EnrichBatchOptions, ReadEvent, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person, PersonRaw } from \u0026#34;../../lib/types\u0026#34;; 3import axios from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: EnrichBatchOptions\u0026lt;PersonRaw, Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.people-to-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 outQueue: \u0026#39;rstreams-example.peopleplus\u0026#39;, 11 batch: { 12 count: 10, 13 time: 1000 14 }, 15 start: \u0026#39;z/2022/04/20\u0026#39;, 16 config: { 17 limit: 100, 18 }, 19 transform: async (people: ReadEvent\u0026lt;PersonRaw\u0026gt;[]) =\u0026gt; { 20 const newPeople: Person[] = people.map((p) =\u0026gt; translate(p.payload)); 21 await addCountryCode(newPeople); 22 return newPeople; 23 } 24 }; 25 26 await rsdk.enrichEvents\u0026lt;PersonRaw, Person\u0026gt;(opts); 27} 28 29(async () =\u0026gt; { 30 await main(); 31})() \r\r\r\r Example 2 addCountryCode and translate functions\r 1interface CountryCode {cca2: string;} 2 3/** 4* @param people The people to add addr.countryCode to by calling a public API to 5* turn a country name in a 2 digit country code (iso cca2) 6*/ 7async function addCountryCode(people: Person[]): Promise\u0026lt;void\u0026gt; { 8 const urls: string[] = people.map((el) =\u0026gt; { 9 return `https://restcountries.com/v3.1/name/${el.addr.country}?fullText=true\u0026amp;fields=cca2`; 10 }); 11 12 const ccs: CountryCode[] = (await Promise.all( 13 urls.map((url) =\u0026gt; axios.get(url)))).map((obj) =\u0026gt; (obj.data[0])); 14 15 people.forEach(function (person, i) { 16 person.addr.countryCode = ccs[i].cca2; 17 }); 18} 19 20/** 21* @param p The type from the public API we want to modify 22* @returns The new type that is flatter and gets rid of some attributes don\u0026#39;t need 23*/ 24 function translate(p: PersonRaw): Person { 25 return { 26 gender: p.gender, 27 firstName: p.name.first, 28 lastName: p.name.last, 29 email: p.email, 30 birthDate: p.dob.date, 31 nationality: p.nat, 32 addr: { 33 addr1: p.location.street.number + \u0026#39; \u0026#39; + p.location.street.name, 34 city: p.location.city, 35 state: p.location.state, 36 country: p.location.country, 37 postcode: p.location.postcode, 38 longitude: p.location.coordinates.longitude, 39 latitude: p.location.coordinates.latitude, 40 tzOffset: p.location.timezone.offset, 41 tzDesc: p.location.timezone.description 42 } 43 } 44} \r\rNote: Person types referenced in the examples\n   "},{"url":"https://rstreams.org/rstreams-flow/project-organization/","title":"Project Organization","description":"","content":"\r\r\r ToC\r   Summary Root-level Files and Directories    \r\r  Summary This doc explains how projects are organized and what all the files are and what they mean, using the example project from the Getting Started Guide.\n  Root-level Files and Directories    Path Description     .mock-data/ Developers put their mock data in this directory. The SDK, when it generates mock data, will also put it in here   .vscode/ Config to make working in Visual Studio code easier with RStreams   bots/ All bots are in this directory   cloudformation/ Additional CloudFormation templates that will be merged into the final stack go in this directory   lib/ Standard directory developers will often use to put project-specific files within   node_modules/ Standard Node JS location for downloaded 3rd party Node libraries   test/ A directory for the projects unit tests   .env.dev A dotenv property file for local config   .gitignore Ignore certain files   .nycrc.json Generates unit test code coverage using the popular Instanbul NYC library   package.json The ubiquitous Node file containing NPM run scripts and your project\u0026rsquo;s dependencies   project-config-new.def.json RStreams Flow will soon release a new way of handling your project\u0026rsquo;s configuration, this is a preview of that feature   project-config-new.ts RStreams Flow will soon release a new way of handling your project\u0026rsquo;s configuration, this is a preview of that feature   serverless.yml Config used by the Serverless Framework to build and run with an RStreams-specific plugin   tsconfig.json Standard tsconfig file for typescript support   types.d.ts Top-level types used by soon to be released RStreams Flow project config feature   webpack.config.js Standard webpack file to help in bundling builds    "},{"url":"https://rstreams.org/rstreams-flow/running-locally/","title":"Running Locally","description":"","content":"\r\r\r ToC\r   Summary Prerequisites Project Serverless File Invoke a bot locally hitting an actual RStreams queue    \r\r  Summary This doc explains how to invoke bots running locally that hit actual queues in a deployed RStreams Bus as well as how to mock out a queue and invoke a bot with that mock data. It also explains how to set breakpoints in your code.\n  Prerequisites  This assumes you are using Visual Studio Code as your IDE, however everything should be possible in your favorite IDE such as IntelliJ or whatever it may be You will need the Serverless Framework installed as a global package. If you want to follow along, you should go through the Getting Started Guide    Project Serverless File Each project has a root-level serverless.yml file. Here\u0026rsquo;s an abbreviated version of the one from the RStreams Flow Example project highlighting a few points of interest.\n\r\r Example Project Serverless File\r 1service: rstreams-example 2 3useDotenv: true 4 5plugins: 6 - serverless-leo 7 - serverless-webpack 8 9package: 10 individually: true 11# artifact: ${opt:artifact, \u0026#34;\u0026#34;} 12 13custom: 14 leo: 15 botIdExcludeStage: true 16 configurationPath: project-config-new.def.json 17 18 rsfConfigResolutionType: secretsmanager 19 _rsfConfigResolutionType: secretsmanager|env 20 replicationRegions: 21 - us-east-1 =\u0026gt; us-west-2 22 - us-west-2 =\u0026gt; us-east-1 23 - europe-west-2 24 25 convention: 26 functions: 27 folders: 28 - api 29 - bots 30 pattern: 31 - \u0026#39;*.yml\u0026#39; 32 - \u0026#39;*.yaml\u0026#39; 33 resources: 34 folders: 35 - cloudformation 36 pattern: 37 - \u0026#39;*.yml\u0026#39; 38 - \u0026#39;*.yaml\u0026#39; 39 - \u0026#39;*.js\u0026#39; 40 - \u0026#39;*.json\u0026#39; 41 - \u0026#39;*.ts\u0026#39; 42 included: ${file(./node_modules/serverless-convention/index.js)} 43 44 leoStack: ${RStreamsBus} 45 dev: 46 stackParameters: 47 - ParameterKey: RStreamsBus 48 ParameterValue: ClintTestBus-Bus 49 - ParameterKey: MyStage2 50 ParameterValue: Prod 51 test: 52 # stackParameters: 53 staging: 54 # stackParameters: 55 prod: 56 # stackParameters: 57 us-east-1: 58 deploymentBucket: leo-cli-publishbucket-19e80lsbylz0f 59 us-west-2: 60 deploymentBucket: leo-cli-publishbucket-13ickrmrh6vyd 61 no-region: 62 deploymentBucket: leo-cli-publishbucket-13ickrmrh6vyd 63 64provider: 65 name: aws 66 runtime: nodejs14.x 67 versionFunctions: false 68 deploymentBucket: ${self:custom.${opt:region, \u0026#39;no-region\u0026#39;}.deploymentBucket} 69 stage: ${opt:stage, \u0026#39;dev\u0026#39;} 70 71 environment: 72 # stuff: ${cf:TestBus.LeoStream} 73 # stuff2: \u0026#39;{{resolve:secretsmanager:rstreams-TestBus:SecretString:::}}\u0026#39; 74 # RSTREAMS_CONFIG_SECRET:  75 # Fn::Sub: rstreams-${RStreamsBus} 76 77 stackParameters: ${self:custom.${self:provider.stage}.stackParameters} 78 79functions: 80 - ${self:custom.included.functions} # Auto-include functions using serverless-convention 81 82resources: 83 - ${self:custom.included.resources} # Auto-include resources using serverless-convention \r\r Line 1 : this is the name of your microservice, it is referenced in various places including bot-specific yml files Line 3 : tells serverless to use the popular dotenv project  Bot A bot is logical wrapper around code that executes to interact with an RStreams queue. Bots are found in the {project_dir}/bots/{bot_dir}. Each bot has its own serverless.yml file. Here\u0026rsquo;s an example from the weather-loader bot of the RStreams Flow Example project.\n\r\r Example 1 code\r 1weather-loader: 2 name: ${self:service}-weather-loader 3 handler: bots/weather-loader/index.handler 4 description: Bot generated by serverless-leo 5 memorySize: 256 6 timeout: 300 7 role: BotRole 8 events: 9 - leo: 10 destination: rstreams-example.weather 11 cron: 0 */1 * * * * 12 extra_data: 123 \r\rStage Each RStreams Flow bot executes in the context of an environment, called a stage, such as dev/test/staging/prod. The RStreams Flow example template project expects that your stage, the environment, is dev and that a file named .env.dev exists that contains the name of the AWS secrets manager secret that contains the RStreams bus instance config to connect to.\nserverless invoke-bot --stage {environment} --function {bot-name} RSTREAMS_CONFIG_SECRET=\u0026ldquo;rstreams-us-west-2-clint-bus\u0026rdquo;\n  Invoke a bot locally hitting an actual RStreams queue  The invoke-bot startup performance was significantly improved in May 2022.\n serverless invoke-bot --stage {environment} --function {bot-name}  serverless invoke-bot : an RStreams plugin to serverless --stage {environment} : specifies the environment to run the bot within which will change the configuration used --function {bot-name} : the bot to invoke  serverless invoke-bot \u0026ndash;stage dev \u0026ndash;function weather-loader\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/source-streams/","title":"Source Streams","description":"","content":"\r\r\r ToC\r    \r\rThese functions create a source stream for you, acting as the first stream in a pipe. Each source stream feeds a pipe with data that you specify, allowing it to flow through to the next stream step in your pipe.\n Read Function A function that creates a source stream that gets events from the specified queue and feeds them into the pipe. Create Source Function A function that creates a source stream that gets events from the specified queue and feeds them into the pipe.  "},{"url":"https://rstreams.org/rstreams-node-sdk/streams-primer/","title":"Streams Primer","description":"","content":"\r\r\r ToC\r   Overview Pipes and Streams  Readable Writeable Duplex Transform or Through      \r\rThis primer provides exactly enough knowledge of streaming concepts for a developer to successfully write streaming applications using the RStreams SDK and bus. It is not intended as an exhaustive treatise on the vagaries of Node streams. We all have work to do.\n   Overview There truly is nothing new under the sun. Streaming is really nothing more than Unix pipes, albeit in a more distributed manner, invented more than 50 years ago. The RStreams Node SDK relies on streaming data in and out just exactly as Unix pipes stream together commands in POSIX-based systems.\nStreaming involves creating a series of steps in a pipe where the first step, the Source, generates the data to move through the pipe. The last step is the Sink, whose job it is to do something with the data moving through the pipe. The Sink is responsible for pulling data from the previous step, which causes data to flow in the pipe: no Sink step in the pipe means no data flows. In between the source step and the sink step may optionally be any number of Transform steps that can modify data that flows through the pipe on its way to the sink.   \r\r Care to hear why some think streams are too hard?\r Streams get a bad rap. There are some who claim learning to stream data is too hard for developers. Most who dis on streams were quoted some years ago, though you can still find some articles today. The negativity was a reaction to Java and Node and C# releasing streams and their functional programming approach, which was uber complex and often used when it shouldn\u0026rsquo;t have been.\nThis is in large measure because streams became synonymous with functional programming in Java and C# and elsewhere. Java’s streaming solution, which is how many got their first experience with streaming, is complicated, ill suited to streaming because of Java’s verbosity and feel to many developers like regular expressions: going back to one requires painstakingly decomposing what it is doing, having to understand code that is hard to read and understand.\nNode’s original streaming API was hard to understand and use and has been significantly improved over the years. Don\u0026rsquo;t worry, the RStreams Node SDK dramatically simplifies it for you.   \r\r\r\r Care to read why streams might be worth it for you?\r Why code in a series of chained steps? Sounds complicated. The answer is you turn to streaming when you are working with systems where you need to process data as it is coming in because so much data needs to flow in that you can\u0026rsquo;t wait to start sending it out.\nIt\u0026rsquo;s also applicable when you need to minimize the delay in processing lots of data. Finally, it\u0026rsquo;s a great way to creat a reactive system, where the data that flows are events that cause distributed event handlers to wake up and process them, moving and transforming them from one place to another.\n\r\r  Pipes and Streams  99% of the time, all you need to know is which RStreams SDK pipe step interface to use. This section helps you develop the mental model of a pipe so you can do that. Actually using a pipe step is brain dead simple. Don\u0026rsquo;t get overwhelmed as you don\u0026rsquo;t need to understand the actual functions of a Node Readable or Node Writable or the intracies of pipes as the SDK abstracts all that complexity for you. Here\u0026rsquo;s a good article if you want just a bit more detail but you you\u0026rsquo;ll be fine without it if you read the section below.\n As mentioned, a pipe is a set of steps that data flows through in sequence. Each step in the pipe is itself called a stream because they are meant to read/write data sequentially one after the other. Steps near the beginning of the pipe are upstream and the Sink downstream: data flows is the furthest step downstream. The pipe exists to daisy chain the stream steps together.\nIf you want the super short version, everything in a pipe must be linked together where start with a Readable followed by a Writable followed by a Readableand eventually end with a Writable as the Sink. Pipe step streams between the Source and the Sink have to be both a Writable and a Readable to allow the data to flow through the step: these in between steps are often called Through or Transform stream steps.     Readable In node, a pipe is a function and each argument is a step, thus a stream, in the pipe. The first step, the Source, must get or create data somehow. It might do this by continuosly querying a database and making the data available for the next step to grab it. Remember that each downstream step pulls data from the step before it. In other words, the Source step must be readable by the next step so it can pull data from it. So, Source steps will always be of the Node type Readable. For example, the fs.createReadStream() Node file function will create a source stream that reads data from a file.\nA Readable stream is an abstraction for a source from which data can be consumed.\nThe RStreams SDK provides extremely simple Readable interfaces to make getting data from an RStreams bus queue a breeze. These simplified RStreams SDK pipe steps are of the RStreams SDK type ReadableStream which inherits from the Node Readable.\n  Writeable The last step in a pipe, the Sink, needs to be able to do something with the data. In other words, it needs to be a step we can write to such as fs.createWriteStream() that creates a Sink stream that will write the data flowing through the pipe to a file.\nA Writable stream is an abstraction for a destination to which data can be written.\nThe RStreams SDK provides extremely simple Writable interfaces to make sending data to other resources, such as a database or Elastic Search, etc., a snap. These simplified RStreams SDK pipe steps are of the RStreams SDK type WritableStream which inherits from the Node Writable. That\u0026rsquo;s all you need to know.\n  Duplex A stream step that sits between the Source and the Sink is by definition a Duplex stream. Think of a Duplex stream like its really two streams smashed together. The input to the Duplex stream is a Writable so it can consume the data from the Readable in the step before it. The output from the Duplex stream is a Readable so the next step downstream can pull data from it.\nA Duplex stream is one that is both Readable and Writeable at the same time, e.g. a TCP socket.\n     Transform or Through A Transform stream is just a Duplex stream with a function that modifies the data or perhaps causes some other side effect and then sends the data downstream. A Transform stream is often called a Through stream.\nA Transform stream is a duplex stream that allows you to transform the data in between when it is written to the stream and later read from the stream.\nThe RStreams SDK provides extremely simple Transform interfaces to make moving data through a pipe easy. These simplified RStreams SDK pipe steps are of the RStreams SDK type TransformStream which inherits from the Node Duplex.\n   "},{"url":"https://rstreams.org/rstreams-botmon/using-botmon/","title":"Using Botmon","description":"","content":"\r\r\r ToC\r   Summary Botmon to the Rescue What is the structure of data that flows through the bus?         How do I find a given event to diagnose something?  Finding an Event  Text Search Attribute Search Event ID / Date Search Combined Search        \r\rThis article assumes you have a working installation of an RStreams Bus and know how to access botmon.\n   Summary This article explains how to reason about data moving through the RStreams bus while explaining the basics of botmon, queues and events. If you have one of these questions, read on:\n How do I trace a data event’s path as it moves through the bus? How do I debug where my data event went bad? How do I find out where I’ve got a bug as data moves through the bus?    Botmon to the Rescue The answer to all of the above questions lies with botmon, RStreams’ monitoring and visualization tool. Botmon allows engineers to visually understand what data is flowing where in the bus and provides excellent tools to diagnose issues like the following:\n What is the structure of data that flows through the bus? How do I find a given event to diagnose something? Tracing upstream. My data event is missing an attribute I expected to be there. Where did it get lost? Tracing downstream. My data event never made it to my code. Why?  Let’s answer these questions one at a time, building on our knowledge of how to use botmon to solve real problems with each new question.\n  What is the structure of data that flows through the bus? Events travel through the queues of the bus.\n  Each event has an event ID.   Each event shares a common structure. 1{ 2 \u0026#34;id\u0026#34;: \u0026#34;id_of_bot\u0026#34;, 3 \u0026#34;event\u0026#34;: \u0026#34;id_of_the_queue\u0026#34;, 4 \u0026#34;event_source_timestamp\u0026#34;: 1614185828000, // timestamp of the initial event 5 \u0026#34;timestamp\u0026#34;: 1614185828000, // timestamp of this event 6 \u0026#34;correlation_id\u0026#34;: { // information to track what created this event 7 \u0026#34;source\u0026#34;: \u0026#34;previous_queue_id\u0026#34;, 8 \u0026#34;start\u0026#34;: \u0026#34;z/2021/02/24/16/50/1614185855405-0000000\u0026#34;, // first event id included 9 \u0026#34;end\u0026#34;: \u0026#34;\u0026#34;, // last event id included (optional) 10 \u0026#34;units\u0026#34;: 1 // number of records included (optional, defaults to 1) 11 }, 12 \u0026#34;eid\u0026#34;: \u0026#34;z/2021/02/24/16/57/1614185855405-0000000\u0026#34;, 13 \u0026#34;payload\u0026#34;: {} // your custom data 14}  2 : id is the ID of the bot that put the event in this queue\n3 : event is the ID of the queue this event is in\n4 : event_source_timestamp is when the original event far upstream entered the first queue that this and all subsequent events stem from (see the note below)\n5 : timestamp is when this data event entered this queue\n6-11 - correlation_id is how this event flowed to this queue (more later on this)\n12 : eid is the event ID (see above)\n13 : payload is the data of the event which can be anything (an order, an item, whatever)\n   Each queue expects the payload of the event to be something specific to that queue   Events can be correlated Read the Fundamentals Section on Correlation ID\n  How do I find a given event to diagnose something? Something is breaking. Maybe lag is increasing somewhere. Maybe you’ve got errors in a bot log in cloud watch. Maybe a bot is going rogue because it is erroring continuously (check out this quick article on bot states) and you need to understand why. Where do you start? You need these things to begin diagnosing the problem:\n An idea of where in the many streams something is going wrong, the queues that lead up to the bot where something is going wrong for example. One or both of these, depending on what you’re trying to diagnose  An idea of when, as far as time is concerned, there’s an example of an event that can show us what went wrong An object identifier or other string on an event you can search on to find the exact event you want   Knowledge of how to search in a queue  The botmon home page shows bots that are having troubles. If you don’t have specifics of what is happening, start there for bots that experience increased lag or erroring or have gone rogue.\n  Finding an Event Diagnosing an issue is 90% just finding an example of a bad event to pursue or an event that is causing a problem elsewhere that you need to drill in on. Go to a queue in botmon, click on the detail (gear icon when hover) and then click on the Events tab. The text field at the top lets you search. Here\u0026rsquo;s how.\n  Text Search Just type text you know is in an event and hit enter.\n  Attribute Search Queries that start with $$.some_attribute_name begin looking at the root of the JSON object. Queries that start with $.some_attribute_name begin looking in the payload of the data event. The following looks in the payload attributes status to find any events that indicate it is raining. Both do the same thing.\n$.status == \u0026#39;raining\u0026#39; $$.payload.status == \u0026#39;raining\u0026#39;   Event ID / Date Search The following are valid searches and will find the first object at the position indicated by the date/time stamp with however much resolution is provided.\n// This looks for an exact event with this EVENT ID z/2022/03/16/20/52/1647463953511-0001195  // This looks for the first event in March 2022 z/2022/03   Combined Search You can combine event ID, text and attribute searching to do a more complex search. Here are valid searches:\n// Searches on the given event ID and the text term and with an attribute search EVENT_ID \u0026lt;space\u0026gt; TEXT_TERM \u0026lt;space\u0026gt; ATTRIBUTE_SEARCH  // Searches on the given event ID and attribute EVENT_ID \u0026lt;space\u0026gt; ATTRIBUTE_SEARCH  // Searches on the given event ID and text term EVENT_ID \u0026lt;space\u0026gt; TEXT_TERM  // Searches on the given text term and attribute TEXT_TERM \u0026lt;space\u0026gt; ATTRIBUTE_SEARCH "},{"url":"https://rstreams.org/rstreams-guides/core-concepts/event-handling-semantics/","title":"Event Handling Semantics","description":"How event handling works in RStreams","content":"\r\r\r ToC\r    \r\rRStreams was designed so that a developer can register a bot (lambda function) that the RStreams Bus will invoke whenever there are new events to be read from a given queue.\nIt is expected that events could be, and often will be, continuously being produced and flowing into a queue.\nSo, the desire is for your bot to be invoked and get new events from the a queue and process them, doing this for as long as it safely can before shutting itself down. Bots are usually just lambda functions and so lambda functions must shutdown every 15 minutes in AWS. If you don\u0026rsquo;t, AWS will forcefully terminate your lambda - not good.\nBy default, the source stream that pulls data from a queue for you to process or handle in your lambda will only do so for 80% of the time that your bot has to live if it\u0026rsquo;s a lambda, expecting that your bot can finish processing and flush the queue of already received events within the remaining 20% of the time. Of course, there\u0026rsquo;s config on the source stream allowing you to fine tune this, though experience has shown that this default works most of the time.\nWhen your bot stops reading/processing data from a queue and shuts itself down, immediately the RStreams Bus detects this and if there are any more events that your bot hasn\u0026rsquo;t handled yet, will immediately invoke your bot to once again wake up and read/process events from the queue it is registerd as an event handler for.\nIf you are creating a stand-alone node application that uses the RStreams Node SDK to read/process events from a queue that you start by running your app on your dev box or on an EC2 instance, then when your app shuts down nothing restarts it. Why? Well, your app wasn\u0026rsquo;t registered as an event handler for a queue. It\u0026rsquo;s just an app that can read from a queue.\nAll the examples in the RStreams Node SDK section are done as stand-alone runnable apps since the Node SDK doesn\u0026rsquo;t know or care whether it is embedded in a Node app, a Node lambda function or a Fargate for that matter. However, every example in that section is directly applicable to be used as a lambda function event handler. Again, the only difference is that the RStreams Bus knows about bots that are registered as event handlers and ensures that there is exactly one instance of that bot running when there are events waiting to be handled.\n"},{"url":"https://rstreams.org/rstreams-guides/","title":"RStreams Guides","description":"In-depth guides and how-to&#39;s.","content":"This section includes guides that cover RStreams core concepts and common use cases.\n"},{"url":"https://rstreams.org/rstreams-bus/anatomy-of-bus/","title":"Anatomy of a Bus","description":"","content":"\r\r\r ToC\r   Summary What is the \u0026ldquo;Bus\u0026rdquo;?  Event Streaming General Purpose Messaging   The Bus Architecture Diagram The Bus Architecture - 1/2 RStreams Bus is DynamoDB and S3 The Bus Architecture - Writing, the Other Half of the Diagram  Direct to Kinesis Direct to Kinesis with S3 Payloads Kinesis by way of Kinesis Firehose   Data Movement through the Bus    \r\rThe terms defined in the Fundamentals Article may really help before reading this article.\n   Summary This article explains the mechanics of how an instance of the RStreams Bus works by looking at the operations that cause data to flow through the bus.\n  What is the \u0026ldquo;Bus\u0026rdquo;? It\u0026rsquo;s an AWS-native implementation of an event streaming and general purpose messaging system built using AWS services.\n  Event Streaming The bus is a way for a client application (usually a bot) to\u0026hellip;\n Push vast numbers of data events into a named queue quickly, typically using the Node SDK, and keep them in order Retrieve vast numbers of data events from a named queue quickly, retrieving them in order so they can be processed in order Read from one RStreams queue while simultaneously processing data already retrieved and optionally pushing that data to another queue(s)    General Purpose Messaging The following are the messaging models supported by the RStreams Bus:\n Push events to consumer : Bot will be invoked when events are pushed into a given queue and to continue pulling events from the queue until there are no more events or the Bot shuts down and then restarted and invoked again if/when there are more events Consumer pulls events : Bot will be invoked on a cron and pull events from a given queue Consumer reduces events : Bot will read events from a queue and aggregate/reduce them in some manner, turning N events into 1 event and pushing the new event to another queue Producer pushes to a single queue - 1 to 1 : Bot pushes data events to a single queue Producer pushes to multiple queues - 1 to many : Bot reads data events from a queue and then pushes data events to multiple queues Multiple producers push to a single queue - many to 1 : Multiple bots read data from various queues and write data events to a single queue    The Bus Architecture Diagram Here\u0026rsquo;s a diagram of an RStreams bus.     The Bus Architecture - 1/2 RStreams Bus is DynamoDB and S3 The RStreams Bus is mostly bookkeeping. The three DynamoDB tables listed above - Bot Table, Event Table, Stream Table, are the workhorse of RStreams and where the vast majority of its functionality stems from. The Event table stores a definition of every queue in the RStreams instance. The Bot Table stores a definition of every bot in the RStreams instance.\nA bot is code that is registered with RStreams to read from a queue or a “system”. A “system” is a special producer of data that can have a different icon in Botmon and can track checksums. For example, Mongo might be a system in Botmon. Bots can read directly from Mongo since Mongo has been registered as a supported “system.” Additional “systems” might include ElasticSearch, MySQL, Oracle, Postgres, Redshift, MS Sql Server, etc.\nThe Stream table stores every event in every queue, storing either the actual payload of events gzipped or an S3 path to the file that contains the payload of events. Each record may contain multiple contiguous gzipped events and each S3 file referenced may contain multiple contiguous events.\n\r\r Additional detail on DynamoDB tables\r   The Event Table    Column Datatype Description Example     event string The name of the queue this record represents. Used as the hash key in the table. Weather   system.es-account      InvoiceDW-error      archived boolean If true, the queue is considered archived and won’t show up in Botmon, the RStreams monitoring/debugging tool. true   paused boolean If true, this queue is paused and no downstream bots will be invoked. true   max_eid string The event ID of the last event (most recently added) to the queue. z/2019/10/15/02/30/1571106651061-0014155   timestamp string The time of the newest event added to the queue was put in the queue as time since the epoch. 1571106662847      The Bot Table This table keeps track of every bot in the bus. A bot is code that registers itself with the bus to read from a queue.\nUsually, that code is a lambda function, however it doesn’t have to be. It can be code running on an EC2 instance or on Fargate or anywhere really. Bots automatically show up in Botmon, RStreams’ monitoring and debugging tool. Bots keep track of their checkpoints in the queues and “systems” they read from and write to where a checkpoint is the last position read in the queue or “system.”\n   Column Datatype Description Example     id string The unique ID for this bot in the system. weather_loader   archived boolean If true, the bot is considered archived and is hidden from Botmon. true   checkpoints object Maintains the read/write checkpoints for the current read/write position for the bots upstream/downstream queues/systems. See CheckpointObject JSON Example below.   checksum object High level checksums are a way to true-up data retrieved from a system (such as a database table) to verify periodically that all data replicated successfully through the bot to the destination queue and if not to fix it automatically. See ChecksumObject JSON Example below.   description string Human readable text to use within Botmon Mongo to Weather Queue Loader   errorCount int The number of consecutive errors thrown by the bot. RStreams dramatically slows execution of a bot after ten consecutive uncaught errors thrown by a bot and alerts on the bot. This is called a “rogue” bot. 0   executionType enum The service to execute for this bot, one of these: lambda fargate   instances object Each bot may have multiple instances to support fanout - concurrent reading by multiple instances of the same lambda from an upstream queue to keep up with the how quickly new events are dumped in the queue. See InstancesObject JSON Example below.   invokeTime int The last time the bot was invoked as time since the epoch. 1645501894232   lambda object Custom parameters to pass to each instance of a bot on invocation. \u0026ldquo;lambda\u0026rdquo;: {\u0026ldquo;settings\u0026rdquo;: [{\u0026ldquo;value\u0026rdquo;: 0}]},   lambdaName string The lambda function name mongo-weather-load-data   name string Human readable name for display in Botmon Mongo Weather Load Data   paused boolean If paused, the bot will not be invoked. true   requested_kinesis object The current max positions of all queues that will cause this bot to get triggered \u0026ldquo;requested_kinesis\u0026rdquo;: {\u0026ldquo;queue:mongo-events\u0026rdquo;: \u0026ldquo;z/2021/06/11/17/\u0026hellip;000\u0026rdquo;}   trigger int Time since the epoch of the last time the bot was triggered to run 1623432480227   scheduledTrigger int The millisecond time of the next time the bot is allowed to run, used for backoff retries 1614081575844   time string If populated, this means that this bot is run on a schedule using the cron format specified. Use this or triggers not both. 0 */1 * * * *   triggers array The list of upstream queues that that should trigger this bot to run when events arrive in those queues. Use time or triggers not both. [\u0026ldquo;queue:weather-events”]    \r\r CheckpointObject JSON Example\r 1\u0026#34;checkpoints\u0026#34;: { 2 \u0026#34;read\u0026#34;: { 3 \u0026#34;system:mongo-weather\u0026#34;: { 4 \u0026#34;checkpoint\u0026#34;: \u0026#34;z/2021/06/11/14/42/01/002:MT:826...0004\u0026#34;, // eid of last read 5 // event 6 \u0026#34;ended_timestamp\u0026#34;: 1623422523673, // millisecond timestamp of last read event 7 // for last chunk 8 \u0026#34;records\u0026#34;: 1, // number of records for the last chunk 9 \u0026#34;source_timestamp\u0026#34;: 1623422523673, // millisecond event source timestamp of 10 // last read event for last chunk 11 \u0026#34;started_timestamp\u0026#34;: 1623422523673 // millisecond timestamp of first read 12 // event for last chunk 13 } 14 }, 15 \u0026#34;write\u0026#34;: { 16 \u0026#34;queue:weather\u0026#34;: { 17 \u0026#34;checkpoint\u0026#34;: \u0026#34;z/2021/06/11/14/40/1623422416226-0000000\u0026#34;, // eid of last 18 // written event 19 \u0026#34;ended_timestamp\u0026#34;: 1623422416226, 20 \u0026#34;records\u0026#34;: 1, 21 \u0026#34;source_timestamp\u0026#34;: 1623422414001, 22 \u0026#34;started_timestamp\u0026#34;: 1623422416880 23 } 24 } 25} The above shows that the bot in question is reading from an upstream “system” which is named mongo-weather (it’s reading directly from Mongo as opposed to a queue, though it could just be reading from a queue). It shows the event ID of the last event read from mongo-weather, called the checkpoint.\nThe above also shows that the bot is writing to a queue named weather and it has the event ID of the last event written by the bot to that queue, called the checkpoint.\n\r\r\r\r CheckSumObject JSON Example\r 1{ 2 \u0026#34;missing\u0026#34;:{\u0026#34;M\u0026#34;:{\u0026#34;count\u0026#34;:{\u0026#34;N\u0026#34;:\u0026#34;0\u0026#34;}}}, 3 \u0026#34;correct\u0026#34;:{\u0026#34;M\u0026#34;:{\u0026#34;count\u0026#34;:{\u0026#34;N\u0026#34;:\u0026#34;638\u0026#34;}}}, 4 \u0026#34;incorrect\u0026#34;:{\u0026#34;M\u0026#34;:{\u0026#34;count\u0026#34;:{\u0026#34;N\u0026#34;:\u0026#34;0\u0026#34;}}}, 5 \u0026#34;percent\u0026#34;:{\u0026#34;S\u0026#34;:\u0026#34;100.00\u0026#34;} 6} The above shows that there were no events missing or incorrect from the 638 events it compared from data in the queue to data in the source system this bot read from. Note that checksums are not needed when reading from a queue to another queue. They are used when reading from another system such as a database.\n\r\r\r\r InstancesObject JSON Example\r 1\u0026#34;instances\u0026#34;: { 2 \u0026#34;0\u0026#34;: { 3 \u0026#34;invokeTime\u0026#34;: 1623432243253, // millisecond timestamp of invocation 4 \u0026#34;token\u0026#34;: 1623432242544, // invocation token 5 \u0026#34;maxDuration\u0026#34;: 899999, // number of milliseconds the bot is allowed to 6 // run (timeout duration) 7 \u0026#34;requestId\u0026#34;: \u0026#34;723d9f0c-7a66-4dff-9220-f2da19e3fd51\u0026#34;, // id of the current 8 // invocation 9 \u0026#34;startTime\u0026#34;: 1623432243415, // millisecond timestamp of when the bot 10 // started running 11 \u0026#34;status\u0026#34;: \u0026#34;complete\u0026#34;, // status of last invocation (complete|error) 12}} Each instance of the bot that is running is listed with its status. This is where the invocation locks are held based on these three states: Invoked | Running | Complete\n\r\r  The Stream Table This table keeps track of every data event in every queue in the bus. The sequential ordering of records in DynamoDB maintains the order of events in each queue. This single table has all event records for all queues interleaved but a simple and highly efficient query can collect up events sequentially in a single queue, from a single event in time to an ending event in time. Each record in the table represents one or more contiguous events. Each record either contains a gzipped blob of JSON lines that is the event(s) for that record or an S3 path to the file that holds the JSON lines file that contains the data events the record references.\n   Column Datatype Description Example     event string The name of the queue the event(s) are in. item-prod-item-entity-old-new-to-modified-item   gzip string If present, this is the payload of the event. This may be more than one event as JSON lines. The whole thing is gzipped. H4sIAAAAAAAAA9WU3W7jIBCF7/sYXIcww/DnvExEHaJaik3W4G3dKO9e3DTybv9ktb3pJaAzZ858Gk6s2bEN83Udhy7zYx93/HqI+/0h+h0P6XrFVux2zCGxDYI0esXq2Pfh4HMTu+1   gzipSize int This is the size of the gzipped string. 404   offsets object A JSON object that lists offsets of where the gzipped JSON lines start/end in the file for finding events quickly in the file. [{\u0026ldquo;M\u0026rdquo;:{\u0026ldquo;gzipSize\u0026rdquo;:{\u0026ldquo;N\u0026rdquo;:\u0026ldquo;537\u0026rdquo;},\u0026ldquo;size\u0026rdquo;:{\u0026ldquo;N\u0026rdquo;:\u0026ldquo;3054\u0026rdquo;},\u0026ldquo;offset\u0026rdquo;:{\u0026ldquo;N\u0026rdquo;:\u0026ldquo;0\u0026rdquo;},\u0026ldquo;records\u0026rdquo;:{\u0026ldquo;N\u0026rdquo;:\u0026ldquo;6\u0026rdquo;},\u0026ldquo;gzipOffset\u0026rdquo;:{\u0026ldquo;N\u0026rdquo;:\u0026ldquo;0\u0026rdquo;},\u0026ldquo;start\u0026rdquo;:{\u0026ldquo;N\u0026rdquo;:\u0026ldquo;0\u0026rdquo;},\u0026ldquo;end\u0026rdquo;:{\u0026ldquo;N\u0026rdquo;:\u0026ldquo;5\u0026rdquo;},\u0026ldquo;event\u0026rdquo;:{\u0026ldquo;S\u0026rdquo;:\u0026ldquo;system.es-account\u0026rdquo;}}}]   records int The number of events the entry represents. 6   s3 object If this entry refers to an S3 file where the payload of the events are stored, this is where that S3 file is. {\u0026ldquo;bucket\u0026rdquo;:{\u0026ldquo;S\u0026rdquo;:\u0026ldquo;prodbus-leos3-17uqyaemyrrxs\u0026rdquo;},\u0026ldquo;key\u0026rdquo;:{\u0026ldquo;S\u0026rdquo;:\u0026ldquo;bus/system.es-account/z/2019/08/14/23/48/1565826525744-0000001.gz\u0026rdquo;}}   size int The size in bytes of the event payload. 401   start string The event ID of the first event this entry includes. z/2019/08/14/23/48/1565826524436-0000188   end string The event ID of the last event this entry includes. z/2019/08/14/23/48/1565826524436-0000193   ttl int Used by DynamoDB to know when to auto delete this record and send it to a lambda to archive. 1566431324   v int The version of the record format in DynamoDB (current is 2). 2    \r\r  The Bus Architecture - Writing, the Other Half of the Diagram Everything to the left of the three DynamoDB tables in the diagram involves getting data onto the bus, which means into a queue, fast while maintaining total/partial ordering, as necessary.\nEvery time an engineer uses the RStreams Node SDK to write data into a queue, even from a bot that is reading from an upstream queue that then turns around and writes to a downstream queue, what’s really happening is that SDK is writing to Kinesis, in one of three ways.\n  Direct to Kinesis The SDK is simply writing the events directly to Kinesis with the data event payload gzipped and embedded directly on the event sent to Kinesis. This is the fastest method with the lowest latency.\n  Direct to Kinesis with S3 Payloads The SDK is writing a file of one or more actual data events to S3 and then writing a Kinesis event that references back S3 file with the actual payload. This can introduce some downstream latency when reading the S3 files. The SDK will automatically choose this method for a single event when it’s too big for Kinesis to accept - bigger than 1mb. Engineers may choose this method via code when slightly increased read latency warrants making ingestion faster due to the massive number of expected events coming continuously. The read penalty payed for grabbing an S3 file is pretty minimal if the S3 file has many, many events in it. Read performance degrades only when downstream processes have to read many S3 files to work through a relatively small number of events. Read performance actually improves when there is a small number of very large S3 event files. However, the stars have to align correctly for this to make sense. How often is so much data delivered all at once that stuffing a large number of events into an S3 file doesn’t introduce traditional batch-like waiting and latencies? It happens but it doesn’t apply to all use cases.\nThe RStreams roadmap includes a task that has the SDK automatically choose to switch between Direct to Kinesis or writing to S3, given a few heuristics by the queue, so engineers need never care about this detail.\n   Kinesis by way of Kinesis Firehose Sometimes slightly higher ingestion latency, on the order of a 90 seconds, is OK and there is so much data coming through a single bot that batching up over this period makes sense. Firehose makes the most sense, however, when multiple bots are feeding data at massive scale into as single queue. When that’s the case, Kinesis Firehose is the way to go. Kinesis Firehose writes its events directly to S3 for us where RStreams uses the Direct to Kinesis with S3 Payloads described above to drop a single event into Kinesis that references the S3 file of events.\n\r\r Additional Details on the three Approaches\r Let’s go into more detail on each of the three approaches for writing to the bus and see what RStreams is doing underneath the covers.\n  Direct to Kinesis    The SDK writes one Kinesis event for each RStreams event. RStreams will make a best effort to make sure that the event is 1mb or less before writing the event, taking into account gzip size if it’s being used. If it’s greater than 1mb, SDK will use the Direct to Kinesis with S3 Payloads approach for that event. AWS Kinesis Streams is a registered “system” inside of RStreams. This means that a bot may register to read directly from Kinesis. Each Kinesis shard, a shard in Kinesis is a way to partition data for read and write scale, has exactly one RStreams auto-registered bot to read data events from Kinesis and write the RStreams events to the Stream DynamoDB table - more on this Kinesis Processor Bot event later.\nRemember that once the RStreams event is in the Stream DynamoDB table, it’s “in the RStreams queue”, and the rest of the magic of RStreams starts happening which we’ll discuss in the very next section Data Movement through the Bus.\n  Direct to Kinesis with S3 Payloads    The SDK, either because it’s detected that the event is too big for Kinesis or because the engineer told the SDK to use S3 for events in code, first writes the event(s) as a file to S3. Then, it writes to Kinesis with the body of the event referencing the S3 file. So, the events in Kinesis are potentially interleaved records with some being a direct RStreams JSON data event payload and some being a reference to an S3 file. Note, however, that the order is preserved by kinesis regardless since each event is in the correct order in Kinesis and it stays that way. Everything that happens from this point on is the same as the Direct to Kinesis method using the same Kinesis Processor Bot - more on the Kinesis Processor bot later.\n  Kinesis by way of Kinesis Firehose    This approach introduces a little more latency, as much as, but it can be a great way to take a vast amount of data coming all at once that would otherwise overwhelm Kinesis Data Streams, the vanilla Kinesis, and it would start rejecting our attempts to push data into Kinesis because we’ve exceeded the limit on concurrent writes to Kinesis.\nThe SDK is told by the engineer in code to write to Kinesis Firehose instead of vanilla Kinesis Data Streams. By default, Firehose batches up events and writes them to S3. RStreams auto-registers a bot to be invoked each time a new S3 file is written by Firehose. This bot, named Firehose Processor, picks up the file and writes a new S3 file for each set of events t argeted to a different RStreams queue. From here, the Firehose Processor uses the Direct to Kinesis with S3 Payloads approach to write one event for each S3 file to vanilla Kinesis and it flows to the Kinesis Processor and into the Stream table.\nThere is more detail here and we’ve come this far so\u0026hellip; You may have noticed that in the original diagram with the entire RStreams Bus depicted, there was a bot named S3 File Listener. Well, there’s actually a bot named S3 File Listener that is invoked when Firehose files are created that writes to a queue named commands.s3_bus_load that the Firehose Processor bot wakes up to handle events as they are pushed into that queue. Why mention this detail? Like many systems with clean and useful architectures, RStreams uses RStreams to build itself - using bots and queues at the core of its implementation.\n  Kinesis Processor Bot RStreams auto-registers this bot to be invoked by Kinesis when new events arrive. The bot is configured following AWS’ best practices, being invoked for every X events in a micro-batch or every X amount of time since the last invocation, even if the micro-batch isn’t filled up yet. The bot assigns each event a unique event ID (eid), stores the event in the Stream DynamoDB table and updates state on the max eid found in the queue and updates checkpoints (more on checkpoints later).\n\r\r  Data Movement through the Bus Once data hits a queue, what happens? How does it move? How does the bot get the data from the upstream queue? How does data flow to the downstream queue?\nBots register themselves with RStreams, indicating whether RStreams should push events to the bot when invoked or whether the bot will pull events from the upstream queue manually itself on invocation. During registration, bots can also designate micro-batch sizes and other config about how often and when to invoke bots as events arrive in the upstream queue, perhaps favoring more invocations and less latency at the cost of economies of scale of working on a micro-batch of events in a single invocation. Bots can also register and tell RStreams that they are to be invoked on a schedule dictated by the cron statement that accompanies the registration.\nWhen bots are invoked and have data pushed to them or they pull data from the upstream queue, it’s up to the bot what happens next. The bot may save the data into a database or external system and that’s it, the end of the stream. The bot may choose to use the SDK to write to another queue after changing the data it received from the upstream queue somehow. Well, using the SDK to write to a queue is “writing to the bus” which we just went over with the three approaches that use Kinesis. The data gets ingested through kinesis and makes its way into the Stream table. Writing to the Stream DynamoDB table means “new events arrived in the queue.” RStreams will detect this and cause any will at the appropriate time, based on bot config, invoke any registered bots since there are new events in the queue.\n\r\r RStreams Event Example\r 1{ 2 \u0026#34;id\u0026#34;: \u0026#34;id_of_bot\u0026#34;, 3 \u0026#34;event\u0026#34;: \u0026#34;id_of_the_queue\u0026#34;, 4 \u0026#34;event_source_timestamp\u0026#34;: 1614185828000, // timestamp of the initial event 5 \u0026#34;timestamp\u0026#34;: 1614185828000, // timestamp of this event 6 \u0026#34;correlation_id\u0026#34;: { // information to track what created this event 7 \u0026#34;source\u0026#34;: \u0026#34;previous_queue_id\u0026#34;, 8 \u0026#34;start\u0026#34;: \u0026#34;z/2021/02/24/16/50/1614185855405-0000000\u0026#34;, // first event id 9 // included 10 \u0026#34;end\u0026#34;: \u0026#34;\u0026#34;, // last event id included (optional) 11 \u0026#34;units\u0026#34;: 1 // number of records included (optional, defaults to 1) 12 }, 13 \u0026#34;eid\u0026#34;: \u0026#34;z/2021/02/24/16/57/1614185855405-0000000\u0026#34;, // Unique ID for event 14 \u0026#34;payload\u0026#34;: {} // your custom data 15} \r\r"},{"url":"https://rstreams.org/rstreams-botmon/bot-states/","title":"Bot States","description":"","content":"\r\r\r ToC\r   Summary States    \r\r  Summary A bot is a logical representation of code that will run, usually a serverless lambda function.\n  States Nominal State\nAll is well and the bot is functioning normally.   Warning State\nThe bot is experiencing either a delay (lag) in either reading upstream events or writing downstream events.   Error State\nThe bot is returning uncaught errors to the logs and something is wrong.   Rogue State\nThe bot has gone “rogue” and been stopped by RStreams because ten successive invocations returned uncaught errors.   Note the pause icon on the Error State image above. That indicates that the bot was manually paused and is no longer executing. Bots may be paused in whatever state they currently occupy. Rogue bots are by definition paused.\n "},{"url":"https://rstreams.org/rstreams-flow/configuring-rstreams/","title":"Configuring RStreams","description":"","content":"Coming soon.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/standalone-ops/offload/","title":"Offload","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1 Example 2        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nA standalone function, meaning one that doesn\u0026rsquo;t use pipes and streams, that reads events from the specified source RStreams queue and then calls your transform function allowing you to do anything you want to with the data.\n  When would I use this?   You want to read from a source queue and then write it to a resource or system that isn\u0026rsquo;t another RStreams queue\n Write to a database Send data to an API    You want to read from a source queue and perform aggregations/analytics on data before sending to another system\n    Runnable Examples  This expects you\u0026rsquo;ve run the examples in the enrich Operation to populate queues with data.\n   Example 1 The first example illustrates code running as a bot with ID of rstreams-example.offload-one-peopleplus and getting exactly two events from queue rstreams-example.peopleplus, starting at position z/2022/04/20, and then simply saves each event to another system by calling that system\u0026rsquo;s API. The endpoint here is a free, public API that lets you mock out the response and just throws away your request, but works for our purposes.\nTwo things to note here. First is that the transform function is typed for both the callback and async variety but please only use the async version going forward - all new features are only being added to the async approach.\nSecond, there are actually three arguments to the transform function, even though in our example we are only using the first. What is stored in an RStreams queue is an instance of a ReadEvent where the payload attribute is the data the queue exists for. The first argument is just the payload pulled out since usually that\u0026rsquo;s all you need. The second argument is the full event from the queue with the event ID and other sometimes useful things. The third argument is only used in the callback version where you call done exactly once to trigger the callback. It\u0026rsquo;s there for backwared compat. Don\u0026rsquo;t use it on new things.\n\r\r\rReturning from an offload async transform function  \r throw Error\nIf you throw an error at anytime the pipe will error out and your upstream queue will not be checkpointed return true\nThis tells the SDK to checkpoint for me in the upstream queue read from. If we\u0026rsquo;re not batching, then this checkpoints the one event. If we\u0026rsquo;re batching, this checkpoints up to the final event in the batch return false\nThis tells the SDK not to checkpoint this event in the upstream queue read from  \r \r\r Example 1 code\r 1import { OffloadOptions, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import axios, { AxiosResponse } from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: OffloadOptions\u0026lt;Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.offload-one-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 start: \u0026#39;z/2022/04/20\u0026#39;, 11 limit: 2, 12 transform: async (person: Person) =\u0026gt; { 13 await savePerson(person); 14 return true; 15 } 16 }; 17 18 await rsdk.offloadEvents\u0026lt;Person\u0026gt;(opts); 19} 20 21interface PostResponse { 22 success: boolean; 23} 24 25/** 26* @param person Save the person to another system. 27*/ 28async function savePerson(person: Person): Promise\u0026lt;void\u0026gt; { 29 const url = `https://run.mocky.io/v3/83997150-ab13-43da-9fb9-66051ba06c10?mocky-delay=500ms`; 30 const {data, status}: AxiosResponse\u0026lt;PostResponse, any\u0026gt; = await axios.post\u0026lt;PostResponse\u0026gt;(url, person); 31 if (status !== 200 || !data || data.success !== true) { 32 throw new Error(\u0026#39;Saving person to external system failed\u0026#39;); 33 } 34} 35 36(async () =\u0026gt; { 37 await main(); 38})() \r\rNote: Person types referenced in the examples\n  Example 2 This example is nearly identical to Example 1 above except that this time we are are going to use config to tell the SDK to batch up events for us so we can be more efficient. The calls out to a public API to save the event elsewhere are intentionally delayed by 500ms each, a not uncommon API latency. So, we\u0026rsquo;re at risk of not being able to read and offload events from the upstream queue fast enough to keep up if events are slamming into that upstream queue super fast.\nSo, we\u0026rsquo;re going to ask the SDK to micro-batch up events 10 at a time and then invoke our transform function with all ten at once and if it\u0026rsquo;s waited more than one second for 10 to show up then our config tells the SDK to just go ahead and invoke transform with whatever it\u0026rsquo;s got so far. Then in the offload transform function we\u0026rsquo;re going to modify our savePerson function to make concurrent POST API calls for each person we are saving, parallelizing the work and making it much faster so we can keep up. To make the example more interesting, we set limit now to 100 so we get a lot more events before we stop reading from the upstream queue. The config that is inherited from the ReadOptions is important for specifying how long we\u0026rsquo;re meant to read from the upstream queue before we stop reading and close down shop. If you\u0026rsquo;re running in a lambda function, you\u0026rsquo;ve only got 15 min before AWS shuts down your lambda and that may sound like a long time unless you are reading from a queue that is forever getting new events shoved into it, a pretty common case. By default, if you don\u0026rsquo;t set any config to tell the SDK when to stop reading from the upstream queue, the SDK will read for up to 80% of the total time remaining for your lambda, if you are in fact running as a lambda. That then saves 20% of the time for you to finish processing.\nYou\u0026rsquo;ll notice that because we used the OffloadBatchOptions to batch things up that the transform function arguments change. That\u0026rsquo;s because the SDK isn\u0026rsquo;t invoking transform with just one object but with the batch: an array of objects.\nThe first argument is just the array of events direct from the upstream queue. The second arg is an event wrapper around the whole array of events directly from the upstream queue - not really needed except in rare use cases. The third argument is for backward compatability when using the offload as a callback instead of using async. Please only use async going forward and so you don\u0026rsquo;t need the third arg.\nWhen we\u0026rsquo;re done offloading the events, we simply return true telling the SDK to checkpoint for us in the upstream queue. See Returning from an offload async transform function above for more details.\nNote: Person types referenced in the examples\n\r\r Example 2 code\r 1import { OffloadBatchOptions, ReadEvent, RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2import { Person } from \u0026#34;../lib/types\u0026#34;; 3import axios, { AxiosResponse } from \u0026#34;axios\u0026#34;; 4 5async function main() { 6 const rsdk: RStreamsSdk = new RStreamsSdk(); 7 const opts: OffloadBatchOptions\u0026lt;Person\u0026gt; = { 8 id: \u0026#39;rstreams-example.offload-one-peopleplus\u0026#39;, 9 inQueue: \u0026#39;rstreams-example.people\u0026#39;, 10 batch: { 11 count: 10, 12 time: 1000 13 }, 14 start: \u0026#39;z/2022/04/20\u0026#39;, 15 limit: 2, 16 transform: async (people: ReadEvent\u0026lt;Person\u0026gt;[]) =\u0026gt; { 17 await savePeople(people); 18 return true; 19 } 20 }; 21 22 await rsdk.offloadEvents\u0026lt;Person\u0026gt;(opts); 23} 24 25interface PostResponse {success: boolean;} 26interface PostResponseStatus extends PostResponse {status: number} ; 27 28/** 29* @param person Save the person to another system. 30*/ 31async function savePeople(people: ReadEvent\u0026lt;Person\u0026gt;[]): Promise\u0026lt;void\u0026gt; { 32 const url = `https://run.mocky.io/v3/83997150-ab13-43da-9fb9-66051ba06c10?mocky-delay=500ms`; 33 34 const responses: PostResponseStatus[] = (await Promise.all( 35 people.map((person) =\u0026gt; axios.post\u0026lt;PostResponse\u0026gt;(url, person.payload)))).map((obj) =\u0026gt; { 36 return {status: obj.status, success: obj.data ? obj.data.success : false}; 37 }); 38 39 responses.forEach((resp) =\u0026gt; { 40 if (resp.status !== 200 || resp.success !== true) { 41 throw new Error(\u0026#39;Saving person to external system failed\u0026#39;); 42 } 43 }); 44} 45 46(async () =\u0026gt; { 47 await main(); 48})() \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/","title":"SDK APIs","description":"","content":"\r\r\r ToC\r   Overview Standalone Operations Source Stream Functions Transform Stream Functions Sink Stream Functions    \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n A note on typescript types in pipe stream steps. Great effort was made so that pipe steps can infer types based on what\u0026rsquo;s been defined in previous pipe steps. Follow the examples closely to understand the minimum types necessary to keep unknown types from cropping up.\n \r\r Person types referenced in the examples\r 1export interface Person { 2 gender: string; 3 firstName: string; 4 lastName: string; 5 email: string; 6 birthDate: string; 7 nationality: string; 8 addr: { 9 addr1: string; 10 city: string; 11 state: string; 12 country: string; 13 countryCode?: string; 14 postcode: number; 15 longitude: string; 16 latitude: string; 17 tzOffset: string; 18 tzDesc: string; 19 } 20} 21 22export interface PersonRaw { 23 gender: string; 24 name: { 25 title: string; 26 first: string; 27 last: string; 28 } 29 location: { 30 street: { 31 number: number; 32 name: string; 33 } 34 city: string; 35 state: string; 36 country: string; 37 postcode: number; 38 coordinates: { 39 longitude: string; 40 latitude: string; 41 } 42 timezone: { 43 offset: string; 44 description: string; 45 } 46 } 47 email: string; 48 dob: { 49 date: string; 50 age: number; 51 } 52 nat: string; 53} 54 55export interface PersonRawResults { 56 results: PersonRaw[]; 57} \r\r  Overview The RStreams Node SDK includes a simple utility function to create to create pipes and nearly every kind of stream you\u0026rsquo;d need to work with massive amounts of continuously generated data in an instance of the RStreams bus. It also includes functions to allow you to skip the complexity of dealing with pipes and streams at all for the most common use cases: getting data from the bus and sending data to the bus.\n  Standalone Operations These powerful standalone operations, meaning without needing to use pipes and streams, do some heavy lifting for you to hide all the complexity of sending events to and getting events from the RStreams bus.\n put Operation A function that lets you write a single event to the specified RStreams queue enrich Operation A function that reads from the specified source RStreams queue, lets you transform the events and then sends the modified events to the specified destination RStreams queue offload Operation A function that reads from the specified RStreams queue and lets you do something with the events retrieved, perhaps save them in a DB    Source Stream Functions These functions create a source stream for you, acting as the first stream in a pipe. Each source stream feeds a pipe with data that you specify, allowing it to flow through to the next stream step in your pipe.\n Read Function A function that creates a source stream that gets events from the specified queue and feeds them into the pipe. Create Source Function A function that creates a source stream that gets events from the specified queue and feeds them into the pipe.    Transform Stream Functions These functions create a transform stream for you, acting as a pipe step sitting between a source and sink. Each transform stream feeds accepts data from the previous pipe stream step, does something with it and then sends the resulting data on to the next pipe stream step.\n Stringify Function A function that creates a transform stream that takes in an upstream event, turns it into a string and tacks on a newline character to help in creating JSON lines files Through Function A function that creates a transform stream that takes in un upstream event and allows the developer to modiy/enrich/aggregate/reduce events and then send them on to the next stream step in the pipe ToCSV Function A function that creates a transform stream that helps build a csv file by taking each upstream event that comes in and formatting it as a line to put in a CSV file which it outputs to the next pipe stream step    Sink Stream Functions These function create a sink for you, the last step in a pipe.\n Load Function A function that creates a sink that takes in an upstream event and pushes it to an RStreams queue on the bus Devnull Function A function that creates a sink stream that takes in un upstream event and does absolutely nothing with it, except log it if you ask it to  "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/transform-streams/","title":"Transform Streams","description":"","content":"\r\r\r ToC\r    \r\rThese functions create a transform stream for you, acting as a pipe step sitting between a source and sink. Each transform stream feeds accepts data from the previous pipe stream step, does something with it and then sends the resulting data on to the next pipe stream step.\n Stringify Function A function that creates a transform stream that takes in an upstream event, turns it into a string and tacks on a newline character to help in creating JSON lines files Through Function A function that creates a transform stream that takes in un upstream event and allows the developer to modiy/enrich/aggregate/reduce events and then send them on to the next stream step in the pipe ToCSV Function A function that creates a transform stream that helps build a csv file by taking each upstream event that comes in and formatting it as a line to put in a CSV file which it outputs to the next pipe stream step  "},{"url":"https://rstreams.org/rstreams-guides/core-concepts/event-ids/","title":"Event IDs &amp; Searching","description":"Event IDs.","content":"\r\r\r ToC\r       Summary Event IDs Searching        \r\r  Summary All RStreams events have a system-generated event ID, also known as eid. This is set on an event when the event is first processed into a queue by RStreams, using the date/time of that ingestion as the basis of the ID.\n  Event IDs An event ID, again also called eid, uniquely identifies the position of a data event in a queue and the date/time part of the event ID is in UTC. Here’s an event ID:\nz/2022/03/16/06/10/1647411035490-0003221\n z/ all event IDs start with this to identify them as an event ID z/2022/ year z/2022/03/ month z/2022/03/16/ day z/2022/03/16/06/ hour z/2022/03/16/06/10/ minute z/2022/03/16/06/10/1647411035490/ millisecond z/2022/03/16/06/10/1647411035490-0003221 position in millisecond    Searching Events can be searched within a queue based on a complete or partial event ID. When searching in botmon on a queue’s detail page events tab search field, every one of the above bullets is a valid event ID including the first one, z/. Searching with just z/ will search on any event ID. It’s not a very interesting search but it’s valid.\nSearching on this z/2022/03/16/06/ will start searching for events at the very beginning of the sixth hour on march 16th 2022 UTC in a given queue.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/","title":"RStreams Node SDK","description":"The smart SDK for Node/Typescript.","content":"\r\r\r ToC\r   Prerequisites  RStreams Bus RStreams Flow   Are you setup to run the examples? Do you know how to access Botmon? Sample Apps    \r\rMany SDK operations provide both a callback and async/await friendly version. All callback-based operations are to be considered deprecated. RStreams will continue supporting them for backward compatibility but all new features and capabilities will only be added to the async operation flavor.\n This is the RStreams Node SDK, a client-side library designed to push data to and pull data from queues of an RStreams Bus instance.\n  Prerequisites   RStreams Bus You will need an RStreams Bus instance running to connect to. If you don\u0026rsquo;t, head on over to the RStreams Bus section first. This should only take about 10 minutes.\n  RStreams Flow Get a sample RStreams Flow project running that can connect to your RStreams Bus with the right config to verify you are up and running. It should take you less than 5 minutes.\n  Are you setup to run the examples? \r\r\r\rExpand this section if you\u0026rsquo;re not sure  \rAll examples in the SDK documentation assume that when these apps run, the RStreams SDK can discover the configuration it needs. The config it needs is the AWS resource IDs of the RStreams Bus instance deployed in your AWS account. Things like the ID of the kinesis stream used by the bus and so on.\nOf course, in a production environment the SDK will get the config in an intelligent and safe manner, say from AWS Secrets Manager. See the RStreams Flow Configuring RStreams doc.\nHere\u0026rsquo;s the typescript type of the config.\n  Get the config You will first need to get this config. By default, the RStreams Bus puts a secret in secrets manager that is the JSON config blob. The secret will be named rstreams-\u0026lt;bus name\u0026gt;. Go get the JSON config from this secret.\n  Save the config   As a file Create a file named rstreams.config.json and put it in the same directory you are running your app in or in any parent director and the SDK will just find it and use it.\n  As an environment variable Create an environment variable named RSTREAMS_CONFIG whose value is the config JSON blob.\n  As an argument to the SDK itself Create a variable in the code that is the config and then pass it into the SDK\u0026rsquo;s constructor.\n1 2const RSTREAMS_BUS_CONFIG: ConfigurationResources = { 3 \u0026#34;Region\u0026#34;: \u0026#34;some-value\u0026#34;, 4 \u0026#34;LeoStream\u0026#34;: \u0026#34;some-value\u0026#34;, 5 \u0026#34;LeoCron\u0026#34;: \u0026#34;some-value\u0026#34;, 6 \u0026#34;LeoSettings\u0026#34;: \u0026#34;some-value\u0026#34;, 7 \u0026#34;LeoEvent\u0026#34;: \u0026#34;some-value\u0026#34;, 8 \u0026#34;LeoKinesisStream\u0026#34; : \u0026#34;some-value\u0026#34;, 9 \u0026#34;LeoFirehoseStream\u0026#34;: \u0026#34;some-value\u0026#34;, 10 \u0026#34;LeoS3\u0026#34;: \u0026#34;some-value\u0026#34; 11}; 12 13const rsdk: RStreamsSdk = new RStreamsSdk(RSTREAMS_BUS_CONFIG); \r   Do you know how to access Botmon? \r\r\r\rExpand this section if you\u0026rsquo;re not sure  \rBotmon is a visualization, monitoring and debugging tool that installs with an instance of the RStreams as a website. Most examples will have you use Botmon to visualize what\u0026rsquo;s happening and to help diagnose issues.\nTODO: how do they know how to access botmon?\n\r   Sample Apps The examples in this section are geared toward creating apps that use the RStreams SDK to interact with an RStreams bus instance regardless of how those apps are written - standalone app or lambda function. For simplicity, the examples are simply standalone runnables node applications. All examples are applicable whether running as a standalone node app or as a registered queue event handler bot (lambda function). See the Event Handling Semantics article for more.\nThe RStreams Flow section focuses on apps written specifically as bots that are deployed as lambda functions and go into great detail on the specific use cases applicable to serverless applications.\n"},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/sink-streams/devnull/","title":"Devnull","description":"","content":"\r\r\r ToC\r     When would I use this? Runnable Examples  Example 1        \r\rYou need to understand what a pipe and stream step in that pipe is AND nothing comes for free. The cost of working with large amounts of data in near real-time environments with RStreams is you have to think about what you are doing and what it means with respect to reading and writing. It is strongly recommended you read the Read/Write at Scale article at some point.\n API Doc\nThis function creates a sink stream whose purpose is simply to pull events downstream and do nothing with them. All pipes have to have a sink or nothing flows in the pipe since the sink pulls data along from the upstream step before it and then that step pulls from its antecedent and so on. So, no sink means nothing moves in the pipe. However, you don\u0026rsquo;t always want your sink to actually do work like write to a file or to a database or another queue and so devnull is your answer.\n  When would I use this?  When you have a pipe where all you want to do is log data moving through the pipe When you have a pipe that does processing in one of the stream steps before the sink    Runnable Examples   Example 1 This example uses the very popular event-stream Node library, which is exported via the SDK it\u0026rsquo;s used so much, to turn a hard-coded array into a source stream to feed the pipe.\nThen, devnull is used since we don\u0026rsquo;t really want to do anything more than log the events moving through the stream.\nThe argument to devnull, if true, will log events that come to the sink. You can also pass a string in which tells the SDK to log events and starts each event in the console output with the string you provided and not the word \u0026ldquo;devnull\u0026rdquo; which is the default behavior.\n\r\r Example 1 code\r 1import { RStreamsSdk } from \u0026#34;leo-sdk\u0026#34;; 2 3async function main() { 4 const rsdk: RStreamsSdk = new RStreamsSdk(); 5 6 const DATA: SoftDrink[] = [ 7 {name: \u0026#39;Pepsi\u0026#39;, yearInvented: 1893}, 8 {name: \u0026#39;Coca-Cola\u0026#39;, yearInvented: 1886}, 9 {name: \u0026#39;Dr Pepper\u0026#39;, yearInvented: 1885}, 10 {name: \u0026#39;Hires Root Beer\u0026#39;, yearInvented: 1876}, 11 {name: \u0026#39;Vernors Ginger Ale\u0026#39;, yearInvented: 1866}, 12 {name: \u0026#39;Schweppes\u0026#39;, yearInvented: 1783} 13 ] 14 15 await rsdk.streams.pipeAsync( 16 rsdk.streams.eventstream.readArray(DATA), 17 rsdk.streams.devnull(true) 18 ); 19} 20 21interface SoftDrink { 22 name: string; 23 yearInvented: number; 24} 25 26(async () =\u0026gt; { 27 try { 28 await main(); 29 } catch(err) { 30 console.log(err); 31 } 32})() \r\r\r\r Example 1 console output\r 1➜ rstreams-runnable-examples ts-node apps/devnull-stream.ts 2devnull { 3 \u0026#34;name\u0026#34;: \u0026#34;Pepsi\u0026#34;, 4 \u0026#34;yearInvented\u0026#34;: 1893 5} 6devnull { 7 \u0026#34;name\u0026#34;: \u0026#34;Coca-Cola\u0026#34;, 8 \u0026#34;yearInvented\u0026#34;: 1886 9} 10devnull { 11 \u0026#34;name\u0026#34;: \u0026#34;Dr Pepper\u0026#34;, 12 \u0026#34;yearInvented\u0026#34;: 1885 13} 14devnull { 15 \u0026#34;name\u0026#34;: \u0026#34;Hires Root Beer\u0026#34;, 16 \u0026#34;yearInvented\u0026#34;: 1876 17} 18devnull { 19 \u0026#34;name\u0026#34;: \u0026#34;Vernors Ginger Ale\u0026#34;, 20 \u0026#34;yearInvented\u0026#34;: 1866 21} 22devnull { 23 \u0026#34;name\u0026#34;: \u0026#34;Schweppes\u0026#34;, 24 \u0026#34;yearInvented\u0026#34;: 1783 25} \r\r"},{"url":"https://rstreams.org/rstreams-node-sdk/read-write-scale/","title":"Read/Write at Scale","description":"","content":"\r\r\r ToC\r   Overview Reading at Scale  App Use Cases and Considerations Config to the Rescue   Writing at Scale  Considerations Config to the Rescue      \r\rYou need to understand what a pipe and stream step in a pipe are.\n   Overview The RStreams Node SDK includes a simple utility function to create pipes and nearly every kind of stream you\u0026rsquo;d need to work with massive amounts of continuously generated data in an instance of the RStreams bus. It also includes functions to allow you to skip the complexity of dealing with pipes and streams at all for the most common use cases: getting data from the bus and sending data to the bus.\n  Reading at Scale You want to read from an RStreams queue. What do you need to consider to ensure you do that efficiently and responsibly at massive scale?\n  App Use Cases and Considerations What kind of app are you making?\n  CASE 1: Are you writing an app that runs once in a while, pulling events from a specific start/end range in the queue?\nMaybe you are writing an app to recover from a failure somewhere in your enterprise and so your app gets a start/end date of events that needs to be re-processed from the queue and it is manually kicked off.\nOr maybe you\u0026rsquo;re writing an app to sample data in a queue as part of monitoring and health checks that gets kicked off on a cron every five minutes to read a few events and go back to sleep.\n  CASE 2: Are you writing an app that runs continously as a daemon, pulling new events from the queue as fast as they show up?\nYou care about each and every event and you want to get each one in order from the queue and process it. If events are pushed into the queue faster than you can read them and process them then you\u0026rsquo;re in trouble because the number of events in the queue that are waiting for you to grab and process will grow unbounded. This means that the data you are processing is forever getting older and older and isn\u0026rsquo;t being processed in near real-time, seconds to a few minutes typically.\nAlso, what happens if your daemon crashes? You will need to restart it and keep reading from where you left off.\n  CASE 3: Are you writing a serverless app that has to shut down every 15 minutes as an AWS lambda function and get restarted and keep going?\nLet\u0026rsquo;s assume that this is just CASE 2 above but instead of a daemon it\u0026rsquo;s a lambda function. You can\u0026rsquo;t miss an event and you need to process them efficiently. You need to make sure you leave enough time to complete processing the events you have and know for sure where you left off before your lambda gets restarted.\n  How much processing are you doing and what latency is acceptable?\nThe more events that are pushed into a queue per unit time the more efficiently your app needs to be able to read and process these events. Reading events from a queue is lightning, but what if you need to call out to an API to get data to enrich each and every event? What if you need to hit a database for each and every event? That\u0026rsquo;s going to slow everything down and could make you upside down in that you can\u0026rsquo;t process events as fast as they are being pushed into a queue.\nHow big are the data events you are reading?\nLarge events can\u0026rsquo;t flow through many of AWS\u0026rsquo;s services. The RStreams SDK will detect this and push them to S3 and write an event that flows into the stream that actually points to the events stored in a file in S3. The SDK handles all of this transparently and you won\u0026rsquo;t even be aware you are reading from S3. However, the larger the events the more this is going to happen and the more time it could take to read events from S3 if those events are striped to hell and back in individual S3 files.\n  Config to the Rescue RStreams includes config in read operations to let you tune reading based upon your specific uses cases.\nThe following applies to the enrich, offloadEvents and read operations.\nReadOptions Interface\nNote there are other options not listed below that are less often needed but might be interesting in some rare cases to fine tune performance such as stream_query_limit, size or loops.\n  fast_s3_read\n Problem reading events is slow, likely because there\u0026rsquo;s lots of small S3 files the SDK is reading events from Solution set this to true and the SDK will read concurrently from multiple S3 files and your reads will be much faster - will default to on in Q3 2022 (you can control how much is ready concurrently if you need fine-grained control, which you likely won\u0026rsquo;t, using fast_s3_read_parallel_fetch_max_bytes)    runTime | stopTime\n Problem your lambda function (bot) is shutting down after 15 minutes instead of ending gracefully because it is endlessly reading events from a queue Solution tell the RStreams read operation you are using to end after runTime number of milliseconds and set that to be 75-80% of the amount of time the lambda has left before it runs out of time before AWS shuts it down forcefully or calculate the exact stopTime that saves roughly 20% of the 15 min shutdown window for the pipe to complete processing, flush and checkpoint.    start\n Problem I don\u0026rsquo;t want to read the latest events, I want to start from a specific position in the queue Solution use the start attribute to specify the event ID of when to start    maxOverride\n Problem I don\u0026rsquo;t want to keep reading events forever, I want to stop at a certain time in the queue Solution use the maxOverride attribute to specify the event ID of when to stop    BatchOptions Interface\nThese don\u0026rsquo;t control reading from a queue but allow you to hold on to a group of events and present those events all at once to the next stream step in the pipe, a concept called micro-batching.\n bytes | count | time  Problem It\u0026rsquo;s taking me longer and longer to process events and I can\u0026rsquo;t keep up with new events coming into the queue and so reading is getting further and further behind Solution Try micro-batching using these attributes to group of events in small batches that are sent to the next pipe stream step all at once and then rewrite whatever your code is doing in that pipe stream step to do it in paralled: if writing to a DB write the entire batch in one SQL query; if reading from a DB, do one read to get all the data you need for all the events in the batch; if hitting an API use Promise.all to run each API request in parallel for the batch. NOTE, if you just can\u0026rsquo;t keep up no matter what, consider implementing Fanout    BufferOptions Interface\nThese serve the same purpose as the BatchOptions Interface above and solve the same problem. The difference is that BatchOptions are built into an RStreams operation to let you control it while BufferOptions is used with the Buffer pipe stream step operation that may be inserted into the pipe to choose to micro-batch events before flowing to the next pipe stream step. The attribute names are named slightly differently but are identical in purpose and function.\nToCheckpointOptions Interface\nHead over to the checkpointing article if you don\u0026rsquo;t know what a checkpoint is or what it\u0026rsquo;s used for.\n records | time  Problem I can\u0026rsquo;t ever re-process an event and so I need to checkpoint after I process each and every event Problem I\u0026rsquo;m OK if I re-process some events in the rare case of a failure and so I only want to checkpoint after so much time or so many records Solution Use these attributes to control checkpointing in a stream (see the checkpoint operation)      Writing at Scale You want to write to an RStreams queue. What do you need to consider to ensure you do that efficiently and responsibly at massive scale?\n  Considerations What\u0026rsquo;s really happening underneath the covers with a write?\nThe SDK is writing to either Kinesis, S3 and Firehose and S3 followed by Kinesis. See the Anatomy of a Bus article for more on this.\nSo, that means Kinesis has limitations on the size of events and how much data you can concurrently write to kinesis at once without having to jump through hoops.\nAm I getting data to write onesie twosie or all at once in big batches?\nPerhaps you are receiving a file from a customer where each row in the file is an object you want send into an RStreams queue or are you getting data in an event driven manner and the flow of those events can\u0026rsquo;t be predicted but is likely either coming one at a time or in a micro-batch.\n  Config to the Rescue RStreams includes config in write operations to let you tune writing based upon your specific uses cases.\nThe following applies to the load, offloadEvents and write operations.\nWriteOptions Interface\n  useS3\n Problem I have lots of events to send to an RStreams queue all at once and it\u0026rsquo;s slow Solution Set the useS3 option to true and the SDK will write a file chock full of events, many thousands is normal, and then send one event through kinesis that points back to the S3 file Problem It\u0026rsquo;s taking too long to read events Solution Wait. Why is this here in the write section? The reason is that how you write can affect how you read. If you write tons and tons and small S3 files, say with one event each, that\u0026rsquo;s going to affect read performance since the SDK will have to make many calls to S3 to read a small number of events. Yes, there\u0026rsquo;s a new fast_s3_read capability that will read multiple files at once that makes this much better but still it can be an issue. So the solution is to be smart about your use of the useS3 attribute. Be sure you micro-batch successfully if you use it, meaning that there is enough data available to be written all at once using the batch or buffer options listed above.    firehose\n Problem My event handler that writes to an RStreams queue is invoked one at a time by the nature of how it runs and the pace at which events come in that I want to write and so I\u0026rsquo;m writing lots of individual events that flow through kinesis and take up concurrent write bandwidth Solution Set firehose to true. Firehose will automatically micro-batch events for us in one minute increments, writing them to an S3 file which will then get sent to kinesis as one event. This does mean that ingestion will be delayed by up to a minute, so this will only work in use cases where this is acceptable.    records | size | time\n Problem I don\u0026rsquo;t want to inundate kinesis with events going one at a time but I need control over how group up events and send as a micro-batch to kinesis because ingestion time matters Solution Use one of these attributes, and probably all of them, to control how long to wait before the SDK micro-batches up events, zips them as a single blob and sends them to kinesis, which performs like a champ. Set number of events, the max size of the events and the max time to wait and the max number of events to wait and whichever is tripped first will cause the micro-batch to be sent as is.    "},{"url":"https://rstreams.org/rstreams-node-sdk/sdk-apis/sink-streams/","title":"Sink Streams","description":"","content":"\r\r\r ToC\r    \r\rThese function create a sink for you, the last step in a pipe.\n Load Function A function that creates a sink that takes in an upstream event and pushes it to an RStreams queue on the bus Devnull Function A function that creates a sink stream that takes in un upstream event and does absolutely nothing with it, except log it if you ask it to  "},{"url":"https://rstreams.org/rstreams-bus/","title":"RStreams Bus","description":"RStreams event bus, built with native AWS services.","content":"The RStreams Bus is a light-weight framework that uses AWS Kinesis, S3, Lambda and Dynamo DB to create an event-streaming and general purpose messaging platform.\n"},{"url":"https://rstreams.org/rstreams-botmon/","title":"RStreams Monitoring","description":"Monitor, trace and debug events in the bus in real-time.","content":"RStreams includes a webapp called Botmon that provides real time monitoring, data visualization, tracing and debugging.\n"},{"url":"https://rstreams.org/","title":"RStreams","description":"","content":""},{"url":"https://rstreams.org/categories/","title":"Categories","description":"","content":""},{"url":"https://rstreams.org/changelog/","title":"Changelog posts","description":"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.","content":"  February Updates Feb 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nChanged\r  Better support for using applying additional filters to posts_tax_query for categories for custom WordPress syncs\n  Reporting fine-tuning for speed improvements (up to 60% improvement in latency)\n  Replaced login / registration pre-app screens with a cleaner design\n   Removed\r Removed an issue with the sync autolinker only interlinking selectively. Removed up an issue with prematurely logging out users   \r  March Updates Mar 6, 2019\nLorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt dolore magna aliquyam erat, sed diam voluptua. At vero eos et ustoLorem ipsum dolor sit amet, consetetur.\u0026quot;\nAdded\r Some scheduled changelogs, tweets, and slack messages queued up this weekend and were not published on time. We fixed the issue and all delayed publications should be out. We now prioritize keywords over title and body so customers can more effectively influence search results Support form in the Assistant is now protected with reCaptcha to reduce spam reinitializeOnUrlChange added to the JavaScript API to improve support for pages with turbolinks   Fixed\r Fixed an issue with the sync autolinker only interlinking selectively. Fixed up an issue with prematurely logging out users   \r  Changelog label \rAdded\r Changed\r Depricated\r Removed\r Fixed\r Security\r Unreleased\r "},{"url":"https://rstreams.org/contact/","title":"Got Any Questions","description":"this is meta description","content":""},{"url":"https://rstreams.org/rstreams-node-sdk/api-docs/","title":"SDK API Docs","description":"The generated RStreams Node SDK API Docs","content":"Jump over to the RStreams Node SDK API Docs.\n"},{"url":"https://rstreams.org/search/","title":"Search Result","description":"this is meta description","content":""},{"url":"https://rstreams.org/tags/","title":"Tags","description":"","content":""}]